
import os
import threading
import logging
import time
import pandas as pd
import shutil
from PyPDF2 import PdfMerger
from zipfile import ZipFile
import yadisk

TRANSLIT = {'a':'–∞','b':'–±','c':'—Ü','d':'–¥','e':'–µ','f':'—Ñ','g':'–≥',
            'h':'—Ö','i':'–∏','j':'–π','k':'–∫','l':'–ª','m':'–º','n':'–Ω',
            'o':'–æ','p':'–ø','r':'—Ä','s':'—Å','t':'—Ç','u':'—É','v':'–≤',
            'w':'–≤–≤','x':'–∫—Å','y':'—ã','z':'–∑'}
NUMS = {'1':'–æ–¥–∏–Ω','2':'–¥–≤–∞','3':'—Ç—Ä–∏','4':'—á–µ—Ç—ã—Ä–µ',
        '5':'–ø—è—Ç—å','6':'—à–µ—Å—Ç—å','7':'—Å–µ–º—å',
        '8':'–≤–æ—Å–µ–º—å','9':'–¥–µ–≤—è—Ç—å','0':'–Ω–æ–ª—å'}

CITY_CODEWORDS = {
    "Moscow": "—Ç—é–ª–µ–Ω—å",
    "Novosibirsk": "–Ω–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫"
}

y = yadisk.YaDisk(token=os.getenv("YANDEX_TOKEN"))

def download_and_extract_folder_as_zip(sity, site):
    remote_folder = f"{sity}_pdf/{site}"
    local_folder = f"{sity}_pdf/"
    local_zip = f"{site}.zip"

    if os.path.exists(remote_folder):
        shutil.rmtree(remote_folder)
    os.makedirs(remote_folder, exist_ok=True)

    try:
        y.download(remote_folder, local_zip)
        with ZipFile(local_zip, 'r') as zip_ref:
            zip_ref.extractall(local_folder)
        os.remove(local_zip)
        return f"{local_folder}{site}"
    except Exception as e:
        raise RuntimeError(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–∫–∞—á–∞—Ç—å –∏ —Ä–∞–∑–∞—Ä—Ö–∏–≤–∏—Ä–æ–≤–∞—Ç—å PDF: {e}")

def fill_missing_identifiers(df, codeword):
    for col in ['–§–∞–º–∏–ª–∏—è', '–ò–º—è', '–ö–æ–¥–æ–≤–æ–µ —Å–ª–æ–≤–æ']:
        df[col] = df[col].fillna(codeword).replace('', codeword)
    return df

def check_missing_errors(df):
    cols = ['–û—Ä—Ñ. –æ—à–∏–±–∫–∏', '–ü—É–Ω–∫—Ç. –æ—à–∏–±–∫–∏']
    has_null = df[cols].isnull().any().any()
    has_empty = (df[cols] == '').any().any()
    return has_null or has_empty

def merge_duplicate_pdfs(df, pdf_folder, log,site):
    duplicates = df[df.duplicated(['–§–∞–º–∏–ª–∏—è','–ò–º—è','–ö–æ–¥–æ–≤–æ–µ —Å–ª–æ–≤–æ'],keep=False)]
    merged = []
    for key, grp in duplicates.groupby(['–§–∞–º–∏–ª–∏—è','–ò–º—è','–ö–æ–¥–æ–≤–æ–µ —Å–ª–æ–≤–æ'],sort=False):
        merger = PdfMerger()
        parts = [site]
        for fid in grp['Id —Ä–∞–±–æ—Ç—ã']:
            src = os.path.join(pdf_folder, f"{fid}.pdf")
            merger.append(src)
            os.remove(src)
            parts.extend(fid.split('_')[1].split('.')[0])
        new_name = "_".join(parts)
        dest = os.path.join(pdf_folder, f"{new_name}.pdf")
        merger.write(dest)
        merger.close()
        merged.append((grp.index[0], new_name, *key,
                       grp['–û—Ä—Ñ. –æ—à–∏–±–∫–∏'].iloc[0], grp['–ü—É–Ω–∫—Ç. –æ—à–∏–±–∫–∏'].iloc[0]))
        log.append(f"üîÄ –û–±—ä–µ–¥–∏–Ω–µ–Ω—ã —Å–∫–∞–Ω—ã: {grp['Id —Ä–∞–±–æ—Ç—ã'].tolist()} ‚Üí {new_name}")
    merged_df = pd.DataFrame(merged,
        columns=['index','Id —Ä–∞–±–æ—Ç—ã','–§–∞–º–∏–ª–∏—è','–ò–º—è','–ö–æ–¥–æ–≤–æ–µ —Å–ª–æ–≤–æ','–û—Ä—Ñ. –æ—à–∏–±–∫–∏','–ü—É–Ω–∫—Ç. –æ—à–∏–±–∫–∏'])
    rest = df.drop(duplicates.index).copy()
    rest['index'] = rest.index
    rest = rest[['index','Id —Ä–∞–±–æ—Ç—ã','–§–∞–º–∏–ª–∏—è','–ò–º—è','–ö–æ–¥–æ–≤–æ–µ —Å–ª–æ–≤–æ','–û—Ä—Ñ. –æ—à–∏–±–∫–∏','–ü—É–Ω–∫—Ç. –æ—à–∏–±–∫–∏']]
    return pd.concat([merged_df, rest], ignore_index=True).sort_values('index')

def normalize_and_explode(df, translit, nums, log,site):
    df = df.rename(columns={'Id —Ä–∞–±–æ—Ç—ã':'id'}).reset_index(drop=True)
    for col in ['–§–∞–º–∏–ª–∏—è','–ò–º—è','–ö–æ–¥–æ–≤–æ–µ —Å–ª–æ–≤–æ']:
        df[col] = df[col].str.lower().str.split(',')
    df = df.explode("–§–∞–º–∏–ª–∏—è").explode("–ò–º—è").explode("–ö–æ–¥–æ–≤–æ–µ —Å–ª–æ–≤–æ")
    for mapping in (translit, nums):
        for k, v in mapping.items():
            for col in ['–§–∞–º–∏–ª–∏—è', '–ò–º—è', '–ö–æ–¥–æ–≤–æ–µ —Å–ª–æ–≤–æ']:
                before = df[col].copy()
                df[col] = df[col].replace(k, v, regex=True)
                if not df[col].equals(before):
                    log.append(f"üî§ –ó–∞–º–µ–Ω–∞ –≤ —Ñ–∞–π–ª–µ {site} '{col}': {k} ‚Üí {v}")
    df[['–§–∞–º–∏–ª–∏—è','–ò–º—è','–ö–æ–¥–æ–≤–æ–µ —Å–ª–æ–≤–æ']] = df[['–§–∞–º–∏–ª–∏—è','–ò–º—è','–ö–æ–¥–æ–≤–æ–µ —Å–ª–æ–≤–æ']].replace('[^—ë–∞-—è]','',regex=True)
    df['–§–ò–û'] = df['–§–∞–º–∏–ª–∏—è'] + ' ' + df['–ò–º—è']
    df['–°–∫–∞–Ω'] = df['id'].astype(str)+'.pdf'
    return df[['–§–ò–û','–û—Ä—Ñ. –æ—à–∏–±–∫–∏','–ü—É–Ω–∫—Ç. –æ—à–∏–±–∫–∏','–ö–æ–¥–æ–≤–æ–µ —Å–ª–æ–≤–æ','–°–∫–∞–Ω']]

def fire_and_forget_upload(path):
    def upload():
        try:
            if y.exists(path):
                y.remove(path, permanently=True)
            y.upload(path, path)
        except Exception as e:
            logging.warning(f"–§–æ–Ω–æ–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –Ω–µ —É–¥–∞–ª–∞—Å—å: {e}")

    threading.Thread(target=upload).start()

async def process_csv(sity, site):
    logs = []

    try:
        start_time = time.time()
        codeword = CITY_CODEWORDS.get(sity)
        pdf_folder = download_and_extract_folder_as_zip(sity, site)
        xlsx_file = f"{sity}_xlsx/verified/{site}.xlsx"

        os.makedirs(os.path.dirname(xlsx_file), exist_ok=True)
        try:
            y.download(xlsx_file, xlsx_file)
        except Exception as e:
            raise FileNotFoundError(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–∫–∞—á–∞—Ç—å Excel —Å –Ø–Ω–¥–µ–∫—Å.–î–∏—Å–∫–∞: {e}")

        df = pd.read_excel(xlsx_file)
        df = df.rename(columns={'–û—Ä—Ñ.\n –æ—à–∏–±–∫–∏' : '–û—Ä—Ñ. –æ—à–∏–±–∫–∏', '–ü—É–Ω–∫—Ç.\n –æ—à–∏–±–∫–∏' : '–ü—É–Ω–∫—Ç. –æ—à–∏–±–∫–∏'})
        df = fill_missing_identifiers(df, codeword)
        if check_missing_errors(df):
            logs.append(f"‚ö†Ô∏è –í {site}.xlsx –µ—Å—Ç—å –ø—Ä–æ–ø—É—Å–∫–∏ –≤ –∫–æ–ª–æ–Ω–∫–∞—Ö –æ—à–∏–±–æ–∫")
            return { "status": "skipped","name": site, "logs": logs }

        merged = merge_duplicate_pdfs(df, pdf_folder, logs,site)
        final_df = normalize_and_explode(merged, TRANSLIT, NUMS, logs,site)
        final_df = final_df.rename(columns={
        "–û—Ä—Ñ. –æ—à–∏–±–∫–∏": "–û—Ä—Ñ–æ–≥—Ä–∞—Ñ–∏—è",
        "–ü—É–Ω–∫—Ç. –æ—à–∏–±–∫–∏": "–ü—É–Ω–∫—Ç—É–∞—Ü–∏—è"
        })

        os.makedirs(f"./{sity}_csv", exist_ok=True)
        csv_path = f"{sity}_csv/{site}.csv"
        final_df.to_csv(csv_path, sep=';', encoding='cp1251', index=False)
        fire_and_forget_upload(csv_path)
        remote_pdf_folder = f"{sity}_csv/{site}"
        if y.exists(remote_pdf_folder):
            y.remove(remote_pdf_folder, permanently=True)
        y.mkdir(remote_pdf_folder)

        # –ö–æ–ø–∏—Ä—É–µ–º PDF-—Ñ–∞–π–ª—ã –∏–∑ –ª–æ–∫–∞–ª—å–Ω–æ–π –ø–∞–ø–∫–∏
        for fname in os.listdir(pdf_folder):
            full_path = os.path.join(pdf_folder, fname)
            remote_path = f"{remote_pdf_folder}/{fname}"
            y.upload(full_path, remote_path)

        logs.append(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω: {site} –∑–∞ {time.time()-start_time} —Å–µ–∫—É–Ω–¥")
        logging.info(logs)

        done_folder = f"{sity}_xlsx/done"
        if not y.exists(done_folder):
            y.mkdir(done_folder)
        done_path = f"{done_folder}/{site}.xlsx"
        if y.exists(done_path):
            y.remove(done_path, permanently=True)
        y.move(xlsx_file, done_path)

        return {
            "status": "success",
            "name": site,
            "csv_path": f"/{sity}_csv/{site}.csv",
            "zip_path": f"/{sity}_csv/{site}.zip",
            "logs": logs
        }

    except Exception as e:
        logging.error(e)
        logs.append(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {site}: {str(e)}")
        return { "status": "error", "name": site, "logs": logs }
