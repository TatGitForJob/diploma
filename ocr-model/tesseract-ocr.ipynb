{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install easyocr pytesseract\n",
        "!apt-get install -y tesseract-ocr-rus\n",
        "!tesseract --list-langs\n"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDbLm_mAlYWF",
        "outputId": "e9a83c44-5d8c-4eaa-f44c-0080f8582aa8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: easyocr in /usr/local/lib/python3.11/dist-packages (1.7.2)\n",
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from easyocr) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision>=0.5 in /usr/local/lib/python3.11/dist-packages (from easyocr) (0.21.0+cu124)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.11/dist-packages (from easyocr) (4.11.0.86)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from easyocr) (1.14.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from easyocr) (2.0.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from easyocr) (11.1.0)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (from easyocr) (0.25.2)\n",
            "Requirement already satisfied: python-bidi in /usr/local/lib/python3.11/dist-packages (from easyocr) (0.6.6)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from easyocr) (6.0.2)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.11/dist-packages (from easyocr) (2.0.7)\n",
            "Requirement already satisfied: pyclipper in /usr/local/lib/python3.11/dist-packages (from easyocr) (1.3.0.post6)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.11/dist-packages (from easyocr) (1.11.1.4)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->easyocr) (1.3.0)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image->easyocr) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image->easyocr) (2025.3.13)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image->easyocr) (0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->easyocr) (3.0.2)\n",
            "Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: pytesseract\n",
            "Successfully installed pytesseract-0.3.13\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tesseract-ocr-rus is already the newest version (1:4.00~git30-7274cfa-1.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 29 not upgraded.\n",
            "List of available languages (3):\n",
            "eng\n",
            "osd\n",
            "rus\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import easyocr\n",
        "import pytesseract\n",
        "from PIL import Image\n",
        "\n",
        "def remove_black_lines_and_fix_symbols(image_path):\n",
        "    # –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ HSV\n",
        "    img = cv2.imread(image_path)\n",
        "    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
        "\n",
        "    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ –¥–ª—è —Å–∏–Ω–µ–≥–æ –∏ –≥–æ–ª—É–±–æ–≥–æ —Ü–≤–µ—Ç–æ–≤\n",
        "    lower_blue = np.array([85, 30, 50])\n",
        "    upper_blue = np.array([135, 255, 255])\n",
        "\n",
        "    # –°–æ–∑–¥–∞–Ω–∏–µ –º–∞—Å–∫–∏ –∏ –≤—ã–¥–µ–ª–µ–Ω–∏–µ —Å–∏–Ω–µ–≥–æ —Ç–µ–∫—Å—Ç–∞\n",
        "    blue_mask = cv2.inRange(hsv, lower_blue, upper_blue)\n",
        "\n",
        "    # –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∞—è –æ–ø–µ—Ä–∞—Ü–∏—è (—Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ), —á—Ç–æ–±—ã —É—Å—Ç—Ä–∞–Ω–∏—Ç—å —Ä–∞–∑—Ä—ã–≤—ã\n",
        "    kernel = np.ones((3, 3), np.uint8)  # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —è–¥—Ä–∞ (—Ä–∞–∑–º–µ—Ä –º–æ–∂–Ω–æ –ø–æ–¥–±–∏—Ä–∞—Ç—å)\n",
        "    blue_mask_dilated = cv2.dilate(blue_mask, kernel, iterations=25)  # –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –º–∞—Å–∫–∏\n",
        "    # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –º–∞—Å–∫–∏ –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é\n",
        "    result = cv2.bitwise_and(img, img, mask=blue_mask_dilated)\n",
        "\n",
        "    # –ó–∞–º–µ–Ω–∞ —á—ë—Ä–Ω–æ–≥–æ —Ñ–æ–Ω–∞ –Ω–∞ –±–µ–ª—ã–π\n",
        "    result[blue_mask_dilated == 0] = [255, 255, 255]\n",
        "\n",
        "    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Ä—è–¥–æ–º —Å –∏—Å—Ö–æ–¥–Ω—ã–º —Ñ–∞–π–ª–æ–º\n",
        "    processed_image_path = image_path.replace(\".jpg\", \"_processed.jpg\")\n",
        "    cv2.imwrite(processed_image_path, result)\n",
        "\n",
        "    return processed_image_path"
      ],
      "metadata": {
        "id": "-TV19BY-vQi6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_path_new = \"image.jpg\"  # –ü—É—Ç—å –∫ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–º—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è\n",
        "\n",
        "#image_path_new=remove_black_lines(image_path)\n",
        "\n",
        "reader = easyocr.Reader(['ru'])  # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–¥–¥–µ—Ä–∂–∫—É —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–æ–≤\n",
        "# –†–∞—Å–ø–æ–∑–Ω–∞—ë–º —Ç–µ–∫—Å—Ç –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏\n",
        "results = reader.readtext(image_path_new,allowlist=\"–π—Ü—É–∫–µ–Ω–≥—à—â–∑—Ö—ä—Ñ—ã–≤–∞–ø—Ä–æ–ª–¥–∂—ç—è—á—Å–º–∏—Ç—å–±—é—ë\")  # detail=1 –≤–µ—Ä–Ω—ë—Ç –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –∏ —Ç–µ–∫—Å—Ç\n",
        "# –í—ã–≤–æ–¥–∏–º —Ç–µ–∫—Å—Ç –∏ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã\n",
        "for (bbox, text, confidence) in results:\n",
        "    print(f\"–¢–µ–∫—Å—Ç: {text}, –î–æ–≤–µ—Ä–∏–µ: {confidence}\")\n",
        "\n",
        "\n",
        "img = cv2.imread(image_path_new)\n",
        "custom_config = r'-c tessedit_char_whitelist=–π—Ü—É–∫–µ–Ω–≥—à—â–∑—Ö—ä—Ñ—ã–≤–∞–ø—Ä–æ–ª–¥–∂—ç—è—á—Å–º–∏—Ç—å–±—é—ë'\n",
        "text = pytesseract.image_to_string(img, lang=\"rus\", config=custom_config)\n",
        "print(\"–†–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç:\", text)\n",
        "\n",
        "img = Image.open(image_path_new)\n",
        "plt.imshow(img)\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "yfKISSMpr4rL",
        "outputId": "4a91b66b-f984-41f7-a3eb-db041351f6ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:easyocr.easyocr:Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "–¢–µ–∫—Å—Ç: –º, –î–æ–≤–µ—Ä–∏–µ: 0.4334957390534022\n",
            "–†–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç: \f\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "McIYxBxnsiF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from graphviz import Digraph\n",
        "\n",
        "# –°–æ–∑–¥–∞–Ω–∏–µ UML –¥–∏–∞–≥—Ä–∞–º–º—ã –ø—Ä–µ—Ü–µ–¥–µ–Ω—Ç–æ–≤\n",
        "dot = Digraph('UserScenario', format='png')\n",
        "\n",
        "# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∞–∫—Ç–µ—Ä–∞ (–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å)\n",
        "dot.node('U', '–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å', shape='actor')\n",
        "\n",
        "# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–µ—Ü–µ–¥–µ–Ω—Ç–æ–≤ (—Ñ—É–Ω–∫—Ü–∏–π —Å–∏—Å—Ç–µ–º—ã)\n",
        "dot.node('P1', '–ó–∞–≥—Ä—É–∑–∏—Ç—å PDF', shape='ellipse')\n",
        "dot.node('P2', '–û–±—Ä–∞–±–æ—Ç–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç', shape='ellipse')\n",
        "dot.node('P3', '–í—ã–¥–µ–ª–∏—Ç—å –æ—à–∏–±–∫–∏', shape='ellipse')\n",
        "dot.node('P4', '–°—Ä–∞–≤–Ω–∏—Ç—å –§–ò–û —Å –±–∞–∑–æ–π', shape='ellipse')\n",
        "dot.node('P5', '–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ', shape='ellipse')\n",
        "dot.node('P6', '–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –≤ XLS', shape='ellipse')\n",
        "\n",
        "# –°–≤—è–∑–∏ –º–µ–∂–¥—É –∞–∫—Ç–æ—Ä–æ–º –∏ –ø—Ä–µ—Ü–µ–¥–µ–Ω—Ç–∞–º–∏\n",
        "dot.edge('U', 'P1')\n",
        "dot.edge('P1', 'P2')\n",
        "dot.edge('P2', 'P3')\n",
        "dot.edge('P3', 'P4')\n",
        "dot.edge('P4', 'P5')\n",
        "dot.edge('P2', 'P6')\n",
        "dot.edge('P5', 'P6')\n",
        "\n",
        "# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∏–∞–≥—Ä–∞–º–º—ã\n",
        "diagram_path = \"/mnt/data/user_scenario_uml.png\"\n",
        "dot.render(diagram_path, format='png')\n",
        "\n",
        "# –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¥–∏–∞–≥—Ä–∞–º–º—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é\n",
        "import IPython.display as display\n",
        "display.Image(diagram_path)\n"
      ],
      "metadata": {
        "id": "l4wDgWh5aGSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "–¢–µ–∫—Å—Ç–æ–≤–∞—è —è—á–µ–π–∫–∞ <WEYXFOjaiJ6W>\n",
        "# %% [markdown]\n",
        "# Transformers for letters recognition\n",
        "Fine-tuning of ViT and BEiT for handwritten letters recognition.\n",
        "\n",
        "–¢–µ–∫—Å—Ç–æ–≤–∞—è —è—á–µ–π–∫–∞ <3qk31iFpmXFL>\n",
        "# %% [markdown]\n",
        "About:\n",
        "\n",
        "1. [ViT paper](https://arxiv.org/pdf/2010.11929.pdf), [ViT Github](https://github.com/google-research/vision_transformer), [ViT on huggingfaceü§ó](https://huggingface.co/docs/transformers/model_doc/vit)\n",
        "2. [BEiT paper](https://openreview.net/pdf?id=p-BhZSz59o4), [BEiT on huggingfaceü§ó](https://huggingface.co/docs/transformers/model_doc/beit)\n",
        "\n",
        "\n",
        "–¢–µ–∫—Å—Ç–æ–≤–∞—è —è—á–µ–π–∫–∞ <1rQBn4g9tCWC>\n",
        "# %% [markdown]\n",
        "This notebook is based on huggingfaceü§ó [tutorial](https://huggingface.co/blog/fine-tune-vit) for ViT fine-tuning\n",
        "\n",
        "–¢–µ–∫—Å—Ç–æ–≤–∞—è —è—á–µ–π–∫–∞ <jJklyKNKijPs>\n",
        "# %% [markdown]\n",
        "## Imports\n",
        "\n",
        "–ö–æ–¥–æ–≤–∞—è —è—á–µ–π–∫–∞ <Qh2R9NFcmrWT>\n",
        "# %% [code]\n",
        "!pip3 install transformers tokenizers datasets evaluate --quiet\n",
        "–ü–æ–ª—É—á–µ–Ω—ã –≤—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.\n",
        "0KB\n",
        "\tStream\n",
        "\t\t'pip3' is not recognized as an internal or external command,\n",
        "\t\toperable program or batch file.\n",
        "\n",
        "–ö–æ–¥–æ–≤–∞—è —è—á–µ–π–∫–∞ <nDzb1u4ghzgo>\n",
        "# %% [code]\n",
        "import os\n",
        "import random\n",
        "import warnings\n",
        "import google-api-python-client\n",
        "\n",
        "import zipfile\n",
        "from google.colab import drive\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm, trange\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from IPython.display import clear_output\n",
        "from PIL import Image\n",
        "\n",
        "import datasets\n",
        "from datasets import load_dataset, ClassLabel\n",
        "\n",
        "from transformers import ViTFeatureExtractor, BeitFeatureExtractor\n",
        "from transformers import ViTForImageClassification, BeitForImageClassification\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from transformers import pipeline\n",
        "\n",
        "import evaluate\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import balanced_accuracy_score, cohen_kappa_score, matthews_corrcoef\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn import preprocessing\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "%matplotlib inline\n",
        "–ü–æ–ª—É—á–µ–Ω—ã –≤—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.\n",
        "0KB\n",
        "\tError\n",
        "\t\tSyntaxError\n",
        "\t\t  Cell In[16], line 4\n",
        "\t\t    import google-api-python-client\n",
        "\t\t                 ^\n",
        "\t\tSyntaxError: invalid syntax\n",
        "\n",
        "\n",
        "–ö–æ–¥–æ–≤–∞—è —è—á–µ–π–∫–∞ <62xEDJmeikkl>\n",
        "# %% [code]\n",
        "drive.mount('/content/drive')\n",
        "–ü–æ–ª—É—á–µ–Ω—ã –≤—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.\n",
        "0KB\n",
        "\tStream\n",
        "\t\tMounted at /content/drive\n",
        "\n",
        "–ö–æ–¥–æ–≤–∞—è —è—á–µ–π–∫–∞ <OzGgQmNWinHY>\n",
        "# %% [code]\n",
        "path_to_zip_file = '/content/drive/MyDrive/total dict.zip'\n",
        "directory_to_extract_to = '/content/data'\n",
        "\n",
        "with zipfile.ZipFile(path_to_zip_file, 'r') as zip_ref:\n",
        "    zip_ref.extractall(directory_to_extract_to)\n",
        "\n",
        "–ö–æ–¥–æ–≤–∞—è —è—á–µ–π–∫–∞ <QZABgTv2ioi5>\n",
        "# %% [code]\n",
        "!cp '/content/drive/MyDrive/full_dict/total dict/all_by_words.tsv' '/content/data/total dict'\n",
        "\n",
        "–ö–æ–¥–æ–≤–∞—è —è—á–µ–π–∫–∞ <WXj6cxkWiqpn>\n",
        "# %% [code]\n",
        "os.chdir('/content/data/total dict')\n",
        "os.path.abspath('.')\n",
        "–ü–æ–ª—É—á–µ–Ω—ã –≤—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.\n",
        "0KB\n",
        "\ttext/plain\n",
        "\t\t'/content/data/total dict'\n",
        "\n",
        "–ö–æ–¥–æ–≤–∞—è —è—á–µ–π–∫–∞ <BJ5jLI6wslc8>\n",
        "# %% [code]\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "–ö–æ–¥–æ–≤–∞—è —è—á–µ–π–∫–∞ <2Nz4ayH8vms8>\n",
        "# %% [code]\n",
        "accuracy = evaluate.load('accuracy', 'multiclass')\n",
        "–ü–æ–ª—É—á–µ–Ω—ã –≤—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.\n",
        "0KB\n",
        "\ttext/plain\n",
        "\t\tDownloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]\n",
        "\n",
        "–ö–æ–¥–æ–≤–∞—è —è—á–µ–π–∫–∞ <eMhT-QwvirDP>\n",
        "# %% [code]\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        torch.backends.cudnn.benchmark = True\n",
        "        torch.backends.cudnn.deterministic = False\n",
        "\n",
        "–ö–æ–¥–æ–≤–∞—è —è—á–µ–π–∫–∞ <GuD88Uk0itG7>\n",
        "# %% [code]\n",
        "seed_everything()\n",
        "\n",
        "–¢–µ–∫—Å—Ç–æ–≤–∞—è —è—á–µ–π–∫–∞ <5YVAv78Vip2J>\n",
        "# %% [markdown]\n",
        "## Data\n",
        "\n",
        "–ö–æ–¥–æ–≤–∞—è —è—á–µ–π–∫–∞ <9QvxaqrvNpdh>\n",
        "# %% [code]\n",
        "!cp '/content/drive/MyDrive/full_dict/total dict/all_files_new.tsv' '/content/data/total dict'\n",
        "\n",
        "–ö–æ–¥–æ–≤–∞—è —è—á–µ–π–∫–∞ <0hA1uYb0Na4r>\n",
        "# %% [code]\n",
        "df = pd.read_csv('/content/drive/MyDrive/all_files_final.csv', delimiter=',')\n",
        "line_name = \"/content/data/total dict\"\n",
        "full_paths = [line_name + path[1:] for path in df[\"new_path\"]]\n",
        "df[\"new_path\"] = full_paths\n",
        "df = df.dropna(subset=['letter'])\n",
        "df.head()\n",
        "–ü–æ–ª—É—á–µ–Ω—ã –≤—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.\n",
        "6KB\n",
        "\ttext/plain\n",
        "\t\tUnnamed: 0  letter_position   category letter  \\\n",
        "\t\t0           0                0  codewords      –∞\n",
        "\t\t1           1                2  codewords      –∞\n",
        "\t\t2           2                4  codewords      –∞\n",
        "\t\t3           3                1  codewords      –±\n",
        "\t\t4           4                3  codewords      –∫\n",
        "\n",
        "\t\t                                            new_path  word_id word_true\n",
        "\t\t0  /content/data/total dict/codewords/abakan.jpg_...        0    –∞–±–∞–∫–∞–Ω\n",
        "\t\t1  /content/data/total dict/codewords/abakan.jpg_...        0    –∞–±–∞–∫–∞–Ω\n",
        "\t\t2  /content/data/total dict/codewords/abakan.jpg_...        0    –∞–±–∞–∫–∞–Ω\n",
        "\t\t3  /content/data/total dict/codewords/abakan.jpg_...        0    –∞–±–∞–∫–∞–Ω\n",
        "\t\t4  /content/data/total dict/codewords/abakan.jpg_...        0    –∞–±–∞–∫–∞–Ω\n",
        "\n",
        "–¢–µ–∫—Å—Ç–æ–≤–∞—è —è—á–µ–π–∫–∞ <SroCScWoViMr>\n",
        "# %% [markdown]\n",
        "[Issue](https://github.com/huggingface/transformers/issues/21638) on sizes of images. Let's use the ```PIL``` to open images. Then convert it to RGB according to [this duscussion](https://stackoverflow.com/questions/75168665/unsupported-number-of-image-dimensions-while-using-image-utils-from-transforme)\n",
        "\n",
        "–ö–æ–¥–æ–≤–∞—è —è—á–µ–π–∫–∞ <Oq49ZVuisgWK>\n",
        "# %% [code]\n",
        "class LettersDataset(Dataset):\n",
        "    def __init__(self, data, feature_extractor, transform=None):\n",
        "        self.data = data\n",
        "        self.transform = transform\n",
        "        self.feature_extractor = feature_extractor\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.data.iloc[idx].new_path\n",
        "        # print(img_path)\n",
        "        image = Image.open(img_path)\n",
        "        # print(image)\n",
        "        label = self.data.iloc[idx].labels\n",
        "        if self.transform:\n",
        "            item = self.transform(image, label, self.feature_extractor)\n",
        "            return item\n",
        "\n",
        "        return image, label\n",
        "\n",
        "–ö–æ–¥–æ–≤–∞—è —è—á–µ–π–∫–∞ <ler4EUg5IPi7>\n",
        "# %% [code]\n",
        "le = preprocessing.LabelEncoder()\n",
        "df['labels'] = le.fit_transform(df.letter)\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "–ö–æ–¥–æ–≤–∞—è —è—á–µ–π–∫–∞ <m9_WEJTiuwC4>\n",
        "# %% [code]\n",
        "df.head()\n",
        "–ü–æ–ª—É—á–µ–Ω—ã –≤—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.\n",
        "6KB\n",
        "\ttext/plain\n",
        "\t\tUnnamed: 0  letter_position   category letter  \\\n",
        "\t\t0           0                0  codewords      –∞\n",
        "\t\t1           1                2  codewords      –∞\n",
        "\t\t2           2                4  codewords      –∞\n",
        "\t\t3           3                1  codewords      –±\n",
        "\t\t4           4                3  codewords      –∫\n",
        "\n",
        "\t\t                                            new_path  word_id word_true  \\\n",
        "\t\t0  /content/data/total dict/codewords/abakan.jpg_...        0    –∞–±–∞–∫–∞–Ω\n",
        "\t\t1  /content/data/total dict/codewords/abakan.jpg_...        0    –∞–±–∞–∫–∞–Ω\n",
        "\t\t2  /content/data/total dict/codewords/abakan.jpg_...        0    –∞–±–∞–∫–∞–Ω\n",
        "\t\t3  /content/data/total dict/codewords/abakan.jpg_...        0    –∞–±–∞–∫–∞–Ω\n",
        "\t\t4  /content/data/total dict/codewords/abakan.jpg_...        0    –∞–±–∞–∫–∞–Ω\n",
        "\n",
        "\t\t   labels\n",
        "\t\t0       1\n",
        "\t\t1       1\n",
        "\t\t2       1\n",
        "\t\t3       2\n",
        "\t\t4      11\n",
        "\n",
        "–ö–æ–¥–æ–≤–∞—è —è—á–µ–π–∫–∞ <1Qsr81dJuz9K>\n",
        "# %% [code]\n",
        "le.classes_\n",
        "–ü–æ–ª—É—á–µ–Ω—ã –≤—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.\n",
        "0KB\n",
        "\tError\n",
        "\t\tNameError\n",
        "\t\t---------------------------------------------------------------------------\n",
        "\t\tNameError                                 Traceback (most recent call last)\n",
        "\t\tCell In[2], line 1\n",
        "\t\t----> 1 le.classes_\n",
        "\n",
        "\t\tNameError: name 'le' is not defined\n",
        "\n",
        "–¢–µ–∫—Å—Ç–æ–≤–∞—è —è—á–µ–π–∫–∞ <kUESNAU9jIMC>\n",
        "# %% [markdown]\n",
        "## ViT Model\n",
        "\n",
        "–ö–æ–¥–æ–≤–∞—è —è—á–µ–π–∫–∞ <d23a8Z2psiHG>\n",
        "# %% [code]\n",
        "vit_model_name = 'google/vit-base-patch16-224'\n",
        "vit_feature_extractor = ViTFeatureExtractor.from_pretrained(vit_model_name)\n",
        "–ü–æ–ª—É—á–µ–Ω—ã –≤—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.\n",
        "0KB\n",
        "\ttext/plain\n",
        "\t\tDownloading (‚Ä¶)rocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]\n",
        "\n",
        "–ö–æ–¥–æ–≤–∞—è —è—á–µ–π–∫–∞ <cVYMjqnNAj_I>\n",
        "# %% [code]\n",
        "def process_example(image, label, feature_extractor):\n",
        "    inputs = feature_extractor(image.convert('RGB'), return_tensors='pt')\n",
        "    inputs['labels'] = label\n",
        "    return inputs\n",
        "\n",
        "–ö–æ–¥–æ–≤–∞—è —è—á–µ–π–∫–∞ <X89BHzGfI_EI>\n",
        "# %% [code]\n",
        "train_dataset = LettersDataset(train_df, vit_feature_extractor, transform=process_example)\n",
        "test_dataset = LettersDataset(test_df, vit_feature_extractor, transform=process_example)\n",
        "\n",
        "–ö–æ–¥–æ–≤–∞—è —è—á–µ–π–∫–∞ <C5XteBABU6A5>\n",
        "# %% [code]\n",
        "# item size\n",
        "train_dataset[0]['pixel_values'].shape  # batch_size, num_channels, width, height\n",
        "–ü–æ–ª—É—á–µ–Ω—ã –≤—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.\n",
        "0KB\n",
        "\ttext/plain\n",
        "\t\ttorch.Size([1, 3, 224, 224])\n",
        "\n",
        "–ö–æ–¥–æ–≤–∞—è —è—á–µ–π–∫–∞ <zuJ6vqVtvTDB>\n",
        "# %% [code]\n",
        "def collate_fn(batch):\n",
        "    return {\n",
        "        'pixel_values': torch.stack([x['pixel_values'][0] for x in batch], 0),\n",
        "        'labels': torch.tensor([x['labels'] for x in batch])\n",
        "    }\n",
        "\n",
        "–ö–æ–¥–æ–≤–∞—è —è—á–µ–π–∫–∞ <J7jclUuJTI_R>\n",
        "# %% [code]\n",
        "# size after data collator\n",
        "batch = [train_dataset[0], train_dataset[1], train_dataset[2]]\n",
        "collated = collate_fn(batch)\n",
        "print(collated['pixel_values'].shape)\n",
        "print(collated['labels'].shape)\n",
        "–ü–æ–ª—É—á–µ–Ω—ã –≤—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.\n",
        "0KB\n",
        "\tStream\n",
        "\t\ttorch.Size([3, 3, 224, 224])\n",
        "\t\ttorch.Size([3])\n",
        "\n",
        "–ö–æ–¥–æ–≤–∞—è —è—á–µ–π–∫–∞ <zpL1ik_3vZj5>\n",
        "# %% [code]\n",
        "labels = df.labels.unique()\n",
        "\n",
        "model = ViTForImageClassification.from_pretrained(\n",
        "    vit_model_name,\n",
        "    num_labels=labels.shape[0],\n",
        "    id2label={str(i): c for i, c in enumerate(le.classes_)},  # to convert id of class to real label\n",
        "    label2id={c: str(i) for i, c in enumerate(le.classes_)},\n",
        "    ignore_mismatched_sizes=True\n",
        ")\n",
        "–ü–æ–ª—É—á–µ–Ω—ã –≤—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.\n",
        "1KB\n",
        "\tStream\n",
        "\t\tSome weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
        "\t\t- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([34, 768]) in the model instantiated\n",
        "\t\t- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([34]) in the model instantiated\n",
        "\t\tYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
        "\ttext/plain\n",
        "\t\tDownloading (‚Ä¶)lve/main/config.json:   0%|          | 0.00/69.7k [00:00<?, ?B/s]\n",
        "\t\tDownloading pytorch_model.bin:   0%|          | 0.00/346M [00:00<?, ?B/s]\n",
        "\n",
        "–ö–æ–¥–æ–≤–∞—è —è—á–µ–π–∫–∞ <IwO3DQWgOscP>\n",
        "# %% [code]\n",
        "labels\n",
        "–ü–æ–ª—É—á–µ–Ω—ã –≤—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.\n",
        "0KB\n",
        "\ttext/plain\n",
        "\t\tarray([ 1,  2, 11, 14,  9, 22, 12,  5, 20, 15, 18, 17,  4,  3, 29,  6, 26,\n",
        "\t\t       10, 13, 30, 16, 19, 21, 32, 23, 25, 24, 33,  8, 28, 31,  7,  0, 27])\n",
        "\n",
        "–ö–æ–¥–æ–≤–∞—è —è—á–µ–π–∫–∞ <ohwwKj2K2_DP>\n",
        "# %% [code]\n",
        "model.to(device)\n",
        "–ü–æ–ª—É—á–µ–Ω—ã –≤—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.\n",
        "15KB\n",
        "\ttext/plain\n",
        "\t\tViTForImageClassification(\n",
        "\t\t  (vit): ViTModel(\n",
        "\t\t    (embeddings): ViTEmbeddings(\n",
        "\t\t      (patch_embeddings): ViTPatchEmbeddings(\n",
        "\t\t        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
        "\t\t      )\n",
        "\t\t      (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t    )\n",
        "\t\t    (encoder): ViTEncoder(\n",
        "\t\t      (layer): ModuleList(\n",
        "\t\t        (0): ViTLayer(\n",
        "\t\t          (attention): ViTAttention(\n",
        "\t\t            (attention): ViTSelfAttention(\n",
        "\t\t              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (key): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t            (output): ViTSelfOutput(\n",
        "\t\t              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t          )\n",
        "\t\t          (intermediate): ViTIntermediate(\n",
        "\t\t            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "\t\t            (intermediate_act_fn): GELUActivation()\n",
        "\t\t          )\n",
        "\t\t          (output): ViTOutput(\n",
        "\t\t            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "\t\t            (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t          )\n",
        "\t\t          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t        )\n",
        "\t\t        (1): ViTLayer(\n",
        "\t\t          (attention): ViTAttention(\n",
        "\t\t            (attention): ViTSelfAttention(\n",
        "\t\t              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (key): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t            (output): ViTSelfOutput(\n",
        "\t\t              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t          )\n",
        "\t\t          (intermediate): ViTIntermediate(\n",
        "\t\t            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "\t\t            (intermediate_act_fn): GELUActivation()\n",
        "\t\t          )\n",
        "\t\t          (output): ViTOutput(\n",
        "\t\t            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "\t\t            (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t          )\n",
        "\t\t          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t        )\n",
        "\t\t        (2): ViTLayer(\n",
        "\t\t          (attention): ViTAttention(\n",
        "\t\t            (attention): ViTSelfAttention(\n",
        "\t\t              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (key): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t            (output): ViTSelfOutput(\n",
        "\t\t              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t          )\n",
        "\t\t          (intermediate): ViTIntermediate(\n",
        "\t\t            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "\t\t            (intermediate_act_fn): GELUActivation()\n",
        "\t\t          )\n",
        "\t\t          (output): ViTOutput(\n",
        "\t\t            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "\t\t            (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t          )\n",
        "\t\t          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t        )\n",
        "\t\t        (3): ViTLayer(\n",
        "\t\t          (attention): ViTAttention(\n",
        "\t\t            (attention): ViTSelfAttention(\n",
        "\t\t              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (key): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t            (output): ViTSelfOutput(\n",
        "\t\t              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t          )\n",
        "\t\t          (intermediate): ViTIntermediate(\n",
        "\t\t            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "\t\t            (intermediate_act_fn): GELUActivation()\n",
        "\t\t          )\n",
        "\t\t          (output): ViTOutput(\n",
        "\t\t            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "\t\t            (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t          )\n",
        "\t\t          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t        )\n",
        "\t\t        (4): ViTLayer(\n",
        "\t\t          (attention): ViTAttention(\n",
        "\t\t            (attention): ViTSelfAttention(\n",
        "\t\t              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (key): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t            (output): ViTSelfOutput(\n",
        "\t\t              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t          )\n",
        "\t\t          (intermediate): ViTIntermediate(\n",
        "\t\t            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "\t\t            (intermediate_act_fn): GELUActivation()\n",
        "\t\t          )\n",
        "\t\t          (output): ViTOutput(\n",
        "\t\t            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "\t\t            (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t          )\n",
        "\t\t          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t        )\n",
        "\t\t        (5): ViTLayer(\n",
        "\t\t          (attention): ViTAttention(\n",
        "\t\t            (attention): ViTSelfAttention(\n",
        "\t\t              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (key): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t            (output): ViTSelfOutput(\n",
        "\t\t              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t          )\n",
        "\t\t          (intermediate): ViTIntermediate(\n",
        "\t\t            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "\t\t            (intermediate_act_fn): GELUActivation()\n",
        "\t\t          )\n",
        "\t\t          (output): ViTOutput(\n",
        "\t\t            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "\t\t            (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t          )\n",
        "\t\t          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t        )\n",
        "\t\t        (6): ViTLayer(\n",
        "\t\t          (attention): ViTAttention(\n",
        "\t\t            (attention): ViTSelfAttention(\n",
        "\t\t              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (key): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t            (output): ViTSelfOutput(\n",
        "\t\t              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t          )\n",
        "\t\t          (intermediate): ViTIntermediate(\n",
        "\t\t            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "\t\t            (intermediate_act_fn): GELUActivation()\n",
        "\t\t          )\n",
        "\t\t          (output): ViTOutput(\n",
        "\t\t            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "\t\t            (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t          )\n",
        "\t\t          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t        )\n",
        "\t\t        (7): ViTLayer(\n",
        "\t\t          (attention): ViTAttention(\n",
        "\t\t            (attention): ViTSelfAttention(\n",
        "\t\t              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (key): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t            (output): ViTSelfOutput(\n",
        "\t\t              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t          )\n",
        "\t\t          (intermediate): ViTIntermediate(\n",
        "\t\t            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "\t\t            (intermediate_act_fn): GELUActivation()\n",
        "\t\t          )\n",
        "\t\t          (output): ViTOutput(\n",
        "\t\t            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "\t\t            (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t          )\n",
        "\t\t          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t        )\n",
        "\t\t        (8): ViTLayer(\n",
        "\t\t          (attention): ViTAttention(\n",
        "\t\t            (attention): ViTSelfAttention(\n",
        "\t\t              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (key): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t            (output): ViTSelfOutput(\n",
        "\t\t              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t          )\n",
        "\t\t          (intermediate): ViTIntermediate(\n",
        "\t\t            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "\t\t            (intermediate_act_fn): GELUActivation()\n",
        "\t\t          )\n",
        "\t\t          (output): ViTOutput(\n",
        "\t\t            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "\t\t            (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t          )\n",
        "\t\t          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t        )\n",
        "\t\t        (9): ViTLayer(\n",
        "\t\t          (attention): ViTAttention(\n",
        "\t\t            (attention): ViTSelfAttention(\n",
        "\t\t              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (key): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t            (output): ViTSelfOutput(\n",
        "\t\t              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t          )\n",
        "\t\t          (intermediate): ViTIntermediate(\n",
        "\t\t            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "\t\t            (intermediate_act_fn): GELUActivation()\n",
        "\t\t          )\n",
        "\t\t          (output): ViTOutput(\n",
        "\t\t            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "\t\t            (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t          )\n",
        "\t\t          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t        )\n",
        "\t\t        (10): ViTLayer(\n",
        "\t\t          (attention): ViTAttention(\n",
        "\t\t            (attention): ViTSelfAttention(\n",
        "\t\t              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (key): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t            (output): ViTSelfOutput(\n",
        "\t\t              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t          )\n",
        "\t\t          (intermediate): ViTIntermediate(\n",
        "\t\t            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "\t\t            (intermediate_act_fn): GELUActivation()\n",
        "\t\t          )\n",
        "\t\t          (output): ViTOutput(\n",
        "\t\t            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "\t\t            (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t          )\n",
        "\t\t          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t        )\n",
        "\t\t        (11): ViTLayer(\n",
        "\t\t          (attention): ViTAttention(\n",
        "\t\t            (attention): ViTSelfAttention(\n",
        "\t\t              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (key): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t            (output): ViTSelfOutput(\n",
        "\t\t              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t          )\n",
        "\t\t          (intermediate): ViTIntermediate(\n",
        "\t\t            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "\t\t            (intermediate_act_fn): GELUActivation()\n",
        "\t\t          )\n",
        "\t\t          (output): ViTOutput(\n",
        "\t\t            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "\t\t            (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t          )\n",
        "\t\t          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t        )\n",
        "\t\t      )\n",
        "\t\t    )\n",
        "\t\t    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t  )\n",
        "\t\t  (classifier): Linear(in_features=768, out_features=34, bias=True)\n",
        "\t\t)\n",
        "\n",
        "–ö–æ–¥–æ–≤–∞—è —è—á–µ–π–∫–∞ <yWXgxQx03B1U>\n",
        "# %% [code]\n",
        "def compute_metrics(p):\n",
        "    return accuracy.compute(predictions=np.argmax(p.predictions, axis=1), references=p.label_ids)\n",
        "\n",
        "–ö–æ–¥–æ–≤–∞—è —è—á–µ–π–∫–∞ <L8WY_ZJz3GS8>\n",
        "# %% [code]\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./vit-base-letters\",\n",
        "    per_device_train_batch_size=16,\n",
        "    num_train_epochs=4,\n",
        "    fp16=True,\n",
        "    logging_steps=100,\n",
        "    learning_rate=2e-4,\n",
        "    save_total_limit=2,\n",
        "    remove_unused_columns=False,\n",
        "    push_to_hub=False,\n",
        "    report_to='tensorboard',\n",
        ")\n",
        "\n",
        "–ö–æ–¥–æ–≤–∞—è —è—á–µ–π–∫–∞ <Qrkmfykz3OFD>\n",
        "# %% [code]\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=collate_fn,\n",
        "    compute_metrics=compute_metrics,\n",
        "    train_dataset=train_dataset,\n",
        "    tokenizer=vit_feature_extractor,\n",
        ")\n",
        "–ü–æ–ª—É—á–µ–Ω—ã –≤—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.\n",
        "0KB\n",
        "\tStream\n",
        "\t\tUsing cuda_amp half precision backend\n",
        "\n",
        "–ö–æ–¥–æ–≤–∞—è —è—á–µ–π–∫–∞ <JfEXIoUC3srf>\n",
        "# %% [code]\n",
        "trainer.train()\n",
        "trainer.save_model('/content/drive/MyDrive/models/vit-base-letters_14032023')\n",
        "–ü–æ–ª—É—á–µ–Ω—ã –≤—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.\n",
        "29KB\n",
        "\tStream\n",
        "\t\t***** Running training *****\n",
        "\t\t  Num examples = 70761\n",
        "\t\t  Num Epochs = 4\n",
        "\t\t  Instantaneous batch size per device = 16\n",
        "\t\t  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
        "\t\t  Gradient Accumulation steps = 1\n",
        "\t\t  Total optimization steps = 17692\n",
        "\t\t  Number of trainable parameters = 85824802\n",
        "\t\tSaving model checkpoint to ./vit-base-letters/checkpoint-500\n",
        "\t\tConfiguration saved in ./vit-base-letters/checkpoint-500/config.json\n",
        "\t\tModel weights saved in ./vit-base-letters/checkpoint-500/pytorch_model.bin\n",
        "\t\tImage processor saved in ./vit-base-letters/checkpoint-500/preprocessor_config.json\n",
        "\t\tSaving model checkpoint to ./vit-base-letters/checkpoint-1000\n",
        "\t\tConfiguration saved in ./vit-base-letters/checkpoint-1000/config.json\n",
        "\t\tModel weights saved in ./vit-base-letters/checkpoint-1000/pytorch_model.bin\n",
        "\t\tImage processor saved in ./vit-base-letters/checkpoint-1000/preprocessor_config.json\n",
        "\t\tSaving model checkpoint to ./vit-base-letters/checkpoint-1500\n",
        "\t\tConfiguration saved in ./vit-base-letters/checkpoint-1500/config.json\n",
        "\t\tModel weights saved in ./vit-base-letters/checkpoint-1500/pytorch_model.bin\n",
        "\t\tImage processor saved in ./vit-base-letters/checkpoint-1500/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [vit-base-letters/checkpoint-500] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./vit-base-letters/checkpoint-2000\n",
        "\t\tConfiguration saved in ./vit-base-letters/checkpoint-2000/config.json\n",
        "\t\tModel weights saved in ./vit-base-letters/checkpoint-2000/pytorch_model.bin\n",
        "\t\tImage processor saved in ./vit-base-letters/checkpoint-2000/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [vit-base-letters/checkpoint-1000] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./vit-base-letters/checkpoint-2500\n",
        "\t\tConfiguration saved in ./vit-base-letters/checkpoint-2500/config.json\n",
        "\t\tModel weights saved in ./vit-base-letters/checkpoint-2500/pytorch_model.bin\n",
        "\t\tImage processor saved in ./vit-base-letters/checkpoint-2500/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [vit-base-letters/checkpoint-1500] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./vit-base-letters/checkpoint-3000\n",
        "\t\tConfiguration saved in ./vit-base-letters/checkpoint-3000/config.json\n",
        "\t\tModel weights saved in ./vit-base-letters/checkpoint-3000/pytorch_model.bin\n",
        "\t\tImage processor saved in ./vit-base-letters/checkpoint-3000/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [vit-base-letters/checkpoint-2000] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./vit-base-letters/checkpoint-3500\n",
        "\t\tConfiguration saved in ./vit-base-letters/checkpoint-3500/config.json\n",
        "\t\tModel weights saved in ./vit-base-letters/checkpoint-3500/pytorch_model.bin\n",
        "\t\tImage processor saved in ./vit-base-letters/checkpoint-3500/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [vit-base-letters/checkpoint-2500] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./vit-base-letters/checkpoint-4000\n",
        "\t\tConfiguration saved in ./vit-base-letters/checkpoint-4000/config.json\n",
        "\t\tModel weights saved in ./vit-base-letters/checkpoint-4000/pytorch_model.bin\n",
        "\t\tImage processor saved in ./vit-base-letters/checkpoint-4000/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [vit-base-letters/checkpoint-3000] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./vit-base-letters/checkpoint-4500\n",
        "\t\tConfiguration saved in ./vit-base-letters/checkpoint-4500/config.json\n",
        "\t\tModel weights saved in ./vit-base-letters/checkpoint-4500/pytorch_model.bin\n",
        "\t\tImage processor saved in ./vit-base-letters/checkpoint-4500/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [vit-base-letters/checkpoint-3500] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./vit-base-letters/checkpoint-5000\n",
        "\t\tConfiguration saved in ./vit-base-letters/checkpoint-5000/config.json\n",
        "\t\tModel weights saved in ./vit-base-letters/checkpoint-5000/pytorch_model.bin\n",
        "\t\tImage processor saved in ./vit-base-letters/checkpoint-5000/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [vit-base-letters/checkpoint-4000] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./vit-base-letters/checkpoint-5500\n",
        "\t\tConfiguration saved in ./vit-base-letters/checkpoint-5500/config.json\n",
        "\t\tModel weights saved in ./vit-base-letters/checkpoint-5500/pytorch_model.bin\n",
        "\t\tImage processor saved in ./vit-base-letters/checkpoint-5500/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [vit-base-letters/checkpoint-4500] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./vit-base-letters/checkpoint-6000\n",
        "\t\tConfiguration saved in ./vit-base-letters/checkpoint-6000/config.json\n",
        "\t\tModel weights saved in ./vit-base-letters/checkpoint-6000/pytorch_model.bin\n",
        "\t\tImage processor saved in ./vit-base-letters/checkpoint-6000/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [vit-base-letters/checkpoint-5000] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./vit-base-letters/checkpoint-6500\n",
        "\t\tConfiguration saved in ./vit-base-letters/checkpoint-6500/config.json\n",
        "\t\tModel weights saved in ./vit-base-letters/checkpoint-6500/pytorch_model.bin\n",
        "\t\tImage processor saved in ./vit-base-letters/checkpoint-6500/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [vit-base-letters/checkpoint-5500] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./vit-base-letters/checkpoint-7000\n",
        "\t\tConfiguration saved in ./vit-base-letters/checkpoint-7000/config.json\n",
        "\t\tModel weights saved in ./vit-base-letters/checkpoint-7000/pytorch_model.bin\n",
        "\t\tImage processor saved in ./vit-base-letters/checkpoint-7000/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [vit-base-letters/checkpoint-6000] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./vit-base-letters/checkpoint-7500\n",
        "\t\tConfiguration saved in ./vit-base-letters/checkpoint-7500/config.json\n",
        "\t\tModel weights saved in ./vit-base-letters/checkpoint-7500/pytorch_model.bin\n",
        "\t\tImage processor saved in ./vit-base-letters/checkpoint-7500/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [vit-base-letters/checkpoint-6500] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./vit-base-letters/checkpoint-8000\n",
        "\t\tConfiguration saved in ./vit-base-letters/checkpoint-8000/config.json\n",
        "\t\tModel weights saved in ./vit-base-letters/checkpoint-8000/pytorch_model.bin\n",
        "\t\tImage processor saved in ./vit-base-letters/checkpoint-8000/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [vit-base-letters/checkpoint-7000] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./vit-base-letters/checkpoint-8500\n",
        "\t\tConfiguration saved in ./vit-base-letters/checkpoint-8500/config.json\n",
        "\t\tModel weights saved in ./vit-base-letters/checkpoint-8500/pytorch_model.bin\n",
        "\t\tImage processor saved in ./vit-base-letters/checkpoint-8500/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [vit-base-letters/checkpoint-7500] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./vit-base-letters/checkpoint-9000\n",
        "\t\tConfiguration saved in ./vit-base-letters/checkpoint-9000/config.json\n",
        "\t\tModel weights saved in ./vit-base-letters/checkpoint-9000/pytorch_model.bin\n",
        "\t\tImage processor saved in ./vit-base-letters/checkpoint-9000/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [vit-base-letters/checkpoint-8000] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./vit-base-letters/checkpoint-9500\n",
        "\t\tConfiguration saved in ./vit-base-letters/checkpoint-9500/config.json\n",
        "\t\tModel weights saved in ./vit-base-letters/checkpoint-9500/pytorch_model.bin\n",
        "\t\tImage processor saved in ./vit-base-letters/checkpoint-9500/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [vit-base-letters/checkpoint-8500] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./vit-base-letters/checkpoint-10000\n",
        "\t\tConfiguration saved in ./vit-base-letters/checkpoint-10000/config.json\n",
        "\t\tModel weights saved in ./vit-base-letters/checkpoint-10000/pytorch_model.bin\n",
        "\t\tImage processor saved in ./vit-base-letters/checkpoint-10000/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [vit-base-letters/checkpoint-9000] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./vit-base-letters/checkpoint-10500\n",
        "\t\tConfiguration saved in ./vit-base-letters/checkpoint-10500/config.json\n",
        "\t\tModel weights saved in ./vit-base-letters/checkpoint-10500/pytorch_model.bin\n",
        "\t\tImage processor saved in ./vit-base-letters/checkpoint-10500/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [vit-base-letters/checkpoint-9500] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./vit-base-letters/checkpoint-11000\n",
        "\t\tConfiguration saved in ./vit-base-letters/checkpoint-11000/config.json\n",
        "\t\tModel weights saved in ./vit-base-letters/checkpoint-11000/pytorch_model.bin\n",
        "\t\tImage processor saved in ./vit-base-letters/checkpoint-11000/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [vit-base-letters/checkpoint-10000] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./vit-base-letters/checkpoint-11500\n",
        "\t\tConfiguration saved in ./vit-base-letters/checkpoint-11500/config.json\n",
        "\t\tModel weights saved in ./vit-base-letters/checkpoint-11500/pytorch_model.bin\n",
        "\t\tImage processor saved in ./vit-base-letters/checkpoint-11500/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [vit-base-letters/checkpoint-10500] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./vit-base-letters/checkpoint-12000\n",
        "\t\tConfiguration saved in ./vit-base-letters/checkpoint-12000/config.json\n",
        "\t\tModel weights saved in ./vit-base-letters/checkpoint-12000/pytorch_model.bin\n",
        "\t\tImage processor saved in ./vit-base-letters/checkpoint-12000/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [vit-base-letters/checkpoint-11000] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./vit-base-letters/checkpoint-12500\n",
        "\t\tConfiguration saved in ./vit-base-letters/checkpoint-12500/config.json\n",
        "\t\tModel weights saved in ./vit-base-letters/checkpoint-12500/pytorch_model.bin\n",
        "\t\tImage processor saved in ./vit-base-letters/checkpoint-12500/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [vit-base-letters/checkpoint-11500] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./vit-base-letters/checkpoint-13000\n",
        "\t\tConfiguration saved in ./vit-base-letters/checkpoint-13000/config.json\n",
        "\t\tModel weights saved in ./vit-base-letters/checkpoint-13000/pytorch_model.bin\n",
        "\t\tImage processor saved in ./vit-base-letters/checkpoint-13000/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [vit-base-letters/checkpoint-12000] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./vit-base-letters/checkpoint-13500\n",
        "\t\tConfiguration saved in ./vit-base-letters/checkpoint-13500/config.json\n",
        "\t\tModel weights saved in ./vit-base-letters/checkpoint-13500/pytorch_model.bin\n",
        "\t\tImage processor saved in ./vit-base-letters/checkpoint-13500/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [vit-base-letters/checkpoint-12500] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./vit-base-letters/checkpoint-14000\n",
        "\t\tConfiguration saved in ./vit-base-letters/checkpoint-14000/config.json\n",
        "\t\tModel weights saved in ./vit-base-letters/checkpoint-14000/pytorch_model.bin\n",
        "\t\tImage processor saved in ./vit-base-letters/checkpoint-14000/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [vit-base-letters/checkpoint-13000] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./vit-base-letters/checkpoint-14500\n",
        "\t\tConfiguration saved in ./vit-base-letters/checkpoint-14500/config.json\n",
        "\t\tModel weights saved in ./vit-base-letters/checkpoint-14500/pytorch_model.bin\n",
        "\t\tImage processor saved in ./vit-base-letters/checkpoint-14500/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [vit-base-letters/checkpoint-13500] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./vit-base-letters/checkpoint-15000\n",
        "\t\tConfiguration saved in ./vit-base-letters/checkpoint-15000/config.json\n",
        "\t\tModel weights saved in ./vit-base-letters/checkpoint-15000/pytorch_model.bin\n",
        "\t\tImage processor saved in ./vit-base-letters/checkpoint-15000/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [vit-base-letters/checkpoint-14000] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./vit-base-letters/checkpoint-15500\n",
        "\t\tConfiguration saved in ./vit-base-letters/checkpoint-15500/config.json\n",
        "\t\tModel weights saved in ./vit-base-letters/checkpoint-15500/pytorch_model.bin\n",
        "\t\tImage processor saved in ./vit-base-letters/checkpoint-15500/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [vit-base-letters/checkpoint-14500] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./vit-base-letters/checkpoint-16000\n",
        "\t\tConfiguration saved in ./vit-base-letters/checkpoint-16000/config.json\n",
        "\t\tModel weights saved in ./vit-base-letters/checkpoint-16000/pytorch_model.bin\n",
        "\t\tImage processor saved in ./vit-base-letters/checkpoint-16000/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [vit-base-letters/checkpoint-15000] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./vit-base-letters/checkpoint-16500\n",
        "\t\tConfiguration saved in ./vit-base-letters/checkpoint-16500/config.json\n",
        "\t\tModel weights saved in ./vit-base-letters/checkpoint-16500/pytorch_model.bin\n",
        "\t\tImage processor saved in ./vit-base-letters/checkpoint-16500/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [vit-base-letters/checkpoint-15500] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./vit-base-letters/checkpoint-17000\n",
        "\t\tConfiguration saved in ./vit-base-letters/checkpoint-17000/config.json\n",
        "\t\tModel weights saved in ./vit-base-letters/checkpoint-17000/pytorch_model.bin\n",
        "\t\tImage processor saved in ./vit-base-letters/checkpoint-17000/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [vit-base-letters/checkpoint-16000] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./vit-base-letters/checkpoint-17500\n",
        "\t\tConfiguration saved in ./vit-base-letters/checkpoint-17500/config.json\n",
        "\t\tModel weights saved in ./vit-base-letters/checkpoint-17500/pytorch_model.bin\n",
        "\t\tImage processor saved in ./vit-base-letters/checkpoint-17500/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [vit-base-letters/checkpoint-16500] due to args.save_total_limit\n",
        "\n",
        "\n",
        "\t\tTraining completed. Do not forget to share your model on huggingface.co/models =)\n",
        "\n",
        "\n",
        "\t\tSaving model checkpoint to /content/drive/MyDrive/models/vit-base-letters_14032023\n",
        "\t\tConfiguration saved in /content/drive/MyDrive/models/vit-base-letters_14032023/config.json\n",
        "\t\tModel weights saved in /content/drive/MyDrive/models/vit-base-letters_14032023/pytorch_model.bin\n",
        "\t\tImage processor saved in /content/drive/MyDrive/models/vit-base-letters_14032023/preprocessor_config.json\n",
        "\n",
        "–¢–µ–∫—Å—Ç–æ–≤–∞—è —è—á–µ–π–∫–∞ <ADXPe0H2SeUt>\n",
        "# %% [markdown]\n",
        "### Evaluation\n",
        "\n",
        "–ö–æ–¥–æ–≤–∞—è —è—á–µ–π–∫–∞ <2q0sgFBPSctF>\n",
        "# %% [code]\n",
        "new_vit_model_name = '/content/drive/MyDrive/models/vit-base-letters_14032023'\n",
        "vit_model_name = 'google/vit-base-patch16-224'\n",
        "vit_feature_extractor = ViTFeatureExtractor.from_pretrained(vit_model_name)\n",
        "\n",
        "–ö–æ–¥–æ–≤–∞—è —è—á–µ–π–∫–∞ <RkvOjqInSaKD>\n",
        "# %% [code]\n",
        "labels = df.labels.unique()\n",
        "\n",
        "new_model = ViTForImageClassification.from_pretrained(\n",
        "    new_vit_model_name,\n",
        "    num_labels=labels.shape[0],\n",
        "    id2label={str(i): c for i, c in enumerate(le.classes_)},  # to convert id of class to real label\n",
        "    label2id={c: str(i) for i, c in enumerate(le.classes_)},\n",
        "    ignore_mismatched_sizes=True\n",
        ")\n",
        "\n",
        "–ö–æ–¥–æ–≤–∞—è —è—á–µ–π–∫–∞ <F1m7olkatPMC>\n",
        "# %% [code]\n",
        "# sample\n",
        "outputs = new_model(test_dataset[0]['pixel_values'])\n",
        "logits = outputs.logits\n",
        "predicted_class_idx = logits.argmax(-1).item()\n",
        "print(\"Predicted class:\", new_model.config.id2label[str(predicted_class_idx)])\n",
        "print('Actual class:', new_model.config.id2label[str(test_dataset[0]['labels'])])\n",
        "–ü–æ–ª—É—á–µ–Ω—ã –≤—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.\n",
        "0KB\n",
        "\tStream\n",
        "\t\tPredicted class: –∫\n",
        "\t\tActual class: –µ\n",
        "\n",
        "–¢–µ–∫—Å—Ç–æ–≤–∞—è —è—á–µ–π–∫–∞ <LGlHhQNxvChT>\n",
        "# %% [markdown]\n",
        ":(\n",
        "\n",
        "–ö–æ–¥–æ–≤–∞—è —è—á–µ–π–∫–∞ <gjBAq6b4vuDo>\n",
        "# %% [code]\n",
        "new_model.to(device)\n",
        "–ü–æ–ª—É—á–µ–Ω—ã –≤—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.\n",
        "15KB\n",
        "\ttext/plain\n",
        "\t\tViTForImageClassification(\n",
        "\t\t  (vit): ViTModel(\n",
        "\t\t    (embeddings): ViTEmbeddings(\n",
        "\t\t      (patch_embeddings): ViTPatchEmbeddings(\n",
        "\t\t        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
        "\t\t      )\n",
        "\t\t      (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t    )\n",
        "\t\t    (encoder): ViTEncoder(\n",
        "\t\t      (layer): ModuleList(\n",
        "\t\t        (0): ViTLayer(\n",
        "\t\t          (attention): ViTAttention(\n",
        "\t\t            (attention): ViTSelfAttention(\n",
        "\t\t              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (key): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t            (output): ViTSelfOutput(\n",
        "\t\t              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t          )\n",
        "\t\t          (intermediate): ViTIntermediate(\n",
        "\t\t            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "\t\t            (intermediate_act_fn): GELUActivation()\n",
        "\t\t          )\n",
        "\t\t          (output): ViTOutput(\n",
        "\t\t            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "\t\t            (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t          )\n",
        "\t\t          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t        )\n",
        "\t\t        (1): ViTLayer(\n",
        "\t\t          (attention): ViTAttention(\n",
        "\t\t            (attention): ViTSelfAttention(\n",
        "\t\t              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (key): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t            (output): ViTSelfOutput(\n",
        "\t\t              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t          )\n",
        "\t\t          (intermediate): ViTIntermediate(\n",
        "\t\t            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "\t\t            (intermediate_act_fn): GELUActivation()\n",
        "\t\t          )\n",
        "\t\t          (output): ViTOutput(\n",
        "\t\t            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "\t\t            (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t          )\n",
        "\t\t          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t        )\n",
        "\t\t        (2): ViTLayer(\n",
        "\t\t          (attention): ViTAttention(\n",
        "\t\t            (attention): ViTSelfAttention(\n",
        "\t\t              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (key): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t            (output): ViTSelfOutput(\n",
        "\t\t              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t          )\n",
        "\t\t          (intermediate): ViTIntermediate(\n",
        "\t\t            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "\t\t            (intermediate_act_fn): GELUActivation()\n",
        "\t\t          )\n",
        "\t\t          (output): ViTOutput(\n",
        "\t\t            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "\t\t            (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t          )\n",
        "\t\t          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t        )\n",
        "\t\t        (3): ViTLayer(\n",
        "\t\t          (attention): ViTAttention(\n",
        "\t\t            (attention): ViTSelfAttention(\n",
        "\t\t              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (key): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t            (output): ViTSelfOutput(\n",
        "\t\t              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t          )\n",
        "\t\t          (intermediate): ViTIntermediate(\n",
        "\t\t            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "\t\t            (intermediate_act_fn): GELUActivation()\n",
        "\t\t          )\n",
        "\t\t          (output): ViTOutput(\n",
        "\t\t            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "\t\t            (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t          )\n",
        "\t\t          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t        )\n",
        "\t\t        (4): ViTLayer(\n",
        "\t\t          (attention): ViTAttention(\n",
        "\t\t            (attention): ViTSelfAttention(\n",
        "\t\t              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (key): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t            (output): ViTSelfOutput(\n",
        "\t\t              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t          )\n",
        "\t\t          (intermediate): ViTIntermediate(\n",
        "\t\t            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "\t\t            (intermediate_act_fn): GELUActivation()\n",
        "\t\t          )\n",
        "\t\t          (output): ViTOutput(\n",
        "\t\t            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "\t\t            (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t          )\n",
        "\t\t          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t        )\n",
        "\t\t        (5): ViTLayer(\n",
        "\t\t          (attention): ViTAttention(\n",
        "\t\t            (attention): ViTSelfAttention(\n",
        "\t\t              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (key): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t            (output): ViTSelfOutput(\n",
        "\t\t              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t          )\n",
        "\t\t          (intermediate): ViTIntermediate(\n",
        "\t\t            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "\t\t            (intermediate_act_fn): GELUActivation()\n",
        "\t\t          )\n",
        "\t\t          (output): ViTOutput(\n",
        "\t\t            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "\t\t            (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t          )\n",
        "\t\t          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t        )\n",
        "\t\t        (6): ViTLayer(\n",
        "\t\t          (attention): ViTAttention(\n",
        "\t\t            (attention): ViTSelfAttention(\n",
        "\t\t              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (key): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t            (output): ViTSelfOutput(\n",
        "\t\t              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t          )\n",
        "\t\t          (intermediate): ViTIntermediate(\n",
        "\t\t            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "\t\t            (intermediate_act_fn): GELUActivation()\n",
        "\t\t          )\n",
        "\t\t          (output): ViTOutput(\n",
        "\t\t            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "\t\t            (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t          )\n",
        "\t\t          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t        )\n",
        "\t\t        (7): ViTLayer(\n",
        "\t\t          (attention): ViTAttention(\n",
        "\t\t            (attention): ViTSelfAttention(\n",
        "\t\t              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (key): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t            (output): ViTSelfOutput(\n",
        "\t\t              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t          )\n",
        "\t\t          (intermediate): ViTIntermediate(\n",
        "\t\t            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "\t\t            (intermediate_act_fn): GELUActivation()\n",
        "\t\t          )\n",
        "\t\t          (output): ViTOutput(\n",
        "\t\t            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "\t\t            (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t          )\n",
        "\t\t          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t        )\n",
        "\t\t        (8): ViTLayer(\n",
        "\t\t          (attention): ViTAttention(\n",
        "\t\t            (attention): ViTSelfAttention(\n",
        "\t\t              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (key): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t            (output): ViTSelfOutput(\n",
        "\t\t              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t          )\n",
        "\t\t          (intermediate): ViTIntermediate(\n",
        "\t\t            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "\t\t            (intermediate_act_fn): GELUActivation()\n",
        "\t\t          )\n",
        "\t\t          (output): ViTOutput(\n",
        "\t\t            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "\t\t            (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t          )\n",
        "\t\t          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t        )\n",
        "\t\t        (9): ViTLayer(\n",
        "\t\t          (attention): ViTAttention(\n",
        "\t\t            (attention): ViTSelfAttention(\n",
        "\t\t              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (key): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t            (output): ViTSelfOutput(\n",
        "\t\t              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t          )\n",
        "\t\t          (intermediate): ViTIntermediate(\n",
        "\t\t            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "\t\t            (intermediate_act_fn): GELUActivation()\n",
        "\t\t          )\n",
        "\t\t          (output): ViTOutput(\n",
        "\t\t            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "\t\t            (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t          )\n",
        "\t\t          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t        )\n",
        "\t\t        (10): ViTLayer(\n",
        "\t\t          (attention): ViTAttention(\n",
        "\t\t            (attention): ViTSelfAttention(\n",
        "\t\t              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (key): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t            (output): ViTSelfOutput(\n",
        "\t\t              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t          )\n",
        "\t\t          (intermediate): ViTIntermediate(\n",
        "\t\t            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "\t\t            (intermediate_act_fn): GELUActivation()\n",
        "\t\t          )\n",
        "\t\t          (output): ViTOutput(\n",
        "\t\t            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "\t\t            (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t          )\n",
        "\t\t          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t        )\n",
        "\t\t        (11): ViTLayer(\n",
        "\t\t          (attention): ViTAttention(\n",
        "\t\t            (attention): ViTSelfAttention(\n",
        "\t\t              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (key): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t            (output): ViTSelfOutput(\n",
        "\t\t              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t          )\n",
        "\t\t          (intermediate): ViTIntermediate(\n",
        "\t\t            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "\t\t            (intermediate_act_fn): GELUActivation()\n",
        "\t\t          )\n",
        "\t\t          (output): ViTOutput(\n",
        "\t\t            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "\t\t            (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t          )\n",
        "\t\t          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t        )\n",
        "\t\t      )\n",
        "\t\t    )\n",
        "\t\t    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t  )\n",
        "\t\t  (classifier): Linear(in_features=768, out_features=34, bias=True)\n",
        "\t\t)\n",
        "\n",
        "–ö–æ–¥–æ–≤–∞—è —è—á–µ–π–∫–∞ <b1ZpZjXBu9FU>\n",
        "# %% [code]\n",
        "actual_labels = []\n",
        "predicted_labels = []\n",
        "\n",
        "for item in tqdm(test_dataset):\n",
        "    pixels = item['pixel_values'].to(device)\n",
        "    outputs = new_model(pixels)\n",
        "    logits = outputs.logits.to('cpu')\n",
        "    predicted_class_idx = logits.argmax(-1).item()\n",
        "    predicted_labels.append(new_model.config.id2label[str(predicted_class_idx)])\n",
        "    actual_labels.append(new_model.config.id2label[str(item['labels'])])\n",
        "–ü–æ–ª—É—á–µ–Ω—ã –≤—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.\n",
        "0KB\n",
        "\ttext/plain\n",
        "\t\t0%|          | 0/17691 [00:00<?, ?it/s]\n",
        "\n",
        "–ö–æ–¥–æ–≤–∞—è —è—á–µ–π–∫–∞ <rnR0y1_9fu3c>\n",
        "# %% [code]\n",
        "len(predicted_labels)\n",
        "–ü–æ–ª—É—á–µ–Ω—ã –≤—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.\n",
        "0KB\n",
        "\ttext/plain\n",
        "\t\t17691\n",
        "\n",
        "–ö–æ–¥–æ–≤–∞—è —è—á–µ–π–∫–∞ <-cr62wvFWuTM>\n",
        "# %% [code]\n",
        "print(classification_report(actual_labels, predicted_labels))\n",
        "–ü–æ–ª—É—á–µ–Ω—ã –≤—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.\n",
        "2KB\n",
        "\tStream\n",
        "\t\tprecision    recall  f1-score   support\n",
        "\n",
        "\t\t           -       0.00      0.00      0.00         1\n",
        "\t\t           –∞       0.98      0.98      0.98      3042\n",
        "\t\t           –±       0.97      0.83      0.90       264\n",
        "\t\t           –≤       0.99      0.97      0.98      1026\n",
        "\t\t           –≥       0.91      0.95      0.93       265\n",
        "\t\t           –¥       0.95      0.91      0.93       441\n",
        "\t\t           –µ       0.97      0.97      0.97      1165\n",
        "\t\t           –∂       0.98      0.93      0.95        85\n",
        "\t\t           –∑       0.88      0.95      0.92       154\n",
        "\t\t           –∏       0.88      0.96      0.92      1389\n",
        "\t\t           –π       0.62      0.15      0.24       137\n",
        "\t\t           –∫       0.97      0.97      0.97       934\n",
        "\t\t           –ª       0.94      0.96      0.95       979\n",
        "\t\t           –º       0.97      0.95      0.96       518\n",
        "\t\t           –Ω       0.95      0.97      0.96      1427\n",
        "\t\t           –æ       0.98      0.98      0.98      1361\n",
        "\t\t           –ø       0.96      0.94      0.95       238\n",
        "\t\t           —Ä       0.98      0.97      0.97       996\n",
        "\t\t           —Å       0.97      0.97      0.97       703\n",
        "\t\t           —Ç       0.98      0.97      0.98       654\n",
        "\t\t           —É       0.95      0.95      0.95       311\n",
        "\t\t           —Ñ       0.94      0.93      0.94       111\n",
        "\t\t           —Ö       0.99      0.98      0.99       111\n",
        "\t\t           —Ü       0.90      0.88      0.89        80\n",
        "\t\t           —á       0.87      0.85      0.86       150\n",
        "\t\t           —à       0.94      0.94      0.94       154\n",
        "\t\t           —â       0.70      1.00      0.82         7\n",
        "\t\t           —ä       0.00      0.00      0.00         1\n",
        "\t\t           —ã       0.87      0.23      0.37        56\n",
        "\t\t           —å       0.76      0.96      0.85       285\n",
        "\t\t           —ç       0.83      1.00      0.91        30\n",
        "\t\t           —é       0.96      0.95      0.95        97\n",
        "\t\t           —è       0.99      0.98      0.98       513\n",
        "\t\t           —ë       0.00      0.00      0.00         6\n",
        "\n",
        "\t\t    accuracy                           0.95     17691\n",
        "\t\t   macro avg       0.84      0.82      0.82     17691\n",
        "\t\tweighted avg       0.95      0.95      0.95     17691\n",
        "\n",
        "–¢–µ–∫—Å—Ç–æ–≤–∞—è —è—á–µ–π–∫–∞ <2lWiIpAQtoiu>\n",
        "# %% [markdown]\n",
        "## BEiT Model\n",
        "\n",
        "–ö–æ–¥–æ–≤–∞—è —è—á–µ–π–∫–∞ <l7IGXb06trLe>\n",
        "# %% [code]\n",
        "beit_model_name = 'microsoft/beit-base-patch16-224'\n",
        "beit_feature_extractor = BeitFeatureExtractor.from_pretrained(beit_model_name)\n",
        "–ü–æ–ª—É—á–µ–Ω—ã –≤—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.\n",
        "2KB\n",
        "\tStream\n",
        "\t\tloading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--beit-base-patch16-224/snapshots/5f0307aec45cf4c914ee8700108b779fb8390654/preprocessor_config.json\n",
        "\t\tsize should be a dictionary on of the following set of keys: ({'width', 'height'}, {'shortest_edge'}, {'longest_edge', 'shortest_edge'}), got 224. Converted to {'height': 224, 'width': 224}.\n",
        "\t\tcrop_size should be a dictionary on of the following set of keys: ({'width', 'height'}, {'shortest_edge'}, {'longest_edge', 'shortest_edge'}), got 224. Converted to {'height': 224, 'width': 224}.\n",
        "\t\tImage processor BeitFeatureExtractor {\n",
        "\t\t  \"crop_size\": {\n",
        "\t\t    \"height\": 224,\n",
        "\t\t    \"width\": 224\n",
        "\t\t  },\n",
        "\t\t  \"do_center_crop\": false,\n",
        "\t\t  \"do_normalize\": true,\n",
        "\t\t  \"do_reduce_labels\": false,\n",
        "\t\t  \"do_rescale\": true,\n",
        "\t\t  \"do_resize\": true,\n",
        "\t\t  \"feature_extractor_type\": \"BeitFeatureExtractor\",\n",
        "\t\t  \"image_mean\": [\n",
        "\t\t    0.5,\n",
        "\t\t    0.5,\n",
        "\t\t    0.5\n",
        "\t\t  ],\n",
        "\t\t  \"image_processor_type\": \"BeitFeatureExtractor\",\n",
        "\t\t  \"image_std\": [\n",
        "\t\t    0.5,\n",
        "\t\t    0.5,\n",
        "\t\t    0.5\n",
        "\t\t  ],\n",
        "\t\t  \"resample\": 2,\n",
        "\t\t  \"rescale_factor\": 0.00392156862745098,\n",
        "\t\t  \"size\": {\n",
        "\t\t    \"height\": 224,\n",
        "\t\t    \"width\": 224\n",
        "\t\t  }\n",
        "\t\t}\n",
        "\ttext/plain\n",
        "\t\tDownloading (‚Ä¶)rocessor_config.json:   0%|          | 0.00/276 [00:00<?, ?B/s]\n",
        "\n",
        "–ö–æ–¥–æ–≤–∞—è —è—á–µ–π–∫–∞ <OeVlJYTxewY_>\n",
        "# %% [code]\n",
        "train_dataset = LettersDataset(train_df, beit_feature_extractor, transform=process_example)\n",
        "test_dataset = LettersDataset(test_df, beit_feature_extractor, transform=process_example)\n",
        "\n",
        "–ö–æ–¥–æ–≤–∞—è —è—á–µ–π–∫–∞ <I4s8lO3Xey9K>\n",
        "# %% [code]\n",
        "labels = df.labels.unique()\n",
        "\n",
        "model = BeitForImageClassification.from_pretrained(\n",
        "    beit_model_name,\n",
        "    num_labels=labels.shape[0],\n",
        "    id2label={str(i): c for i, c in enumerate(le.classes_)},  # to convert id of class to real label\n",
        "    label2id={c: str(i) for i, c in enumerate(le.classes_)},\n",
        "    ignore_mismatched_sizes=True\n",
        ")\n",
        "–ü–æ–ª—É—á–µ–Ω—ã –≤—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.\n",
        "5KB\n",
        "\tStream\n",
        "\t\tloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--beit-base-patch16-224/snapshots/5f0307aec45cf4c914ee8700108b779fb8390654/config.json\n",
        "\t\tModel config BeitConfig {\n",
        "\t\t  \"architectures\": [\n",
        "\t\t    \"BeitForImageClassification\"\n",
        "\t\t  ],\n",
        "\t\t  \"attention_probs_dropout_prob\": 0.0,\n",
        "\t\t  \"auxiliary_channels\": 256,\n",
        "\t\t  \"auxiliary_concat_input\": false,\n",
        "\t\t  \"auxiliary_loss_weight\": 0.4,\n",
        "\t\t  \"auxiliary_num_convs\": 1,\n",
        "\t\t  \"drop_path_rate\": 0.1,\n",
        "\t\t  \"hidden_act\": \"gelu\",\n",
        "\t\t  \"hidden_dropout_prob\": 0.0,\n",
        "\t\t  \"hidden_size\": 768,\n",
        "\t\t  \"id2label\": {\n",
        "\t\t    \"0\": \"-\",\n",
        "\t\t    \"1\": \"\\u0430\",\n",
        "\t\t    \"10\": \"\\u0439\",\n",
        "\t\t    \"11\": \"\\u043a\",\n",
        "\t\t    \"12\": \"\\u043b\",\n",
        "\t\t    \"13\": \"\\u043c\",\n",
        "\t\t    \"14\": \"\\u043d\",\n",
        "\t\t    \"15\": \"\\u043e\",\n",
        "\t\t    \"16\": \"\\u043f\",\n",
        "\t\t    \"17\": \"\\u0440\",\n",
        "\t\t    \"18\": \"\\u0441\",\n",
        "\t\t    \"19\": \"\\u0442\",\n",
        "\t\t    \"2\": \"\\u0431\",\n",
        "\t\t    \"20\": \"\\u0443\",\n",
        "\t\t    \"21\": \"\\u0444\",\n",
        "\t\t    \"22\": \"\\u0445\",\n",
        "\t\t    \"23\": \"\\u0446\",\n",
        "\t\t    \"24\": \"\\u0447\",\n",
        "\t\t    \"25\": \"\\u0448\",\n",
        "\t\t    \"26\": \"\\u0449\",\n",
        "\t\t    \"27\": \"\\u044a\",\n",
        "\t\t    \"28\": \"\\u044b\",\n",
        "\t\t    \"29\": \"\\u044c\",\n",
        "\t\t    \"3\": \"\\u0432\",\n",
        "\t\t    \"30\": \"\\u044d\",\n",
        "\t\t    \"31\": \"\\u044e\",\n",
        "\t\t    \"32\": \"\\u044f\",\n",
        "\t\t    \"33\": \"\\u0451\",\n",
        "\t\t    \"4\": \"\\u0433\",\n",
        "\t\t    \"5\": \"\\u0434\",\n",
        "\t\t    \"6\": \"\\u0435\",\n",
        "\t\t    \"7\": \"\\u0436\",\n",
        "\t\t    \"8\": \"\\u0437\",\n",
        "\t\t    \"9\": \"\\u0438\"\n",
        "\t\t  },\n",
        "\t\t  \"image_size\": 224,\n",
        "\t\t  \"initializer_range\": 0.02,\n",
        "\t\t  \"intermediate_size\": 3072,\n",
        "\t\t  \"label2id\": {\n",
        "\t\t    \"-\": \"0\",\n",
        "\t\t    \"\\u0430\": \"1\",\n",
        "\t\t    \"\\u0431\": \"2\",\n",
        "\t\t    \"\\u0432\": \"3\",\n",
        "\t\t    \"\\u0433\": \"4\",\n",
        "\t\t    \"\\u0434\": \"5\",\n",
        "\t\t    \"\\u0435\": \"6\",\n",
        "\t\t    \"\\u0436\": \"7\",\n",
        "\t\t    \"\\u0437\": \"8\",\n",
        "\t\t    \"\\u0438\": \"9\",\n",
        "\t\t    \"\\u0439\": \"10\",\n",
        "\t\t    \"\\u043a\": \"11\",\n",
        "\t\t    \"\\u043b\": \"12\",\n",
        "\t\t    \"\\u043c\": \"13\",\n",
        "\t\t    \"\\u043d\": \"14\",\n",
        "\t\t    \"\\u043e\": \"15\",\n",
        "\t\t    \"\\u043f\": \"16\",\n",
        "\t\t    \"\\u0440\": \"17\",\n",
        "\t\t    \"\\u0441\": \"18\",\n",
        "\t\t    \"\\u0442\": \"19\",\n",
        "\t\t    \"\\u0443\": \"20\",\n",
        "\t\t    \"\\u0444\": \"21\",\n",
        "\t\t    \"\\u0445\": \"22\",\n",
        "\t\t    \"\\u0446\": \"23\",\n",
        "\t\t    \"\\u0447\": \"24\",\n",
        "\t\t    \"\\u0448\": \"25\",\n",
        "\t\t    \"\\u0449\": \"26\",\n",
        "\t\t    \"\\u044a\": \"27\",\n",
        "\t\t    \"\\u044b\": \"28\",\n",
        "\t\t    \"\\u044c\": \"29\",\n",
        "\t\t    \"\\u044d\": \"30\",\n",
        "\t\t    \"\\u044e\": \"31\",\n",
        "\t\t    \"\\u044f\": \"32\",\n",
        "\t\t    \"\\u0451\": \"33\"\n",
        "\t\t  },\n",
        "\t\t  \"layer_norm_eps\": 1e-12,\n",
        "\t\t  \"layer_scale_init_value\": 0.1,\n",
        "\t\t  \"model_type\": \"beit\",\n",
        "\t\t  \"num_attention_heads\": 12,\n",
        "\t\t  \"num_channels\": 3,\n",
        "\t\t  \"num_hidden_layers\": 12,\n",
        "\t\t  \"out_indices\": [\n",
        "\t\t    3,\n",
        "\t\t    5,\n",
        "\t\t    7,\n",
        "\t\t    11\n",
        "\t\t  ],\n",
        "\t\t  \"patch_size\": 16,\n",
        "\t\t  \"pool_scales\": [\n",
        "\t\t    1,\n",
        "\t\t    2,\n",
        "\t\t    3,\n",
        "\t\t    6\n",
        "\t\t  ],\n",
        "\t\t  \"semantic_loss_ignore_index\": 255,\n",
        "\t\t  \"torch_dtype\": \"float32\",\n",
        "\t\t  \"transformers_version\": \"4.26.1\",\n",
        "\t\t  \"use_absolute_position_embeddings\": false,\n",
        "\t\t  \"use_auxiliary_head\": true,\n",
        "\t\t  \"use_mask_token\": false,\n",
        "\t\t  \"use_mean_pooling\": true,\n",
        "\t\t  \"use_relative_position_bias\": true,\n",
        "\t\t  \"use_shared_relative_position_bias\": false,\n",
        "\t\t  \"vocab_size\": 8192\n",
        "\t\t}\n",
        "\t\tloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--microsoft--beit-base-patch16-224/snapshots/5f0307aec45cf4c914ee8700108b779fb8390654/pytorch_model.bin\n",
        "\t\tAll model checkpoint weights were used when initializing BeitForImageClassification.\n",
        "\n",
        "\t\tSome weights of BeitForImageClassification were not initialized from the model checkpoint at microsoft/beit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
        "\t\t- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([34, 768]) in the model instantiated\n",
        "\t\t- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([34]) in the model instantiated\n",
        "\t\tYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
        "\ttext/plain\n",
        "\t\tDownloading (‚Ä¶)lve/main/config.json:   0%|          | 0.00/69.9k [00:00<?, ?B/s]\n",
        "\t\tDownloading pytorch_model.bin:   0%|          | 0.00/350M [00:00<?, ?B/s]\n",
        "\n",
        "–ö–æ–¥–æ–≤–∞—è —è—á–µ–π–∫–∞ <SP_k0hvUfBTp>\n",
        "# %% [code]\n",
        "model.to(device)\n",
        "–ü–æ–ª—É—á–µ–Ω—ã –≤—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.\n",
        "17KB\n",
        "\ttext/plain\n",
        "\t\tBeitForImageClassification(\n",
        "\t\t  (beit): BeitModel(\n",
        "\t\t    (embeddings): BeitEmbeddings(\n",
        "\t\t      (patch_embeddings): BeitPatchEmbeddings(\n",
        "\t\t        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
        "\t\t      )\n",
        "\t\t      (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t    )\n",
        "\t\t    (encoder): BeitEncoder(\n",
        "\t\t      (layer): ModuleList(\n",
        "\t\t        (0): BeitLayer(\n",
        "\t\t          (attention): BeitAttention(\n",
        "\t\t            (attention): BeitSelfAttention(\n",
        "\t\t              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (key): Linear(in_features=768, out_features=768, bias=False)\n",
        "\t\t              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t              (relative_position_bias): BeitRelativePositionBias()\n",
        "\t\t            )\n",
        "\t\t            (output): BeitSelfOutput(\n",
        "\t\t              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t          )\n",
        "\t\t          (intermediate): BeitIntermediate(\n",
        "\t\t            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "\t\t            (intermediate_act_fn): GELUActivation()\n",
        "\t\t          )\n",
        "\t\t          (output): BeitOutput(\n",
        "\t\t            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "\t\t            (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t          )\n",
        "\t\t          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t          (drop_path): Identity()\n",
        "\t\t          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t        )\n",
        "\t\t        (1): BeitLayer(\n",
        "\t\t          (attention): BeitAttention(\n",
        "\t\t            (attention): BeitSelfAttention(\n",
        "\t\t              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (key): Linear(in_features=768, out_features=768, bias=False)\n",
        "\t\t              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t              (relative_position_bias): BeitRelativePositionBias()\n",
        "\t\t            )\n",
        "\t\t            (output): BeitSelfOutput(\n",
        "\t\t              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t          )\n",
        "\t\t          (intermediate): BeitIntermediate(\n",
        "\t\t            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "\t\t            (intermediate_act_fn): GELUActivation()\n",
        "\t\t          )\n",
        "\t\t          (output): BeitOutput(\n",
        "\t\t            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "\t\t            (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t          )\n",
        "\t\t          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t          (drop_path): BeitDropPath(p=0.00909090880304575)\n",
        "\t\t          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t        )\n",
        "\t\t        (2): BeitLayer(\n",
        "\t\t          (attention): BeitAttention(\n",
        "\t\t            (attention): BeitSelfAttention(\n",
        "\t\t              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (key): Linear(in_features=768, out_features=768, bias=False)\n",
        "\t\t              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t              (relative_position_bias): BeitRelativePositionBias()\n",
        "\t\t            )\n",
        "\t\t            (output): BeitSelfOutput(\n",
        "\t\t              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t          )\n",
        "\t\t          (intermediate): BeitIntermediate(\n",
        "\t\t            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "\t\t            (intermediate_act_fn): GELUActivation()\n",
        "\t\t          )\n",
        "\t\t          (output): BeitOutput(\n",
        "\t\t            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "\t\t            (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t          )\n",
        "\t\t          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t          (drop_path): BeitDropPath(p=0.0181818176060915)\n",
        "\t\t          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t        )\n",
        "\t\t        (3): BeitLayer(\n",
        "\t\t          (attention): BeitAttention(\n",
        "\t\t            (attention): BeitSelfAttention(\n",
        "\t\t              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (key): Linear(in_features=768, out_features=768, bias=False)\n",
        "\t\t              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t              (relative_position_bias): BeitRelativePositionBias()\n",
        "\t\t            )\n",
        "\t\t            (output): BeitSelfOutput(\n",
        "\t\t              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t          )\n",
        "\t\t          (intermediate): BeitIntermediate(\n",
        "\t\t            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "\t\t            (intermediate_act_fn): GELUActivation()\n",
        "\t\t          )\n",
        "\t\t          (output): BeitOutput(\n",
        "\t\t            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "\t\t            (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t          )\n",
        "\t\t          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t          (drop_path): BeitDropPath(p=0.027272727340459824)\n",
        "\t\t          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t        )\n",
        "\t\t        (4): BeitLayer(\n",
        "\t\t          (attention): BeitAttention(\n",
        "\t\t            (attention): BeitSelfAttention(\n",
        "\t\t              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (key): Linear(in_features=768, out_features=768, bias=False)\n",
        "\t\t              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t              (relative_position_bias): BeitRelativePositionBias()\n",
        "\t\t            )\n",
        "\t\t            (output): BeitSelfOutput(\n",
        "\t\t              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t          )\n",
        "\t\t          (intermediate): BeitIntermediate(\n",
        "\t\t            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "\t\t            (intermediate_act_fn): GELUActivation()\n",
        "\t\t          )\n",
        "\t\t          (output): BeitOutput(\n",
        "\t\t            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "\t\t            (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t          )\n",
        "\t\t          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t          (drop_path): BeitDropPath(p=0.036363635212183)\n",
        "\t\t          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t        )\n",
        "\t\t        (5): BeitLayer(\n",
        "\t\t          (attention): BeitAttention(\n",
        "\t\t            (attention): BeitSelfAttention(\n",
        "\t\t              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (key): Linear(in_features=768, out_features=768, bias=False)\n",
        "\t\t              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t              (relative_position_bias): BeitRelativePositionBias()\n",
        "\t\t            )\n",
        "\t\t            (output): BeitSelfOutput(\n",
        "\t\t              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t          )\n",
        "\t\t          (intermediate): BeitIntermediate(\n",
        "\t\t            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "\t\t            (intermediate_act_fn): GELUActivation()\n",
        "\t\t          )\n",
        "\t\t          (output): BeitOutput(\n",
        "\t\t            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "\t\t            (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t          )\n",
        "\t\t          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t          (drop_path): BeitDropPath(p=0.045454543083906174)\n",
        "\t\t          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t        )\n",
        "\t\t        (6): BeitLayer(\n",
        "\t\t          (attention): BeitAttention(\n",
        "\t\t            (attention): BeitSelfAttention(\n",
        "\t\t              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (key): Linear(in_features=768, out_features=768, bias=False)\n",
        "\t\t              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t              (relative_position_bias): BeitRelativePositionBias()\n",
        "\t\t            )\n",
        "\t\t            (output): BeitSelfOutput(\n",
        "\t\t              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t          )\n",
        "\t\t          (intermediate): BeitIntermediate(\n",
        "\t\t            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "\t\t            (intermediate_act_fn): GELUActivation()\n",
        "\t\t          )\n",
        "\t\t          (output): BeitOutput(\n",
        "\t\t            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "\t\t            (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t          )\n",
        "\t\t          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t          (drop_path): BeitDropPath(p=0.054545458406209946)\n",
        "\t\t          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t        )\n",
        "\t\t        (7): BeitLayer(\n",
        "\t\t          (attention): BeitAttention(\n",
        "\t\t            (attention): BeitSelfAttention(\n",
        "\t\t              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (key): Linear(in_features=768, out_features=768, bias=False)\n",
        "\t\t              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t              (relative_position_bias): BeitRelativePositionBias()\n",
        "\t\t            )\n",
        "\t\t            (output): BeitSelfOutput(\n",
        "\t\t              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t          )\n",
        "\t\t          (intermediate): BeitIntermediate(\n",
        "\t\t            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "\t\t            (intermediate_act_fn): GELUActivation()\n",
        "\t\t          )\n",
        "\t\t          (output): BeitOutput(\n",
        "\t\t            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "\t\t            (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t          )\n",
        "\t\t          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t          (drop_path): BeitDropPath(p=0.06363636255264282)\n",
        "\t\t          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t        )\n",
        "\t\t        (8): BeitLayer(\n",
        "\t\t          (attention): BeitAttention(\n",
        "\t\t            (attention): BeitSelfAttention(\n",
        "\t\t              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (key): Linear(in_features=768, out_features=768, bias=False)\n",
        "\t\t              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t              (relative_position_bias): BeitRelativePositionBias()\n",
        "\t\t            )\n",
        "\t\t            (output): BeitSelfOutput(\n",
        "\t\t              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t          )\n",
        "\t\t          (intermediate): BeitIntermediate(\n",
        "\t\t            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "\t\t            (intermediate_act_fn): GELUActivation()\n",
        "\t\t          )\n",
        "\t\t          (output): BeitOutput(\n",
        "\t\t            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "\t\t            (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t          )\n",
        "\t\t          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t          (drop_path): BeitDropPath(p=0.0727272778749466)\n",
        "\t\t          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t        )\n",
        "\t\t        (9): BeitLayer(\n",
        "\t\t          (attention): BeitAttention(\n",
        "\t\t            (attention): BeitSelfAttention(\n",
        "\t\t              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (key): Linear(in_features=768, out_features=768, bias=False)\n",
        "\t\t              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t              (relative_position_bias): BeitRelativePositionBias()\n",
        "\t\t            )\n",
        "\t\t            (output): BeitSelfOutput(\n",
        "\t\t              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t          )\n",
        "\t\t          (intermediate): BeitIntermediate(\n",
        "\t\t            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "\t\t            (intermediate_act_fn): GELUActivation()\n",
        "\t\t          )\n",
        "\t\t          (output): BeitOutput(\n",
        "\t\t            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "\t\t            (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t          )\n",
        "\t\t          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t          (drop_path): BeitDropPath(p=0.08181818574666977)\n",
        "\t\t          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t        )\n",
        "\t\t        (10): BeitLayer(\n",
        "\t\t          (attention): BeitAttention(\n",
        "\t\t            (attention): BeitSelfAttention(\n",
        "\t\t              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (key): Linear(in_features=768, out_features=768, bias=False)\n",
        "\t\t              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t              (relative_position_bias): BeitRelativePositionBias()\n",
        "\t\t            )\n",
        "\t\t            (output): BeitSelfOutput(\n",
        "\t\t              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t          )\n",
        "\t\t          (intermediate): BeitIntermediate(\n",
        "\t\t            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "\t\t            (intermediate_act_fn): GELUActivation()\n",
        "\t\t          )\n",
        "\t\t          (output): BeitOutput(\n",
        "\t\t            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "\t\t            (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t          )\n",
        "\t\t          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t          (drop_path): BeitDropPath(p=0.09090909361839294)\n",
        "\t\t          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t        )\n",
        "\t\t        (11): BeitLayer(\n",
        "\t\t          (attention): BeitAttention(\n",
        "\t\t            (attention): BeitSelfAttention(\n",
        "\t\t              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (key): Linear(in_features=768, out_features=768, bias=False)\n",
        "\t\t              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t              (relative_position_bias): BeitRelativePositionBias()\n",
        "\t\t            )\n",
        "\t\t            (output): BeitSelfOutput(\n",
        "\t\t              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t          )\n",
        "\t\t          (intermediate): BeitIntermediate(\n",
        "\t\t            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "\t\t            (intermediate_act_fn): GELUActivation()\n",
        "\t\t          )\n",
        "\t\t          (output): BeitOutput(\n",
        "\t\t            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "\t\t            (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t          )\n",
        "\t\t          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t          (drop_path): BeitDropPath(p=0.10000000149011612)\n",
        "\t\t          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t        )\n",
        "\t\t      )\n",
        "\t\t    )\n",
        "\t\t    (layernorm): Identity()\n",
        "\t\t    (pooler): BeitPooler(\n",
        "\t\t      (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t    )\n",
        "\t\t  )\n",
        "\t\t  (classifier): Linear(in_features=768, out_features=34, bias=True)\n",
        "\t\t)\n",
        "\n",
        "–ö–æ–¥–æ–≤–∞—è —è—á–µ–π–∫–∞ <NRgTP-YZezIU>\n",
        "# %% [code]\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./beit-base-letters\",\n",
        "    per_device_train_batch_size=16,\n",
        "    num_train_epochs=4,\n",
        "    fp16=True,\n",
        "    logging_steps=100,\n",
        "    learning_rate=2e-4,\n",
        "    save_total_limit=2,\n",
        "    remove_unused_columns=False,\n",
        "    push_to_hub=False,\n",
        "    report_to='tensorboard',\n",
        ")\n",
        "–ü–æ–ª—É—á–µ–Ω—ã –≤—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.\n",
        "0KB\n",
        "\tStream\n",
        "\t\tPyTorch: setting up devices\n",
        "\n",
        "–ö–æ–¥–æ–≤–∞—è —è—á–µ–π–∫–∞ <9-ZI6WOnfFM->\n",
        "# %% [code]\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=collate_fn,\n",
        "    compute_metrics=compute_metrics,\n",
        "    train_dataset=train_dataset,\n",
        "    tokenizer=beit_feature_extractor,\n",
        ")\n",
        "–ü–æ–ª—É—á–µ–Ω—ã –≤—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.\n",
        "0KB\n",
        "\tStream\n",
        "\t\tUsing cuda_amp half precision backend\n",
        "\n",
        "–ö–æ–¥–æ–≤–∞—è —è—á–µ–π–∫–∞ <u1v3Ymm9fSiN>\n",
        "# %% [code]\n",
        "trainer.train()\n",
        "trainer.save_model('/content/drive/MyDrive/models/beit-base-letters_14032023')\n",
        "–ü–æ–ª—É—á–µ–Ω—ã –≤—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.\n",
        "29KB\n",
        "\tStream\n",
        "\t\t***** Running training *****\n",
        "\t\t  Num examples = 70761\n",
        "\t\t  Num Epochs = 4\n",
        "\t\t  Instantaneous batch size per device = 16\n",
        "\t\t  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
        "\t\t  Gradient Accumulation steps = 1\n",
        "\t\t  Total optimization steps = 17692\n",
        "\t\t  Number of trainable parameters = 85788130\n",
        "\t\tSaving model checkpoint to ./beit-base-letters/checkpoint-500\n",
        "\t\tConfiguration saved in ./beit-base-letters/checkpoint-500/config.json\n",
        "\t\tModel weights saved in ./beit-base-letters/checkpoint-500/pytorch_model.bin\n",
        "\t\tImage processor saved in ./beit-base-letters/checkpoint-500/preprocessor_config.json\n",
        "\t\tSaving model checkpoint to ./beit-base-letters/checkpoint-1000\n",
        "\t\tConfiguration saved in ./beit-base-letters/checkpoint-1000/config.json\n",
        "\t\tModel weights saved in ./beit-base-letters/checkpoint-1000/pytorch_model.bin\n",
        "\t\tImage processor saved in ./beit-base-letters/checkpoint-1000/preprocessor_config.json\n",
        "\t\tSaving model checkpoint to ./beit-base-letters/checkpoint-1500\n",
        "\t\tConfiguration saved in ./beit-base-letters/checkpoint-1500/config.json\n",
        "\t\tModel weights saved in ./beit-base-letters/checkpoint-1500/pytorch_model.bin\n",
        "\t\tImage processor saved in ./beit-base-letters/checkpoint-1500/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [beit-base-letters/checkpoint-500] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./beit-base-letters/checkpoint-2000\n",
        "\t\tConfiguration saved in ./beit-base-letters/checkpoint-2000/config.json\n",
        "\t\tModel weights saved in ./beit-base-letters/checkpoint-2000/pytorch_model.bin\n",
        "\t\tImage processor saved in ./beit-base-letters/checkpoint-2000/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [beit-base-letters/checkpoint-1000] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./beit-base-letters/checkpoint-2500\n",
        "\t\tConfiguration saved in ./beit-base-letters/checkpoint-2500/config.json\n",
        "\t\tModel weights saved in ./beit-base-letters/checkpoint-2500/pytorch_model.bin\n",
        "\t\tImage processor saved in ./beit-base-letters/checkpoint-2500/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [beit-base-letters/checkpoint-1500] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./beit-base-letters/checkpoint-3000\n",
        "\t\tConfiguration saved in ./beit-base-letters/checkpoint-3000/config.json\n",
        "\t\tModel weights saved in ./beit-base-letters/checkpoint-3000/pytorch_model.bin\n",
        "\t\tImage processor saved in ./beit-base-letters/checkpoint-3000/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [beit-base-letters/checkpoint-2000] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./beit-base-letters/checkpoint-3500\n",
        "\t\tConfiguration saved in ./beit-base-letters/checkpoint-3500/config.json\n",
        "\t\tModel weights saved in ./beit-base-letters/checkpoint-3500/pytorch_model.bin\n",
        "\t\tImage processor saved in ./beit-base-letters/checkpoint-3500/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [beit-base-letters/checkpoint-2500] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./beit-base-letters/checkpoint-4000\n",
        "\t\tConfiguration saved in ./beit-base-letters/checkpoint-4000/config.json\n",
        "\t\tModel weights saved in ./beit-base-letters/checkpoint-4000/pytorch_model.bin\n",
        "\t\tImage processor saved in ./beit-base-letters/checkpoint-4000/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [beit-base-letters/checkpoint-3000] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./beit-base-letters/checkpoint-4500\n",
        "\t\tConfiguration saved in ./beit-base-letters/checkpoint-4500/config.json\n",
        "\t\tModel weights saved in ./beit-base-letters/checkpoint-4500/pytorch_model.bin\n",
        "\t\tImage processor saved in ./beit-base-letters/checkpoint-4500/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [beit-base-letters/checkpoint-3500] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./beit-base-letters/checkpoint-5000\n",
        "\t\tConfiguration saved in ./beit-base-letters/checkpoint-5000/config.json\n",
        "\t\tModel weights saved in ./beit-base-letters/checkpoint-5000/pytorch_model.bin\n",
        "\t\tImage processor saved in ./beit-base-letters/checkpoint-5000/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [beit-base-letters/checkpoint-4000] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./beit-base-letters/checkpoint-5500\n",
        "\t\tConfiguration saved in ./beit-base-letters/checkpoint-5500/config.json\n",
        "\t\tModel weights saved in ./beit-base-letters/checkpoint-5500/pytorch_model.bin\n",
        "\t\tImage processor saved in ./beit-base-letters/checkpoint-5500/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [beit-base-letters/checkpoint-4500] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./beit-base-letters/checkpoint-6000\n",
        "\t\tConfiguration saved in ./beit-base-letters/checkpoint-6000/config.json\n",
        "\t\tModel weights saved in ./beit-base-letters/checkpoint-6000/pytorch_model.bin\n",
        "\t\tImage processor saved in ./beit-base-letters/checkpoint-6000/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [beit-base-letters/checkpoint-5000] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./beit-base-letters/checkpoint-6500\n",
        "\t\tConfiguration saved in ./beit-base-letters/checkpoint-6500/config.json\n",
        "\t\tModel weights saved in ./beit-base-letters/checkpoint-6500/pytorch_model.bin\n",
        "\t\tImage processor saved in ./beit-base-letters/checkpoint-6500/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [beit-base-letters/checkpoint-5500] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./beit-base-letters/checkpoint-7000\n",
        "\t\tConfiguration saved in ./beit-base-letters/checkpoint-7000/config.json\n",
        "\t\tModel weights saved in ./beit-base-letters/checkpoint-7000/pytorch_model.bin\n",
        "\t\tImage processor saved in ./beit-base-letters/checkpoint-7000/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [beit-base-letters/checkpoint-6000] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./beit-base-letters/checkpoint-7500\n",
        "\t\tConfiguration saved in ./beit-base-letters/checkpoint-7500/config.json\n",
        "\t\tModel weights saved in ./beit-base-letters/checkpoint-7500/pytorch_model.bin\n",
        "\t\tImage processor saved in ./beit-base-letters/checkpoint-7500/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [beit-base-letters/checkpoint-6500] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./beit-base-letters/checkpoint-8000\n",
        "\t\tConfiguration saved in ./beit-base-letters/checkpoint-8000/config.json\n",
        "\t\tModel weights saved in ./beit-base-letters/checkpoint-8000/pytorch_model.bin\n",
        "\t\tImage processor saved in ./beit-base-letters/checkpoint-8000/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [beit-base-letters/checkpoint-7000] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./beit-base-letters/checkpoint-8500\n",
        "\t\tConfiguration saved in ./beit-base-letters/checkpoint-8500/config.json\n",
        "\t\tModel weights saved in ./beit-base-letters/checkpoint-8500/pytorch_model.bin\n",
        "\t\tImage processor saved in ./beit-base-letters/checkpoint-8500/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [beit-base-letters/checkpoint-7500] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./beit-base-letters/checkpoint-9000\n",
        "\t\tConfiguration saved in ./beit-base-letters/checkpoint-9000/config.json\n",
        "\t\tModel weights saved in ./beit-base-letters/checkpoint-9000/pytorch_model.bin\n",
        "\t\tImage processor saved in ./beit-base-letters/checkpoint-9000/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [beit-base-letters/checkpoint-8000] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./beit-base-letters/checkpoint-9500\n",
        "\t\tConfiguration saved in ./beit-base-letters/checkpoint-9500/config.json\n",
        "\t\tModel weights saved in ./beit-base-letters/checkpoint-9500/pytorch_model.bin\n",
        "\t\tImage processor saved in ./beit-base-letters/checkpoint-9500/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [beit-base-letters/checkpoint-8500] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./beit-base-letters/checkpoint-10000\n",
        "\t\tConfiguration saved in ./beit-base-letters/checkpoint-10000/config.json\n",
        "\t\tModel weights saved in ./beit-base-letters/checkpoint-10000/pytorch_model.bin\n",
        "\t\tImage processor saved in ./beit-base-letters/checkpoint-10000/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [beit-base-letters/checkpoint-9000] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./beit-base-letters/checkpoint-10500\n",
        "\t\tConfiguration saved in ./beit-base-letters/checkpoint-10500/config.json\n",
        "\t\tModel weights saved in ./beit-base-letters/checkpoint-10500/pytorch_model.bin\n",
        "\t\tImage processor saved in ./beit-base-letters/checkpoint-10500/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [beit-base-letters/checkpoint-9500] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./beit-base-letters/checkpoint-11000\n",
        "\t\tConfiguration saved in ./beit-base-letters/checkpoint-11000/config.json\n",
        "\t\tModel weights saved in ./beit-base-letters/checkpoint-11000/pytorch_model.bin\n",
        "\t\tImage processor saved in ./beit-base-letters/checkpoint-11000/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [beit-base-letters/checkpoint-10000] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./beit-base-letters/checkpoint-11500\n",
        "\t\tConfiguration saved in ./beit-base-letters/checkpoint-11500/config.json\n",
        "\t\tModel weights saved in ./beit-base-letters/checkpoint-11500/pytorch_model.bin\n",
        "\t\tImage processor saved in ./beit-base-letters/checkpoint-11500/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [beit-base-letters/checkpoint-10500] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./beit-base-letters/checkpoint-12000\n",
        "\t\tConfiguration saved in ./beit-base-letters/checkpoint-12000/config.json\n",
        "\t\tModel weights saved in ./beit-base-letters/checkpoint-12000/pytorch_model.bin\n",
        "\t\tImage processor saved in ./beit-base-letters/checkpoint-12000/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [beit-base-letters/checkpoint-11000] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./beit-base-letters/checkpoint-12500\n",
        "\t\tConfiguration saved in ./beit-base-letters/checkpoint-12500/config.json\n",
        "\t\tModel weights saved in ./beit-base-letters/checkpoint-12500/pytorch_model.bin\n",
        "\t\tImage processor saved in ./beit-base-letters/checkpoint-12500/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [beit-base-letters/checkpoint-11500] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./beit-base-letters/checkpoint-13000\n",
        "\t\tConfiguration saved in ./beit-base-letters/checkpoint-13000/config.json\n",
        "\t\tModel weights saved in ./beit-base-letters/checkpoint-13000/pytorch_model.bin\n",
        "\t\tImage processor saved in ./beit-base-letters/checkpoint-13000/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [beit-base-letters/checkpoint-12000] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./beit-base-letters/checkpoint-13500\n",
        "\t\tConfiguration saved in ./beit-base-letters/checkpoint-13500/config.json\n",
        "\t\tModel weights saved in ./beit-base-letters/checkpoint-13500/pytorch_model.bin\n",
        "\t\tImage processor saved in ./beit-base-letters/checkpoint-13500/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [beit-base-letters/checkpoint-12500] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./beit-base-letters/checkpoint-14000\n",
        "\t\tConfiguration saved in ./beit-base-letters/checkpoint-14000/config.json\n",
        "\t\tModel weights saved in ./beit-base-letters/checkpoint-14000/pytorch_model.bin\n",
        "\t\tImage processor saved in ./beit-base-letters/checkpoint-14000/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [beit-base-letters/checkpoint-13000] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./beit-base-letters/checkpoint-14500\n",
        "\t\tConfiguration saved in ./beit-base-letters/checkpoint-14500/config.json\n",
        "\t\tModel weights saved in ./beit-base-letters/checkpoint-14500/pytorch_model.bin\n",
        "\t\tImage processor saved in ./beit-base-letters/checkpoint-14500/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [beit-base-letters/checkpoint-13500] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./beit-base-letters/checkpoint-15000\n",
        "\t\tConfiguration saved in ./beit-base-letters/checkpoint-15000/config.json\n",
        "\t\tModel weights saved in ./beit-base-letters/checkpoint-15000/pytorch_model.bin\n",
        "\t\tImage processor saved in ./beit-base-letters/checkpoint-15000/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [beit-base-letters/checkpoint-14000] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./beit-base-letters/checkpoint-15500\n",
        "\t\tConfiguration saved in ./beit-base-letters/checkpoint-15500/config.json\n",
        "\t\tModel weights saved in ./beit-base-letters/checkpoint-15500/pytorch_model.bin\n",
        "\t\tImage processor saved in ./beit-base-letters/checkpoint-15500/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [beit-base-letters/checkpoint-14500] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./beit-base-letters/checkpoint-16000\n",
        "\t\tConfiguration saved in ./beit-base-letters/checkpoint-16000/config.json\n",
        "\t\tModel weights saved in ./beit-base-letters/checkpoint-16000/pytorch_model.bin\n",
        "\t\tImage processor saved in ./beit-base-letters/checkpoint-16000/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [beit-base-letters/checkpoint-15000] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./beit-base-letters/checkpoint-16500\n",
        "\t\tConfiguration saved in ./beit-base-letters/checkpoint-16500/config.json\n",
        "\t\tModel weights saved in ./beit-base-letters/checkpoint-16500/pytorch_model.bin\n",
        "\t\tImage processor saved in ./beit-base-letters/checkpoint-16500/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [beit-base-letters/checkpoint-15500] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./beit-base-letters/checkpoint-17000\n",
        "\t\tConfiguration saved in ./beit-base-letters/checkpoint-17000/config.json\n",
        "\t\tModel weights saved in ./beit-base-letters/checkpoint-17000/pytorch_model.bin\n",
        "\t\tImage processor saved in ./beit-base-letters/checkpoint-17000/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [beit-base-letters/checkpoint-16000] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./beit-base-letters/checkpoint-17500\n",
        "\t\tConfiguration saved in ./beit-base-letters/checkpoint-17500/config.json\n",
        "\t\tModel weights saved in ./beit-base-letters/checkpoint-17500/pytorch_model.bin\n",
        "\t\tImage processor saved in ./beit-base-letters/checkpoint-17500/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [beit-base-letters/checkpoint-16500] due to args.save_total_limit\n",
        "\n",
        "\n",
        "\t\tTraining completed. Do not forget to share your model on huggingface.co/models =)\n",
        "\n",
        "\n",
        "\t\tSaving model checkpoint to /content/drive/MyDrive/models/beit-base-letters_14032023\n",
        "\t\tConfiguration saved in /content/drive/MyDrive/models/beit-base-letters_14032023/config.json\n",
        "\t\tModel weights saved in /content/drive/MyDrive/models/beit-base-letters_14032023/pytorch_model.bin\n",
        "\t\tImage processor saved in /content/drive/MyDrive/models/beit-base-letters_14032023/preprocessor_config.json\n",
        "\n",
        "–¢–µ–∫—Å—Ç–æ–≤–∞—è —è—á–µ–π–∫–∞ <lnzLhVS8jJ1X>\n",
        "# %% [markdown]\n",
        "### Evaluation\n",
        "\n",
        "–ö–æ–¥–æ–≤–∞—è —è—á–µ–π–∫–∞ <nY0uNHSfdhjE>\n",
        "# %% [code]\n",
        "new_beit_model_name = '/content/drive/MyDrive/models/beit-base-letters_14032023'\n",
        "beit_model_name = 'microsoft/beit-base-patch16-224'\n",
        "beit_feature_extractor = BeitFeatureExtractor.from_pretrained(beit_model_name)\n",
        "–ü–æ–ª—É—á–µ–Ω—ã –≤—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.\n",
        "1KB\n",
        "\tStream\n",
        "\t\tloading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--beit-base-patch16-224/snapshots/5f0307aec45cf4c914ee8700108b779fb8390654/preprocessor_config.json\n",
        "\t\tsize should be a dictionary on of the following set of keys: ({'width', 'height'}, {'shortest_edge'}, {'longest_edge', 'shortest_edge'}), got 224. Converted to {'height': 224, 'width': 224}.\n",
        "\t\tcrop_size should be a dictionary on of the following set of keys: ({'width', 'height'}, {'shortest_edge'}, {'longest_edge', 'shortest_edge'}), got 224. Converted to {'height': 224, 'width': 224}.\n",
        "\t\tImage processor BeitFeatureExtractor {\n",
        "\t\t  \"crop_size\": {\n",
        "\t\t    \"height\": 224,\n",
        "\t\t    \"width\": 224\n",
        "\t\t  },\n",
        "\t\t  \"do_center_crop\": false,\n",
        "\t\t  \"do_normalize\": true,\n",
        "\t\t  \"do_reduce_labels\": false,\n",
        "\t\t  \"do_rescale\": true,\n",
        "\t\t  \"do_resize\": true,\n",
        "\t\t  \"feature_extractor_type\": \"BeitFeatureExtractor\",\n",
        "\t\t  \"image_mean\": [\n",
        "\t\t    0.5,\n",
        "\t\t    0.5,\n",
        "\t\t    0.5\n",
        "\t\t  ],\n",
        "\t\t  \"image_processor_type\": \"BeitFeatureExtractor\",\n",
        "\t\t  \"image_std\": [\n",
        "\t\t    0.5,\n",
        "\t\t    0.5,\n",
        "\t\t    0.5\n",
        "\t\t  ],\n",
        "\t\t  \"resample\": 2,\n",
        "\t\t  \"rescale_factor\": 0.00392156862745098,\n",
        "\t\t  \"size\": {\n",
        "\t\t    \"height\": 224,\n",
        "\t\t    \"width\": 224\n",
        "\t\t  }\n",
        "\t\t}\n",
        "\n",
        "–ö–æ–¥–æ–≤–∞—è —è—á–µ–π–∫–∞ <XRygcgv3T3_O>\n",
        "# %% [code]\n",
        "labels = df.labels.unique()\n",
        "\n",
        "new_model = BeitForImageClassification.from_pretrained(\n",
        "    new_beit_model_name,\n",
        "    num_labels=labels.shape[0],\n",
        "    id2label={str(i): c for i, c in enumerate(le.classes_)},  # to convert id of class to real label\n",
        "    label2id={c: str(i) for i, c in enumerate(le.classes_)},\n",
        "    ignore_mismatched_sizes=True\n",
        ")\n",
        "–ü–æ–ª—É—á–µ–Ω—ã –≤—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.\n",
        "4KB\n",
        "\tStream\n",
        "\t\tloading configuration file /content/drive/MyDrive/models/beit-base-letters_14032023/config.json\n",
        "\t\tModel config BeitConfig {\n",
        "\t\t  \"_name_or_path\": \"microsoft/beit-base-patch16-224\",\n",
        "\t\t  \"architectures\": [\n",
        "\t\t    \"BeitForImageClassification\"\n",
        "\t\t  ],\n",
        "\t\t  \"attention_probs_dropout_prob\": 0.0,\n",
        "\t\t  \"auxiliary_channels\": 256,\n",
        "\t\t  \"auxiliary_concat_input\": false,\n",
        "\t\t  \"auxiliary_loss_weight\": 0.4,\n",
        "\t\t  \"auxiliary_num_convs\": 1,\n",
        "\t\t  \"drop_path_rate\": 0.1,\n",
        "\t\t  \"hidden_act\": \"gelu\",\n",
        "\t\t  \"hidden_dropout_prob\": 0.0,\n",
        "\t\t  \"hidden_size\": 768,\n",
        "\t\t  \"id2label\": {\n",
        "\t\t    \"0\": \"-\",\n",
        "\t\t    \"1\": \"\\u0430\",\n",
        "\t\t    \"10\": \"\\u0439\",\n",
        "\t\t    \"11\": \"\\u043a\",\n",
        "\t\t    \"12\": \"\\u043b\",\n",
        "\t\t    \"13\": \"\\u043c\",\n",
        "\t\t    \"14\": \"\\u043d\",\n",
        "\t\t    \"15\": \"\\u043e\",\n",
        "\t\t    \"16\": \"\\u043f\",\n",
        "\t\t    \"17\": \"\\u0440\",\n",
        "\t\t    \"18\": \"\\u0441\",\n",
        "\t\t    \"19\": \"\\u0442\",\n",
        "\t\t    \"2\": \"\\u0431\",\n",
        "\t\t    \"20\": \"\\u0443\",\n",
        "\t\t    \"21\": \"\\u0444\",\n",
        "\t\t    \"22\": \"\\u0445\",\n",
        "\t\t    \"23\": \"\\u0446\",\n",
        "\t\t    \"24\": \"\\u0447\",\n",
        "\t\t    \"25\": \"\\u0448\",\n",
        "\t\t    \"26\": \"\\u0449\",\n",
        "\t\t    \"27\": \"\\u044a\",\n",
        "\t\t    \"28\": \"\\u044b\",\n",
        "\t\t    \"29\": \"\\u044c\",\n",
        "\t\t    \"3\": \"\\u0432\",\n",
        "\t\t    \"30\": \"\\u044d\",\n",
        "\t\t    \"31\": \"\\u044e\",\n",
        "\t\t    \"32\": \"\\u044f\",\n",
        "\t\t    \"33\": \"\\u0451\",\n",
        "\t\t    \"4\": \"\\u0433\",\n",
        "\t\t    \"5\": \"\\u0434\",\n",
        "\t\t    \"6\": \"\\u0435\",\n",
        "\t\t    \"7\": \"\\u0436\",\n",
        "\t\t    \"8\": \"\\u0437\",\n",
        "\t\t    \"9\": \"\\u0438\"\n",
        "\t\t  },\n",
        "\t\t  \"image_size\": 224,\n",
        "\t\t  \"initializer_range\": 0.02,\n",
        "\t\t  \"intermediate_size\": 3072,\n",
        "\t\t  \"label2id\": {\n",
        "\t\t    \"-\": \"0\",\n",
        "\t\t    \"\\u0430\": \"1\",\n",
        "\t\t    \"\\u0431\": \"2\",\n",
        "\t\t    \"\\u0432\": \"3\",\n",
        "\t\t    \"\\u0433\": \"4\",\n",
        "\t\t    \"\\u0434\": \"5\",\n",
        "\t\t    \"\\u0435\": \"6\",\n",
        "\t\t    \"\\u0436\": \"7\",\n",
        "\t\t    \"\\u0437\": \"8\",\n",
        "\t\t    \"\\u0438\": \"9\",\n",
        "\t\t    \"\\u0439\": \"10\",\n",
        "\t\t    \"\\u043a\": \"11\",\n",
        "\t\t    \"\\u043b\": \"12\",\n",
        "\t\t    \"\\u043c\": \"13\",\n",
        "\t\t    \"\\u043d\": \"14\",\n",
        "\t\t    \"\\u043e\": \"15\",\n",
        "\t\t    \"\\u043f\": \"16\",\n",
        "\t\t    \"\\u0440\": \"17\",\n",
        "\t\t    \"\\u0441\": \"18\",\n",
        "\t\t    \"\\u0442\": \"19\",\n",
        "\t\t    \"\\u0443\": \"20\",\n",
        "\t\t    \"\\u0444\": \"21\",\n",
        "\t\t    \"\\u0445\": \"22\",\n",
        "\t\t    \"\\u0446\": \"23\",\n",
        "\t\t    \"\\u0447\": \"24\",\n",
        "\t\t    \"\\u0448\": \"25\",\n",
        "\t\t    \"\\u0449\": \"26\",\n",
        "\t\t    \"\\u044a\": \"27\",\n",
        "\t\t    \"\\u044b\": \"28\",\n",
        "\t\t    \"\\u044c\": \"29\",\n",
        "\t\t    \"\\u044d\": \"30\",\n",
        "\t\t    \"\\u044e\": \"31\",\n",
        "\t\t    \"\\u044f\": \"32\",\n",
        "\t\t    \"\\u0451\": \"33\"\n",
        "\t\t  },\n",
        "\t\t  \"layer_norm_eps\": 1e-12,\n",
        "\t\t  \"layer_scale_init_value\": 0.1,\n",
        "\t\t  \"model_type\": \"beit\",\n",
        "\t\t  \"num_attention_heads\": 12,\n",
        "\t\t  \"num_channels\": 3,\n",
        "\t\t  \"num_hidden_layers\": 12,\n",
        "\t\t  \"out_indices\": [\n",
        "\t\t    3,\n",
        "\t\t    5,\n",
        "\t\t    7,\n",
        "\t\t    11\n",
        "\t\t  ],\n",
        "\t\t  \"patch_size\": 16,\n",
        "\t\t  \"pool_scales\": [\n",
        "\t\t    1,\n",
        "\t\t    2,\n",
        "\t\t    3,\n",
        "\t\t    6\n",
        "\t\t  ],\n",
        "\t\t  \"problem_type\": \"single_label_classification\",\n",
        "\t\t  \"semantic_loss_ignore_index\": 255,\n",
        "\t\t  \"torch_dtype\": \"float32\",\n",
        "\t\t  \"transformers_version\": \"4.26.1\",\n",
        "\t\t  \"use_absolute_position_embeddings\": false,\n",
        "\t\t  \"use_auxiliary_head\": true,\n",
        "\t\t  \"use_mask_token\": false,\n",
        "\t\t  \"use_mean_pooling\": true,\n",
        "\t\t  \"use_relative_position_bias\": true,\n",
        "\t\t  \"use_shared_relative_position_bias\": false,\n",
        "\t\t  \"vocab_size\": 8192\n",
        "\t\t}\n",
        "\n",
        "\t\tloading weights file /content/drive/MyDrive/models/beit-base-letters_14032023/pytorch_model.bin\n",
        "\t\tAll model checkpoint weights were used when initializing BeitForImageClassification.\n",
        "\n",
        "\t\tAll the weights of BeitForImageClassification were initialized from the model checkpoint at /content/drive/MyDrive/models/beit-base-letters_14032023.\n",
        "\t\tIf your task is similar to the task the model of the checkpoint was trained on, you can already use BeitForImageClassification for predictions without further training.\n",
        "\n",
        "–ö–æ–¥–æ–≤–∞—è —è—á–µ–π–∫–∞ <VnSheNDfT8x8>\n",
        "# %% [code]\n",
        "new_model.to(device)\n",
        "–ü–æ–ª—É—á–µ–Ω—ã –≤—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.\n",
        "17KB\n",
        "\ttext/plain\n",
        "\t\tBeitForImageClassification(\n",
        "\t\t  (beit): BeitModel(\n",
        "\t\t    (embeddings): BeitEmbeddings(\n",
        "\t\t      (patch_embeddings): BeitPatchEmbeddings(\n",
        "\t\t        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
        "\t\t      )\n",
        "\t\t      (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t    )\n",
        "\t\t    (encoder): BeitEncoder(\n",
        "\t\t      (layer): ModuleList(\n",
        "\t\t        (0): BeitLayer(\n",
        "\t\t          (attention): BeitAttention(\n",
        "\t\t            (attention): BeitSelfAttention(\n",
        "\t\t              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (key): Linear(in_features=768, out_features=768, bias=False)\n",
        "\t\t              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t              (relative_position_bias): BeitRelativePositionBias()\n",
        "\t\t            )\n",
        "\t\t            (output): BeitSelfOutput(\n",
        "\t\t              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t          )\n",
        "\t\t          (intermediate): BeitIntermediate(\n",
        "\t\t            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "\t\t            (intermediate_act_fn): GELUActivation()\n",
        "\t\t          )\n",
        "\t\t          (output): BeitOutput(\n",
        "\t\t            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "\t\t            (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t          )\n",
        "\t\t          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t          (drop_path): Identity()\n",
        "\t\t          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t        )\n",
        "\t\t        (1): BeitLayer(\n",
        "\t\t          (attention): BeitAttention(\n",
        "\t\t            (attention): BeitSelfAttention(\n",
        "\t\t              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (key): Linear(in_features=768, out_features=768, bias=False)\n",
        "\t\t              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t              (relative_position_bias): BeitRelativePositionBias()\n",
        "\t\t            )\n",
        "\t\t            (output): BeitSelfOutput(\n",
        "\t\t              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t          )\n",
        "\t\t          (intermediate): BeitIntermediate(\n",
        "\t\t            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "\t\t            (intermediate_act_fn): GELUActivation()\n",
        "\t\t          )\n",
        "\t\t          (output): BeitOutput(\n",
        "\t\t            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "\t\t            (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t          )\n",
        "\t\t          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t          (drop_path): BeitDropPath(p=0.00909090880304575)\n",
        "\t\t          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t        )\n",
        "\t\t        (2): BeitLayer(\n",
        "\t\t          (attention): BeitAttention(\n",
        "\t\t            (attention): BeitSelfAttention(\n",
        "\t\t              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (key): Linear(in_features=768, out_features=768, bias=False)\n",
        "\t\t              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t              (relative_position_bias): BeitRelativePositionBias()\n",
        "\t\t            )\n",
        "\t\t            (output): BeitSelfOutput(\n",
        "\t\t              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t          )\n",
        "\t\t          (intermediate): BeitIntermediate(\n",
        "\t\t            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "\t\t            (intermediate_act_fn): GELUActivation()\n",
        "\t\t          )\n",
        "\t\t          (output): BeitOutput(\n",
        "\t\t            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "\t\t            (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t          )\n",
        "\t\t          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t          (drop_path): BeitDropPath(p=0.0181818176060915)\n",
        "\t\t          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t        )\n",
        "\t\t        (3): BeitLayer(\n",
        "\t\t          (attention): BeitAttention(\n",
        "\t\t            (attention): BeitSelfAttention(\n",
        "\t\t              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (key): Linear(in_features=768, out_features=768, bias=False)\n",
        "\t\t              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t              (relative_position_bias): BeitRelativePositionBias()\n",
        "\t\t            )\n",
        "\t\t            (output): BeitSelfOutput(\n",
        "\t\t              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t          )\n",
        "\t\t          (intermediate): BeitIntermediate(\n",
        "\t\t            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "\t\t            (intermediate_act_fn): GELUActivation()\n",
        "\t\t          )\n",
        "\t\t          (output): BeitOutput(\n",
        "\t\t            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "\t\t            (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t          )\n",
        "\t\t          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t          (drop_path): BeitDropPath(p=0.027272727340459824)\n",
        "\t\t          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t        )\n",
        "\t\t        (4): BeitLayer(\n",
        "\t\t          (attention): BeitAttention(\n",
        "\t\t            (attention): BeitSelfAttention(\n",
        "\t\t              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (key): Linear(in_features=768, out_features=768, bias=False)\n",
        "\t\t              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t              (relative_position_bias): BeitRelativePositionBias()\n",
        "\t\t            )\n",
        "\t\t            (output): BeitSelfOutput(\n",
        "\t\t              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t          )\n",
        "\t\t          (intermediate): BeitIntermediate(\n",
        "\t\t            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "\t\t            (intermediate_act_fn): GELUActivation()\n",
        "\t\t          )\n",
        "\t\t          (output): BeitOutput(\n",
        "\t\t            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "\t\t            (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t          )\n",
        "\t\t          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t          (drop_path): BeitDropPath(p=0.036363635212183)\n",
        "\t\t          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t        )\n",
        "\t\t        (5): BeitLayer(\n",
        "\t\t          (attention): BeitAttention(\n",
        "\t\t            (attention): BeitSelfAttention(\n",
        "\t\t              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (key): Linear(in_features=768, out_features=768, bias=False)\n",
        "\t\t              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t              (relative_position_bias): BeitRelativePositionBias()\n",
        "\t\t            )\n",
        "\t\t            (output): BeitSelfOutput(\n",
        "\t\t              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t          )\n",
        "\t\t          (intermediate): BeitIntermediate(\n",
        "\t\t            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "\t\t            (intermediate_act_fn): GELUActivation()\n",
        "\t\t          )\n",
        "\t\t          (output): BeitOutput(\n",
        "\t\t            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "\t\t            (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t          )\n",
        "\t\t          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t          (drop_path): BeitDropPath(p=0.045454543083906174)\n",
        "\t\t          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t        )\n",
        "\t\t        (6): BeitLayer(\n",
        "\t\t          (attention): BeitAttention(\n",
        "\t\t            (attention): BeitSelfAttention(\n",
        "\t\t              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (key): Linear(in_features=768, out_features=768, bias=False)\n",
        "\t\t              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t              (relative_position_bias): BeitRelativePositionBias()\n",
        "\t\t            )\n",
        "\t\t            (output): BeitSelfOutput(\n",
        "\t\t              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t          )\n",
        "\t\t          (intermediate): BeitIntermediate(\n",
        "\t\t            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "\t\t            (intermediate_act_fn): GELUActivation()\n",
        "\t\t          )\n",
        "\t\t          (output): BeitOutput(\n",
        "\t\t            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "\t\t            (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t          )\n",
        "\t\t          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t          (drop_path): BeitDropPath(p=0.054545458406209946)\n",
        "\t\t          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t        )\n",
        "\t\t        (7): BeitLayer(\n",
        "\t\t          (attention): BeitAttention(\n",
        "\t\t            (attention): BeitSelfAttention(\n",
        "\t\t              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (key): Linear(in_features=768, out_features=768, bias=False)\n",
        "\t\t              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t              (relative_position_bias): BeitRelativePositionBias()\n",
        "\t\t            )\n",
        "\t\t            (output): BeitSelfOutput(\n",
        "\t\t              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t          )\n",
        "\t\t          (intermediate): BeitIntermediate(\n",
        "\t\t            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "\t\t            (intermediate_act_fn): GELUActivation()\n",
        "\t\t          )\n",
        "\t\t          (output): BeitOutput(\n",
        "\t\t            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "\t\t            (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t          )\n",
        "\t\t          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t          (drop_path): BeitDropPath(p=0.06363636255264282)\n",
        "\t\t          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t        )\n",
        "\t\t        (8): BeitLayer(\n",
        "\t\t          (attention): BeitAttention(\n",
        "\t\t            (attention): BeitSelfAttention(\n",
        "\t\t              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (key): Linear(in_features=768, out_features=768, bias=False)\n",
        "\t\t              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t              (relative_position_bias): BeitRelativePositionBias()\n",
        "\t\t            )\n",
        "\t\t            (output): BeitSelfOutput(\n",
        "\t\t              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t          )\n",
        "\t\t          (intermediate): BeitIntermediate(\n",
        "\t\t            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "\t\t            (intermediate_act_fn): GELUActivation()\n",
        "\t\t          )\n",
        "\t\t          (output): BeitOutput(\n",
        "\t\t            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "\t\t            (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t          )\n",
        "\t\t          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t          (drop_path): BeitDropPath(p=0.0727272778749466)\n",
        "\t\t          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t        )\n",
        "\t\t        (9): BeitLayer(\n",
        "\t\t          (attention): BeitAttention(\n",
        "\t\t            (attention): BeitSelfAttention(\n",
        "\t\t              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (key): Linear(in_features=768, out_features=768, bias=False)\n",
        "\t\t              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t              (relative_position_bias): BeitRelativePositionBias()\n",
        "\t\t            )\n",
        "\t\t            (output): BeitSelfOutput(\n",
        "\t\t              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t          )\n",
        "\t\t          (intermediate): BeitIntermediate(\n",
        "\t\t            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "\t\t            (intermediate_act_fn): GELUActivation()\n",
        "\t\t          )\n",
        "\t\t          (output): BeitOutput(\n",
        "\t\t            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "\t\t            (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t          )\n",
        "\t\t          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t          (drop_path): BeitDropPath(p=0.08181818574666977)\n",
        "\t\t          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t        )\n",
        "\t\t        (10): BeitLayer(\n",
        "\t\t          (attention): BeitAttention(\n",
        "\t\t            (attention): BeitSelfAttention(\n",
        "\t\t              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (key): Linear(in_features=768, out_features=768, bias=False)\n",
        "\t\t              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t              (relative_position_bias): BeitRelativePositionBias()\n",
        "\t\t            )\n",
        "\t\t            (output): BeitSelfOutput(\n",
        "\t\t              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t          )\n",
        "\t\t          (intermediate): BeitIntermediate(\n",
        "\t\t            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "\t\t            (intermediate_act_fn): GELUActivation()\n",
        "\t\t          )\n",
        "\t\t          (output): BeitOutput(\n",
        "\t\t            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "\t\t            (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t          )\n",
        "\t\t          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t          (drop_path): BeitDropPath(p=0.09090909361839294)\n",
        "\t\t          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t        )\n",
        "\t\t        (11): BeitLayer(\n",
        "\t\t          (attention): BeitAttention(\n",
        "\t\t            (attention): BeitSelfAttention(\n",
        "\t\t              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (key): Linear(in_features=768, out_features=768, bias=False)\n",
        "\t\t              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t              (relative_position_bias): BeitRelativePositionBias()\n",
        "\t\t            )\n",
        "\t\t            (output): BeitSelfOutput(\n",
        "\t\t              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t          )\n",
        "\t\t          (intermediate): BeitIntermediate(\n",
        "\t\t            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "\t\t            (intermediate_act_fn): GELUActivation()\n",
        "\t\t          )\n",
        "\t\t          (output): BeitOutput(\n",
        "\t\t            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "\t\t            (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t          )\n",
        "\t\t          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t          (drop_path): BeitDropPath(p=0.10000000149011612)\n",
        "\t\t          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t        )\n",
        "\t\t      )\n",
        "\t\t    )\n",
        "\t\t    (layernorm): Identity()\n",
        "\t\t    (pooler): BeitPooler(\n",
        "\t\t      (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t    )\n",
        "\t\t  )\n",
        "\t\t  (classifier): Linear(in_features=768, out_features=34, bias=True)\n",
        "\t\t)\n",
        "\n",
        "–ö–æ–¥–æ–≤–∞—è —è—á–µ–π–∫–∞ <1duQZTXujLQs>\n",
        "# %% [code]\n",
        "actual_labels = []\n",
        "predicted_labels = []\n",
        "\n",
        "for item in tqdm(test_dataset):\n",
        "    pixels = item['pixel_values'].to(device)\n",
        "    outputs = new_model(pixels)\n",
        "    logits = outputs.logits.to('cpu')\n",
        "    predicted_class_idx = logits.argmax(-1).item()\n",
        "    predicted_labels.append(new_model.config.id2label[str(predicted_class_idx)])\n",
        "    actual_labels.append(new_model.config.id2label[str(item['labels'])])\n",
        "–ü–æ–ª—É—á–µ–Ω—ã –≤—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.\n",
        "0KB\n",
        "\ttext/plain\n",
        "\t\t0%|          | 0/17691 [00:00<?, ?it/s]\n",
        "\n",
        "–ö–æ–¥–æ–≤–∞—è —è—á–µ–π–∫–∞ <NtNlmtLgT0fq>\n",
        "# %% [code]\n",
        "print(classification_report(actual_labels, predicted_labels))\n",
        "–ü–æ–ª—É—á–µ–Ω—ã –≤—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.\n",
        "2KB\n",
        "\tStream\n",
        "\t\tprecision    recall  f1-score   support\n",
        "\n",
        "\t\t           -       0.00      0.00      0.00         1\n",
        "\t\t           –∞       0.98      0.98      0.98      3042\n",
        "\t\t           –±       0.99      0.83      0.90       264\n",
        "\t\t           –≤       0.99      0.96      0.98      1026\n",
        "\t\t           –≥       0.89      0.93      0.91       265\n",
        "\t\t           –¥       0.96      0.91      0.93       441\n",
        "\t\t           –µ       0.97      0.97      0.97      1165\n",
        "\t\t           –∂       0.97      0.91      0.94        85\n",
        "\t\t           –∑       0.85      0.97      0.90       154\n",
        "\t\t           –∏       0.88      0.97      0.92      1389\n",
        "\t\t           –π       0.75      0.09      0.16       137\n",
        "\t\t           –∫       0.97      0.98      0.97       934\n",
        "\t\t           –ª       0.94      0.96      0.95       979\n",
        "\t\t           –º       0.98      0.94      0.96       518\n",
        "\t\t           –Ω       0.95      0.97      0.96      1427\n",
        "\t\t           –æ       0.98      0.98      0.98      1361\n",
        "\t\t           –ø       0.96      0.94      0.95       238\n",
        "\t\t           —Ä       0.97      0.96      0.97       996\n",
        "\t\t           —Å       0.96      0.97      0.96       703\n",
        "\t\t           —Ç       0.98      0.97      0.97       654\n",
        "\t\t           —É       0.95      0.95      0.95       311\n",
        "\t\t           —Ñ       0.94      0.91      0.92       111\n",
        "\t\t           —Ö       0.98      0.97      0.98       111\n",
        "\t\t           —Ü       0.89      0.85      0.87        80\n",
        "\t\t           —á       0.85      0.85      0.85       150\n",
        "\t\t           —à       0.94      0.94      0.94       154\n",
        "\t\t           —â       0.57      0.57      0.57         7\n",
        "\t\t           —ä       0.00      0.00      0.00         1\n",
        "\t\t           —ã       1.00      0.23      0.38        56\n",
        "\t\t           —å       0.75      0.96      0.85       285\n",
        "\t\t           —ç       0.76      0.93      0.84        30\n",
        "\t\t           —é       0.96      0.96      0.96        97\n",
        "\t\t           —è       0.99      0.98      0.98       513\n",
        "\t\t           —ë       0.00      0.00      0.00         6\n",
        "\n",
        "\t\t    accuracy                           0.95     17691\n",
        "\t\t   macro avg       0.84      0.80      0.80     17691\n",
        "\t\tweighted avg       0.95      0.95      0.95     17691\n",
        "\n",
        "–ê—Ç—Ä–∏–±—É—Ç—ã –±–ª–æ–∫–Ω–æ—Ç–∞\n",
        "–°–≤–µ—Ä–Ω—É—Ç—å —Ä–∞–∑–¥–µ–ª—ã [\"2lWiIpAQtoiu\"]\n"
      ],
      "metadata": {
        "id": "ABjR-s_2smfC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}