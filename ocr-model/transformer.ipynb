{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install easyocr pytesseract\n",
        "!apt-get install -y tesseract-ocr-rus\n",
        "!tesseract --list-langs\n"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDbLm_mAlYWF",
        "outputId": "e9a83c44-5d8c-4eaa-f44c-0080f8582aa8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: easyocr in /usr/local/lib/python3.11/dist-packages (1.7.2)\n",
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from easyocr) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision>=0.5 in /usr/local/lib/python3.11/dist-packages (from easyocr) (0.21.0+cu124)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.11/dist-packages (from easyocr) (4.11.0.86)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from easyocr) (1.14.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from easyocr) (2.0.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from easyocr) (11.1.0)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (from easyocr) (0.25.2)\n",
            "Requirement already satisfied: python-bidi in /usr/local/lib/python3.11/dist-packages (from easyocr) (0.6.6)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from easyocr) (6.0.2)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.11/dist-packages (from easyocr) (2.0.7)\n",
            "Requirement already satisfied: pyclipper in /usr/local/lib/python3.11/dist-packages (from easyocr) (1.3.0.post6)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.11/dist-packages (from easyocr) (1.11.1.4)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->easyocr) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->easyocr) (1.3.0)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image->easyocr) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image->easyocr) (2025.3.13)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image->easyocr) (0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->easyocr) (3.0.2)\n",
            "Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: pytesseract\n",
            "Successfully installed pytesseract-0.3.13\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tesseract-ocr-rus is already the newest version (1:4.00~git30-7274cfa-1.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 29 not upgraded.\n",
            "List of available languages (3):\n",
            "eng\n",
            "osd\n",
            "rus\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import easyocr\n",
        "import pytesseract\n",
        "from PIL import Image\n",
        "\n",
        "def remove_black_lines_and_fix_symbols(image_path):\n",
        "    # Загрузка изображения и преобразование в HSV\n",
        "    img = cv2.imread(image_path)\n",
        "    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
        "\n",
        "    # Настройка диапазона для синего и голубого цветов\n",
        "    lower_blue = np.array([85, 30, 50])\n",
        "    upper_blue = np.array([135, 255, 255])\n",
        "\n",
        "    # Создание маски и выделение синего текста\n",
        "    blue_mask = cv2.inRange(hsv, lower_blue, upper_blue)\n",
        "\n",
        "    # Морфологическая операция (расширение), чтобы устранить разрывы\n",
        "    kernel = np.ones((3, 3), np.uint8)  # Настройка ядра (размер можно подбирать)\n",
        "    blue_mask_dilated = cv2.dilate(blue_mask, kernel, iterations=25)  # Расширение маски\n",
        "    # Применение маски к изображению\n",
        "    result = cv2.bitwise_and(img, img, mask=blue_mask_dilated)\n",
        "\n",
        "    # Замена чёрного фона на белый\n",
        "    result[blue_mask_dilated == 0] = [255, 255, 255]\n",
        "\n",
        "    # Сохраняем результат рядом с исходным файлом\n",
        "    processed_image_path = image_path.replace(\".jpg\", \"_processed.jpg\")\n",
        "    cv2.imwrite(processed_image_path, result)\n",
        "\n",
        "    return processed_image_path"
      ],
      "metadata": {
        "id": "-TV19BY-vQi6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_path_new = \"image.jpg\"  # Путь к загруженному изображения\n",
        "\n",
        "#image_path_new=remove_black_lines(image_path)\n",
        "\n",
        "reader = easyocr.Reader(['ru'])  # Добавляем поддержку русского языков\n",
        "# Распознаём текст на изображении\n",
        "results = reader.readtext(image_path_new,allowlist=\"йцукенгшщзхъфывапролджэячсмитьбюё\")  # detail=1 вернёт координаты и текст\n",
        "# Выводим текст и координаты\n",
        "for (bbox, text, confidence) in results:\n",
        "    print(f\"Текст: {text}, Доверие: {confidence}\")\n",
        "\n",
        "\n",
        "img = cv2.imread(image_path_new)\n",
        "custom_config = r'-c tessedit_char_whitelist=йцукенгшщзхъфывапролджэячсмитьбюё'\n",
        "text = pytesseract.image_to_string(img, lang=\"rus\", config=custom_config)\n",
        "print(\"Распознанный текст:\", text)\n",
        "\n",
        "img = Image.open(image_path_new)\n",
        "plt.imshow(img)\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "yfKISSMpr4rL",
        "outputId": "4a91b66b-f984-41f7-a3eb-db041351f6ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:easyocr.easyocr:Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Текст: м, Доверие: 0.4334957390534022\n",
            "Распознанный текст: \f\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "McIYxBxnsiF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from graphviz import Digraph\n",
        "\n",
        "# Создание UML диаграммы прецедентов\n",
        "dot = Digraph('UserScenario', format='png')\n",
        "\n",
        "# Определение актера (Пользователь)\n",
        "dot.node('U', 'Пользователь', shape='actor')\n",
        "\n",
        "# Определение прецедентов (функций системы)\n",
        "dot.node('P1', 'Загрузить PDF', shape='ellipse')\n",
        "dot.node('P2', 'Обработать документ', shape='ellipse')\n",
        "dot.node('P3', 'Выделить ошибки', shape='ellipse')\n",
        "dot.node('P4', 'Сравнить ФИО с базой', shape='ellipse')\n",
        "dot.node('P5', 'Редактировать данные', shape='ellipse')\n",
        "dot.node('P6', 'Экспортировать в XLS', shape='ellipse')\n",
        "\n",
        "# Связи между актором и прецедентами\n",
        "dot.edge('U', 'P1')\n",
        "dot.edge('P1', 'P2')\n",
        "dot.edge('P2', 'P3')\n",
        "dot.edge('P3', 'P4')\n",
        "dot.edge('P4', 'P5')\n",
        "dot.edge('P2', 'P6')\n",
        "dot.edge('P5', 'P6')\n",
        "\n",
        "# Сохранение диаграммы\n",
        "diagram_path = \"/mnt/data/user_scenario_uml.png\"\n",
        "dot.render(diagram_path, format='png')\n",
        "\n",
        "# Отображение диаграммы пользователю\n",
        "import IPython.display as display\n",
        "display.Image(diagram_path)\n"
      ],
      "metadata": {
        "id": "l4wDgWh5aGSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Текстовая ячейка <WEYXFOjaiJ6W>\n",
        "# %% [markdown]\n",
        "# Transformers for letters recognition\n",
        "Fine-tuning of ViT and BEiT for handwritten letters recognition.\n",
        "\n",
        "Текстовая ячейка <3qk31iFpmXFL>\n",
        "# %% [markdown]\n",
        "About:\n",
        "\n",
        "1. [ViT paper](https://arxiv.org/pdf/2010.11929.pdf), [ViT Github](https://github.com/google-research/vision_transformer), [ViT on huggingface🤗](https://huggingface.co/docs/transformers/model_doc/vit)\n",
        "2. [BEiT paper](https://openreview.net/pdf?id=p-BhZSz59o4), [BEiT on huggingface🤗](https://huggingface.co/docs/transformers/model_doc/beit)\n",
        "\n",
        "\n",
        "Текстовая ячейка <1rQBn4g9tCWC>\n",
        "# %% [markdown]\n",
        "This notebook is based on huggingface🤗 [tutorial](https://huggingface.co/blog/fine-tune-vit) for ViT fine-tuning\n",
        "\n",
        "Текстовая ячейка <jJklyKNKijPs>\n",
        "# %% [markdown]\n",
        "## Imports\n",
        "\n",
        "Кодовая ячейка <Qh2R9NFcmrWT>\n",
        "# %% [code]\n",
        "!pip3 install transformers tokenizers datasets evaluate --quiet\n",
        "Получены выходные данные.\n",
        "0KB\n",
        "\tStream\n",
        "\t\t'pip3' is not recognized as an internal or external command,\n",
        "\t\toperable program or batch file.\n",
        "\n",
        "Кодовая ячейка <nDzb1u4ghzgo>\n",
        "# %% [code]\n",
        "import os\n",
        "import random\n",
        "import warnings\n",
        "import google-api-python-client\n",
        "\n",
        "import zipfile\n",
        "from google.colab import drive\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm, trange\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from IPython.display import clear_output\n",
        "from PIL import Image\n",
        "\n",
        "import datasets\n",
        "from datasets import load_dataset, ClassLabel\n",
        "\n",
        "from transformers import ViTFeatureExtractor, BeitFeatureExtractor\n",
        "from transformers import ViTForImageClassification, BeitForImageClassification\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from transformers import pipeline\n",
        "\n",
        "import evaluate\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import balanced_accuracy_score, cohen_kappa_score, matthews_corrcoef\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn import preprocessing\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "%matplotlib inline\n",
        "Получены выходные данные.\n",
        "0KB\n",
        "\tError\n",
        "\t\tSyntaxError\n",
        "\t\t  Cell In[16], line 4\n",
        "\t\t    import google-api-python-client\n",
        "\t\t                 ^\n",
        "\t\tSyntaxError: invalid syntax\n",
        "\n",
        "\n",
        "Кодовая ячейка <62xEDJmeikkl>\n",
        "# %% [code]\n",
        "drive.mount('/content/drive')\n",
        "Получены выходные данные.\n",
        "0KB\n",
        "\tStream\n",
        "\t\tMounted at /content/drive\n",
        "\n",
        "Кодовая ячейка <OzGgQmNWinHY>\n",
        "# %% [code]\n",
        "path_to_zip_file = '/content/drive/MyDrive/total dict.zip'\n",
        "directory_to_extract_to = '/content/data'\n",
        "\n",
        "with zipfile.ZipFile(path_to_zip_file, 'r') as zip_ref:\n",
        "    zip_ref.extractall(directory_to_extract_to)\n",
        "\n",
        "Кодовая ячейка <QZABgTv2ioi5>\n",
        "# %% [code]\n",
        "!cp '/content/drive/MyDrive/full_dict/total dict/all_by_words.tsv' '/content/data/total dict'\n",
        "\n",
        "Кодовая ячейка <WXj6cxkWiqpn>\n",
        "# %% [code]\n",
        "os.chdir('/content/data/total dict')\n",
        "os.path.abspath('.')\n",
        "Получены выходные данные.\n",
        "0KB\n",
        "\ttext/plain\n",
        "\t\t'/content/data/total dict'\n",
        "\n",
        "Кодовая ячейка <BJ5jLI6wslc8>\n",
        "# %% [code]\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "Кодовая ячейка <2Nz4ayH8vms8>\n",
        "# %% [code]\n",
        "accuracy = evaluate.load('accuracy', 'multiclass')\n",
        "Получены выходные данные.\n",
        "0KB\n",
        "\ttext/plain\n",
        "\t\tDownloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]\n",
        "\n",
        "Кодовая ячейка <eMhT-QwvirDP>\n",
        "# %% [code]\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        torch.backends.cudnn.benchmark = True\n",
        "        torch.backends.cudnn.deterministic = False\n",
        "\n",
        "Кодовая ячейка <GuD88Uk0itG7>\n",
        "# %% [code]\n",
        "seed_everything()\n",
        "\n",
        "Текстовая ячейка <5YVAv78Vip2J>\n",
        "# %% [markdown]\n",
        "## Data\n",
        "\n",
        "Кодовая ячейка <9QvxaqrvNpdh>\n",
        "# %% [code]\n",
        "!cp '/content/drive/MyDrive/full_dict/total dict/all_files_new.tsv' '/content/data/total dict'\n",
        "\n",
        "Кодовая ячейка <0hA1uYb0Na4r>\n",
        "# %% [code]\n",
        "df = pd.read_csv('/content/drive/MyDrive/all_files_final.csv', delimiter=',')\n",
        "line_name = \"/content/data/total dict\"\n",
        "full_paths = [line_name + path[1:] for path in df[\"new_path\"]]\n",
        "df[\"new_path\"] = full_paths\n",
        "df = df.dropna(subset=['letter'])\n",
        "df.head()\n",
        "Получены выходные данные.\n",
        "6KB\n",
        "\ttext/plain\n",
        "\t\tUnnamed: 0  letter_position   category letter  \\\n",
        "\t\t0           0                0  codewords      а\n",
        "\t\t1           1                2  codewords      а\n",
        "\t\t2           2                4  codewords      а\n",
        "\t\t3           3                1  codewords      б\n",
        "\t\t4           4                3  codewords      к\n",
        "\n",
        "\t\t                                            new_path  word_id word_true\n",
        "\t\t0  /content/data/total dict/codewords/abakan.jpg_...        0    абакан\n",
        "\t\t1  /content/data/total dict/codewords/abakan.jpg_...        0    абакан\n",
        "\t\t2  /content/data/total dict/codewords/abakan.jpg_...        0    абакан\n",
        "\t\t3  /content/data/total dict/codewords/abakan.jpg_...        0    абакан\n",
        "\t\t4  /content/data/total dict/codewords/abakan.jpg_...        0    абакан\n",
        "\n",
        "Текстовая ячейка <SroCScWoViMr>\n",
        "# %% [markdown]\n",
        "[Issue](https://github.com/huggingface/transformers/issues/21638) on sizes of images. Let's use the ```PIL``` to open images. Then convert it to RGB according to [this duscussion](https://stackoverflow.com/questions/75168665/unsupported-number-of-image-dimensions-while-using-image-utils-from-transforme)\n",
        "\n",
        "Кодовая ячейка <Oq49ZVuisgWK>\n",
        "# %% [code]\n",
        "class LettersDataset(Dataset):\n",
        "    def __init__(self, data, feature_extractor, transform=None):\n",
        "        self.data = data\n",
        "        self.transform = transform\n",
        "        self.feature_extractor = feature_extractor\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.data.iloc[idx].new_path\n",
        "        # print(img_path)\n",
        "        image = Image.open(img_path)\n",
        "        # print(image)\n",
        "        label = self.data.iloc[idx].labels\n",
        "        if self.transform:\n",
        "            item = self.transform(image, label, self.feature_extractor)\n",
        "            return item\n",
        "\n",
        "        return image, label\n",
        "\n",
        "Кодовая ячейка <ler4EUg5IPi7>\n",
        "# %% [code]\n",
        "le = preprocessing.LabelEncoder()\n",
        "df['labels'] = le.fit_transform(df.letter)\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "Кодовая ячейка <m9_WEJTiuwC4>\n",
        "# %% [code]\n",
        "df.head()\n",
        "Получены выходные данные.\n",
        "6KB\n",
        "\ttext/plain\n",
        "\t\tUnnamed: 0  letter_position   category letter  \\\n",
        "\t\t0           0                0  codewords      а\n",
        "\t\t1           1                2  codewords      а\n",
        "\t\t2           2                4  codewords      а\n",
        "\t\t3           3                1  codewords      б\n",
        "\t\t4           4                3  codewords      к\n",
        "\n",
        "\t\t                                            new_path  word_id word_true  \\\n",
        "\t\t0  /content/data/total dict/codewords/abakan.jpg_...        0    абакан\n",
        "\t\t1  /content/data/total dict/codewords/abakan.jpg_...        0    абакан\n",
        "\t\t2  /content/data/total dict/codewords/abakan.jpg_...        0    абакан\n",
        "\t\t3  /content/data/total dict/codewords/abakan.jpg_...        0    абакан\n",
        "\t\t4  /content/data/total dict/codewords/abakan.jpg_...        0    абакан\n",
        "\n",
        "\t\t   labels\n",
        "\t\t0       1\n",
        "\t\t1       1\n",
        "\t\t2       1\n",
        "\t\t3       2\n",
        "\t\t4      11\n",
        "\n",
        "Кодовая ячейка <1Qsr81dJuz9K>\n",
        "# %% [code]\n",
        "le.classes_\n",
        "Получены выходные данные.\n",
        "0KB\n",
        "\tError\n",
        "\t\tNameError\n",
        "\t\t---------------------------------------------------------------------------\n",
        "\t\tNameError                                 Traceback (most recent call last)\n",
        "\t\tCell In[2], line 1\n",
        "\t\t----> 1 le.classes_\n",
        "\n",
        "\t\tNameError: name 'le' is not defined\n",
        "\n",
        "Текстовая ячейка <kUESNAU9jIMC>\n",
        "# %% [markdown]\n",
        "## ViT Model\n",
        "\n",
        "Кодовая ячейка <d23a8Z2psiHG>\n",
        "# %% [code]\n",
        "vit_model_name = 'google/vit-base-patch16-224'\n",
        "vit_feature_extractor = ViTFeatureExtractor.from_pretrained(vit_model_name)\n",
        "Получены выходные данные.\n",
        "0KB\n",
        "\ttext/plain\n",
        "\t\tDownloading (…)rocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]\n",
        "\n",
        "Кодовая ячейка <cVYMjqnNAj_I>\n",
        "# %% [code]\n",
        "def process_example(image, label, feature_extractor):\n",
        "    inputs = feature_extractor(image.convert('RGB'), return_tensors='pt')\n",
        "    inputs['labels'] = label\n",
        "    return inputs\n",
        "\n",
        "Кодовая ячейка <X89BHzGfI_EI>\n",
        "# %% [code]\n",
        "train_dataset = LettersDataset(train_df, vit_feature_extractor, transform=process_example)\n",
        "test_dataset = LettersDataset(test_df, vit_feature_extractor, transform=process_example)\n",
        "\n",
        "Кодовая ячейка <C5XteBABU6A5>\n",
        "# %% [code]\n",
        "# item size\n",
        "train_dataset[0]['pixel_values'].shape  # batch_size, num_channels, width, height\n",
        "Получены выходные данные.\n",
        "0KB\n",
        "\ttext/plain\n",
        "\t\ttorch.Size([1, 3, 224, 224])\n",
        "\n",
        "Кодовая ячейка <zuJ6vqVtvTDB>\n",
        "# %% [code]\n",
        "def collate_fn(batch):\n",
        "    return {\n",
        "        'pixel_values': torch.stack([x['pixel_values'][0] for x in batch], 0),\n",
        "        'labels': torch.tensor([x['labels'] for x in batch])\n",
        "    }\n",
        "\n",
        "Кодовая ячейка <J7jclUuJTI_R>\n",
        "# %% [code]\n",
        "# size after data collator\n",
        "batch = [train_dataset[0], train_dataset[1], train_dataset[2]]\n",
        "collated = collate_fn(batch)\n",
        "print(collated['pixel_values'].shape)\n",
        "print(collated['labels'].shape)\n",
        "Получены выходные данные.\n",
        "0KB\n",
        "\tStream\n",
        "\t\ttorch.Size([3, 3, 224, 224])\n",
        "\t\ttorch.Size([3])\n",
        "\n",
        "Кодовая ячейка <zpL1ik_3vZj5>\n",
        "# %% [code]\n",
        "labels = df.labels.unique()\n",
        "\n",
        "model = ViTForImageClassification.from_pretrained(\n",
        "    vit_model_name,\n",
        "    num_labels=labels.shape[0],\n",
        "    id2label={str(i): c for i, c in enumerate(le.classes_)},  # to convert id of class to real label\n",
        "    label2id={c: str(i) for i, c in enumerate(le.classes_)},\n",
        "    ignore_mismatched_sizes=True\n",
        ")\n",
        "Получены выходные данные.\n",
        "1KB\n",
        "\tStream\n",
        "\t\tSome weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
        "\t\t- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([34, 768]) in the model instantiated\n",
        "\t\t- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([34]) in the model instantiated\n",
        "\t\tYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
        "\ttext/plain\n",
        "\t\tDownloading (…)lve/main/config.json:   0%|          | 0.00/69.7k [00:00<?, ?B/s]\n",
        "\t\tDownloading pytorch_model.bin:   0%|          | 0.00/346M [00:00<?, ?B/s]\n",
        "\n",
        "Кодовая ячейка <IwO3DQWgOscP>\n",
        "# %% [code]\n",
        "labels\n",
        "Получены выходные данные.\n",
        "0KB\n",
        "\ttext/plain\n",
        "\t\tarray([ 1,  2, 11, 14,  9, 22, 12,  5, 20, 15, 18, 17,  4,  3, 29,  6, 26,\n",
        "\t\t       10, 13, 30, 16, 19, 21, 32, 23, 25, 24, 33,  8, 28, 31,  7,  0, 27])\n",
        "\n",
        "Кодовая ячейка <ohwwKj2K2_DP>\n",
        "# %% [code]\n",
        "model.to(device)\n",
        "Получены выходные данные.\n",
        "15KB\n",
        "\ttext/plain\n",
        "\t\tViTForImageClassification(\n",
        "\t\t  (vit): ViTModel(\n",
        "\t\t    (embeddings): ViTEmbeddings(\n",
        "\t\t      (patch_embeddings): ViTPatchEmbeddings(\n",
        "\t\t        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
        "\t\t      )\n",
        "\t\t      (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t    )\n",
        "\t\t    (encoder): ViTEncoder(\n",
        "\t\t      (layer): ModuleList(\n",
        "\t\t        (0): ViTLayer(\n",
        "\t\t          (attention): ViTAttention(\n",
        "\t\t            (attention): ViTSelfAttention(\n",
        "\t\t              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (key): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t            (output): ViTSelfOutput(\n",
        "\t\t              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t          )\n",
        "\t\t          (intermediate): ViTIntermediate(\n",
        "\t\t            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "\t\t            (intermediate_act_fn): GELUActivation()\n",
        "\t\t          )\n",
        "\t\t          (output): ViTOutput(\n",
        "\t\t            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "\t\t            (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t          )\n",
        "\t\t          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t        )\n",
        "\t\t        (1): ViTLayer(\n",
        "\t\t          (attention): ViTAttention(\n",
        "\t\t            (attention): ViTSelfAttention(\n",
        "\t\t              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (key): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t            (output): ViTSelfOutput(\n",
        "\t\t              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t          )\n",
        "\t\t          (intermediate): ViTIntermediate(\n",
        "\t\t            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "\t\t            (intermediate_act_fn): GELUActivation()\n",
        "\t\t          )\n",
        "\t\t          (output): ViTOutput(\n",
        "\t\t            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "\t\t            (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t          )\n",
        "\t\t          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t        )\n",
        "\t\t        (2): ViTLayer(\n",
        "\t\t          (attention): ViTAttention(\n",
        "\t\t            (attention): ViTSelfAttention(\n",
        "\t\t              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (key): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t            (output): ViTSelfOutput(\n",
        "\t\t              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t          )\n",
        "\t\t          (intermediate): ViTIntermediate(\n",
        "\t\t            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "\t\t            (intermediate_act_fn): GELUActivation()\n",
        "\t\t          )\n",
        "\t\t          (output): ViTOutput(\n",
        "\t\t            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "\t\t            (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t          )\n",
        "\t\t          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t        )\n",
        "\t\t        (3): ViTLayer(\n",
        "\t\t          (attention): ViTAttention(\n",
        "\t\t            (attention): ViTSelfAttention(\n",
        "\t\t              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (key): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t            (output): ViTSelfOutput(\n",
        "\t\t              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t          )\n",
        "\t\t          (intermediate): ViTIntermediate(\n",
        "\t\t            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "\t\t            (intermediate_act_fn): GELUActivation()\n",
        "\t\t          )\n",
        "\t\t          (output): ViTOutput(\n",
        "\t\t            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "\t\t            (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t          )\n",
        "\t\t          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t        )\n",
        "\t\t        (4): ViTLayer(\n",
        "\t\t          (attention): ViTAttention(\n",
        "\t\t            (attention): ViTSelfAttention(\n",
        "\t\t              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (key): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t            (output): ViTSelfOutput(\n",
        "\t\t              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t          )\n",
        "\t\t          (intermediate): ViTIntermediate(\n",
        "\t\t            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "\t\t            (intermediate_act_fn): GELUActivation()\n",
        "\t\t          )\n",
        "\t\t          (output): ViTOutput(\n",
        "\t\t            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "\t\t            (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t          )\n",
        "\t\t          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t        )\n",
        "\t\t        (5): ViTLayer(\n",
        "\t\t          (attention): ViTAttention(\n",
        "\t\t            (attention): ViTSelfAttention(\n",
        "\t\t              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (key): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t            (output): ViTSelfOutput(\n",
        "\t\t              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t          )\n",
        "\t\t          (intermediate): ViTIntermediate(\n",
        "\t\t            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "\t\t            (intermediate_act_fn): GELUActivation()\n",
        "\t\t          )\n",
        "\t\t          (output): ViTOutput(\n",
        "\t\t            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "\t\t            (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t          )\n",
        "\t\t          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t        )\n",
        "\t\t        (6): ViTLayer(\n",
        "\t\t          (attention): ViTAttention(\n",
        "\t\t            (attention): ViTSelfAttention(\n",
        "\t\t              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (key): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t            (output): ViTSelfOutput(\n",
        "\t\t              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t          )\n",
        "\t\t          (intermediate): ViTIntermediate(\n",
        "\t\t            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "\t\t            (intermediate_act_fn): GELUActivation()\n",
        "\t\t          )\n",
        "\t\t          (output): ViTOutput(\n",
        "\t\t            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "\t\t            (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t          )\n",
        "\t\t          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t        )\n",
        "\t\t        (7): ViTLayer(\n",
        "\t\t          (attention): ViTAttention(\n",
        "\t\t            (attention): ViTSelfAttention(\n",
        "\t\t              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (key): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t            (output): ViTSelfOutput(\n",
        "\t\t              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t          )\n",
        "\t\t          (intermediate): ViTIntermediate(\n",
        "\t\t            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "\t\t            (intermediate_act_fn): GELUActivation()\n",
        "\t\t          )\n",
        "\t\t          (output): ViTOutput(\n",
        "\t\t            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "\t\t            (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t          )\n",
        "\t\t          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t        )\n",
        "\t\t        (8): ViTLayer(\n",
        "\t\t          (attention): ViTAttention(\n",
        "\t\t            (attention): ViTSelfAttention(\n",
        "\t\t              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (key): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t            (output): ViTSelfOutput(\n",
        "\t\t              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t          )\n",
        "\t\t          (intermediate): ViTIntermediate(\n",
        "\t\t            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "\t\t            (intermediate_act_fn): GELUActivation()\n",
        "\t\t          )\n",
        "\t\t          (output): ViTOutput(\n",
        "\t\t            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "\t\t            (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t          )\n",
        "\t\t          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t        )\n",
        "\t\t        (9): ViTLayer(\n",
        "\t\t          (attention): ViTAttention(\n",
        "\t\t            (attention): ViTSelfAttention(\n",
        "\t\t              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (key): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t            (output): ViTSelfOutput(\n",
        "\t\t              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t          )\n",
        "\t\t          (intermediate): ViTIntermediate(\n",
        "\t\t            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "\t\t            (intermediate_act_fn): GELUActivation()\n",
        "\t\t          )\n",
        "\t\t          (output): ViTOutput(\n",
        "\t\t            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "\t\t            (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t          )\n",
        "\t\t          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t        )\n",
        "\t\t        (10): ViTLayer(\n",
        "\t\t          (attention): ViTAttention(\n",
        "\t\t            (attention): ViTSelfAttention(\n",
        "\t\t              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (key): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t            (output): ViTSelfOutput(\n",
        "\t\t              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t          )\n",
        "\t\t          (intermediate): ViTIntermediate(\n",
        "\t\t            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "\t\t            (intermediate_act_fn): GELUActivation()\n",
        "\t\t          )\n",
        "\t\t          (output): ViTOutput(\n",
        "\t\t            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "\t\t            (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t          )\n",
        "\t\t          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t        )\n",
        "\t\t        (11): ViTLayer(\n",
        "\t\t          (attention): ViTAttention(\n",
        "\t\t            (attention): ViTSelfAttention(\n",
        "\t\t              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (key): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t            (output): ViTSelfOutput(\n",
        "\t\t              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t          )\n",
        "\t\t          (intermediate): ViTIntermediate(\n",
        "\t\t            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "\t\t            (intermediate_act_fn): GELUActivation()\n",
        "\t\t          )\n",
        "\t\t          (output): ViTOutput(\n",
        "\t\t            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "\t\t            (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t          )\n",
        "\t\t          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t        )\n",
        "\t\t      )\n",
        "\t\t    )\n",
        "\t\t    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t  )\n",
        "\t\t  (classifier): Linear(in_features=768, out_features=34, bias=True)\n",
        "\t\t)\n",
        "\n",
        "Кодовая ячейка <yWXgxQx03B1U>\n",
        "# %% [code]\n",
        "def compute_metrics(p):\n",
        "    return accuracy.compute(predictions=np.argmax(p.predictions, axis=1), references=p.label_ids)\n",
        "\n",
        "Кодовая ячейка <L8WY_ZJz3GS8>\n",
        "# %% [code]\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./vit-base-letters\",\n",
        "    per_device_train_batch_size=16,\n",
        "    num_train_epochs=4,\n",
        "    fp16=True,\n",
        "    logging_steps=100,\n",
        "    learning_rate=2e-4,\n",
        "    save_total_limit=2,\n",
        "    remove_unused_columns=False,\n",
        "    push_to_hub=False,\n",
        "    report_to='tensorboard',\n",
        ")\n",
        "\n",
        "Кодовая ячейка <Qrkmfykz3OFD>\n",
        "# %% [code]\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=collate_fn,\n",
        "    compute_metrics=compute_metrics,\n",
        "    train_dataset=train_dataset,\n",
        "    tokenizer=vit_feature_extractor,\n",
        ")\n",
        "Получены выходные данные.\n",
        "0KB\n",
        "\tStream\n",
        "\t\tUsing cuda_amp half precision backend\n",
        "\n",
        "Кодовая ячейка <JfEXIoUC3srf>\n",
        "# %% [code]\n",
        "trainer.train()\n",
        "trainer.save_model('/content/drive/MyDrive/models/vit-base-letters_14032023')\n",
        "Получены выходные данные.\n",
        "29KB\n",
        "\tStream\n",
        "\t\t***** Running training *****\n",
        "\t\t  Num examples = 70761\n",
        "\t\t  Num Epochs = 4\n",
        "\t\t  Instantaneous batch size per device = 16\n",
        "\t\t  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
        "\t\t  Gradient Accumulation steps = 1\n",
        "\t\t  Total optimization steps = 17692\n",
        "\t\t  Number of trainable parameters = 85824802\n",
        "\t\tSaving model checkpoint to ./vit-base-letters/checkpoint-500\n",
        "\t\tConfiguration saved in ./vit-base-letters/checkpoint-500/config.json\n",
        "\t\tModel weights saved in ./vit-base-letters/checkpoint-500/pytorch_model.bin\n",
        "\t\tImage processor saved in ./vit-base-letters/checkpoint-500/preprocessor_config.json\n",
        "\t\tSaving model checkpoint to ./vit-base-letters/checkpoint-1000\n",
        "\t\tConfiguration saved in ./vit-base-letters/checkpoint-1000/config.json\n",
        "\t\tModel weights saved in ./vit-base-letters/checkpoint-1000/pytorch_model.bin\n",
        "\t\tImage processor saved in ./vit-base-letters/checkpoint-1000/preprocessor_config.json\n",
        "\t\tSaving model checkpoint to ./vit-base-letters/checkpoint-1500\n",
        "\t\tConfiguration saved in ./vit-base-letters/checkpoint-1500/config.json\n",
        "\t\tModel weights saved in ./vit-base-letters/checkpoint-1500/pytorch_model.bin\n",
        "\t\tImage processor saved in ./vit-base-letters/checkpoint-1500/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [vit-base-letters/checkpoint-500] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./vit-base-letters/checkpoint-2000\n",
        "\t\tConfiguration saved in ./vit-base-letters/checkpoint-2000/config.json\n",
        "\t\tModel weights saved in ./vit-base-letters/checkpoint-2000/pytorch_model.bin\n",
        "\t\tImage processor saved in ./vit-base-letters/checkpoint-2000/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [vit-base-letters/checkpoint-1000] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./vit-base-letters/checkpoint-2500\n",
        "\t\tConfiguration saved in ./vit-base-letters/checkpoint-2500/config.json\n",
        "\t\tModel weights saved in ./vit-base-letters/checkpoint-2500/pytorch_model.bin\n",
        "\t\tImage processor saved in ./vit-base-letters/checkpoint-2500/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [vit-base-letters/checkpoint-1500] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./vit-base-letters/checkpoint-3000\n",
        "\t\tConfiguration saved in ./vit-base-letters/checkpoint-3000/config.json\n",
        "\t\tModel weights saved in ./vit-base-letters/checkpoint-3000/pytorch_model.bin\n",
        "\t\tImage processor saved in ./vit-base-letters/checkpoint-3000/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [vit-base-letters/checkpoint-2000] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./vit-base-letters/checkpoint-3500\n",
        "\t\tConfiguration saved in ./vit-base-letters/checkpoint-3500/config.json\n",
        "\t\tModel weights saved in ./vit-base-letters/checkpoint-3500/pytorch_model.bin\n",
        "\t\tImage processor saved in ./vit-base-letters/checkpoint-3500/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [vit-base-letters/checkpoint-2500] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./vit-base-letters/checkpoint-4000\n",
        "\t\tConfiguration saved in ./vit-base-letters/checkpoint-4000/config.json\n",
        "\t\tModel weights saved in ./vit-base-letters/checkpoint-4000/pytorch_model.bin\n",
        "\t\tImage processor saved in ./vit-base-letters/checkpoint-4000/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [vit-base-letters/checkpoint-3000] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./vit-base-letters/checkpoint-4500\n",
        "\t\tConfiguration saved in ./vit-base-letters/checkpoint-4500/config.json\n",
        "\t\tModel weights saved in ./vit-base-letters/checkpoint-4500/pytorch_model.bin\n",
        "\t\tImage processor saved in ./vit-base-letters/checkpoint-4500/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [vit-base-letters/checkpoint-3500] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./vit-base-letters/checkpoint-5000\n",
        "\t\tConfiguration saved in ./vit-base-letters/checkpoint-5000/config.json\n",
        "\t\tModel weights saved in ./vit-base-letters/checkpoint-5000/pytorch_model.bin\n",
        "\t\tImage processor saved in ./vit-base-letters/checkpoint-5000/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [vit-base-letters/checkpoint-4000] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./vit-base-letters/checkpoint-5500\n",
        "\t\tConfiguration saved in ./vit-base-letters/checkpoint-5500/config.json\n",
        "\t\tModel weights saved in ./vit-base-letters/checkpoint-5500/pytorch_model.bin\n",
        "\t\tImage processor saved in ./vit-base-letters/checkpoint-5500/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [vit-base-letters/checkpoint-4500] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./vit-base-letters/checkpoint-6000\n",
        "\t\tConfiguration saved in ./vit-base-letters/checkpoint-6000/config.json\n",
        "\t\tModel weights saved in ./vit-base-letters/checkpoint-6000/pytorch_model.bin\n",
        "\t\tImage processor saved in ./vit-base-letters/checkpoint-6000/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [vit-base-letters/checkpoint-5000] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./vit-base-letters/checkpoint-6500\n",
        "\t\tConfiguration saved in ./vit-base-letters/checkpoint-6500/config.json\n",
        "\t\tModel weights saved in ./vit-base-letters/checkpoint-6500/pytorch_model.bin\n",
        "\t\tImage processor saved in ./vit-base-letters/checkpoint-6500/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [vit-base-letters/checkpoint-5500] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./vit-base-letters/checkpoint-7000\n",
        "\t\tConfiguration saved in ./vit-base-letters/checkpoint-7000/config.json\n",
        "\t\tModel weights saved in ./vit-base-letters/checkpoint-7000/pytorch_model.bin\n",
        "\t\tImage processor saved in ./vit-base-letters/checkpoint-7000/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [vit-base-letters/checkpoint-6000] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./vit-base-letters/checkpoint-7500\n",
        "\t\tConfiguration saved in ./vit-base-letters/checkpoint-7500/config.json\n",
        "\t\tModel weights saved in ./vit-base-letters/checkpoint-7500/pytorch_model.bin\n",
        "\t\tImage processor saved in ./vit-base-letters/checkpoint-7500/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [vit-base-letters/checkpoint-6500] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./vit-base-letters/checkpoint-8000\n",
        "\t\tConfiguration saved in ./vit-base-letters/checkpoint-8000/config.json\n",
        "\t\tModel weights saved in ./vit-base-letters/checkpoint-8000/pytorch_model.bin\n",
        "\t\tImage processor saved in ./vit-base-letters/checkpoint-8000/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [vit-base-letters/checkpoint-7000] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./vit-base-letters/checkpoint-8500\n",
        "\t\tConfiguration saved in ./vit-base-letters/checkpoint-8500/config.json\n",
        "\t\tModel weights saved in ./vit-base-letters/checkpoint-8500/pytorch_model.bin\n",
        "\t\tImage processor saved in ./vit-base-letters/checkpoint-8500/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [vit-base-letters/checkpoint-7500] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./vit-base-letters/checkpoint-9000\n",
        "\t\tConfiguration saved in ./vit-base-letters/checkpoint-9000/config.json\n",
        "\t\tModel weights saved in ./vit-base-letters/checkpoint-9000/pytorch_model.bin\n",
        "\t\tImage processor saved in ./vit-base-letters/checkpoint-9000/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [vit-base-letters/checkpoint-8000] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./vit-base-letters/checkpoint-9500\n",
        "\t\tConfiguration saved in ./vit-base-letters/checkpoint-9500/config.json\n",
        "\t\tModel weights saved in ./vit-base-letters/checkpoint-9500/pytorch_model.bin\n",
        "\t\tImage processor saved in ./vit-base-letters/checkpoint-9500/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [vit-base-letters/checkpoint-8500] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./vit-base-letters/checkpoint-10000\n",
        "\t\tConfiguration saved in ./vit-base-letters/checkpoint-10000/config.json\n",
        "\t\tModel weights saved in ./vit-base-letters/checkpoint-10000/pytorch_model.bin\n",
        "\t\tImage processor saved in ./vit-base-letters/checkpoint-10000/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [vit-base-letters/checkpoint-9000] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./vit-base-letters/checkpoint-10500\n",
        "\t\tConfiguration saved in ./vit-base-letters/checkpoint-10500/config.json\n",
        "\t\tModel weights saved in ./vit-base-letters/checkpoint-10500/pytorch_model.bin\n",
        "\t\tImage processor saved in ./vit-base-letters/checkpoint-10500/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [vit-base-letters/checkpoint-9500] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./vit-base-letters/checkpoint-11000\n",
        "\t\tConfiguration saved in ./vit-base-letters/checkpoint-11000/config.json\n",
        "\t\tModel weights saved in ./vit-base-letters/checkpoint-11000/pytorch_model.bin\n",
        "\t\tImage processor saved in ./vit-base-letters/checkpoint-11000/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [vit-base-letters/checkpoint-10000] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./vit-base-letters/checkpoint-11500\n",
        "\t\tConfiguration saved in ./vit-base-letters/checkpoint-11500/config.json\n",
        "\t\tModel weights saved in ./vit-base-letters/checkpoint-11500/pytorch_model.bin\n",
        "\t\tImage processor saved in ./vit-base-letters/checkpoint-11500/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [vit-base-letters/checkpoint-10500] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./vit-base-letters/checkpoint-12000\n",
        "\t\tConfiguration saved in ./vit-base-letters/checkpoint-12000/config.json\n",
        "\t\tModel weights saved in ./vit-base-letters/checkpoint-12000/pytorch_model.bin\n",
        "\t\tImage processor saved in ./vit-base-letters/checkpoint-12000/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [vit-base-letters/checkpoint-11000] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./vit-base-letters/checkpoint-12500\n",
        "\t\tConfiguration saved in ./vit-base-letters/checkpoint-12500/config.json\n",
        "\t\tModel weights saved in ./vit-base-letters/checkpoint-12500/pytorch_model.bin\n",
        "\t\tImage processor saved in ./vit-base-letters/checkpoint-12500/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [vit-base-letters/checkpoint-11500] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./vit-base-letters/checkpoint-13000\n",
        "\t\tConfiguration saved in ./vit-base-letters/checkpoint-13000/config.json\n",
        "\t\tModel weights saved in ./vit-base-letters/checkpoint-13000/pytorch_model.bin\n",
        "\t\tImage processor saved in ./vit-base-letters/checkpoint-13000/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [vit-base-letters/checkpoint-12000] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./vit-base-letters/checkpoint-13500\n",
        "\t\tConfiguration saved in ./vit-base-letters/checkpoint-13500/config.json\n",
        "\t\tModel weights saved in ./vit-base-letters/checkpoint-13500/pytorch_model.bin\n",
        "\t\tImage processor saved in ./vit-base-letters/checkpoint-13500/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [vit-base-letters/checkpoint-12500] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./vit-base-letters/checkpoint-14000\n",
        "\t\tConfiguration saved in ./vit-base-letters/checkpoint-14000/config.json\n",
        "\t\tModel weights saved in ./vit-base-letters/checkpoint-14000/pytorch_model.bin\n",
        "\t\tImage processor saved in ./vit-base-letters/checkpoint-14000/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [vit-base-letters/checkpoint-13000] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./vit-base-letters/checkpoint-14500\n",
        "\t\tConfiguration saved in ./vit-base-letters/checkpoint-14500/config.json\n",
        "\t\tModel weights saved in ./vit-base-letters/checkpoint-14500/pytorch_model.bin\n",
        "\t\tImage processor saved in ./vit-base-letters/checkpoint-14500/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [vit-base-letters/checkpoint-13500] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./vit-base-letters/checkpoint-15000\n",
        "\t\tConfiguration saved in ./vit-base-letters/checkpoint-15000/config.json\n",
        "\t\tModel weights saved in ./vit-base-letters/checkpoint-15000/pytorch_model.bin\n",
        "\t\tImage processor saved in ./vit-base-letters/checkpoint-15000/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [vit-base-letters/checkpoint-14000] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./vit-base-letters/checkpoint-15500\n",
        "\t\tConfiguration saved in ./vit-base-letters/checkpoint-15500/config.json\n",
        "\t\tModel weights saved in ./vit-base-letters/checkpoint-15500/pytorch_model.bin\n",
        "\t\tImage processor saved in ./vit-base-letters/checkpoint-15500/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [vit-base-letters/checkpoint-14500] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./vit-base-letters/checkpoint-16000\n",
        "\t\tConfiguration saved in ./vit-base-letters/checkpoint-16000/config.json\n",
        "\t\tModel weights saved in ./vit-base-letters/checkpoint-16000/pytorch_model.bin\n",
        "\t\tImage processor saved in ./vit-base-letters/checkpoint-16000/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [vit-base-letters/checkpoint-15000] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./vit-base-letters/checkpoint-16500\n",
        "\t\tConfiguration saved in ./vit-base-letters/checkpoint-16500/config.json\n",
        "\t\tModel weights saved in ./vit-base-letters/checkpoint-16500/pytorch_model.bin\n",
        "\t\tImage processor saved in ./vit-base-letters/checkpoint-16500/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [vit-base-letters/checkpoint-15500] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./vit-base-letters/checkpoint-17000\n",
        "\t\tConfiguration saved in ./vit-base-letters/checkpoint-17000/config.json\n",
        "\t\tModel weights saved in ./vit-base-letters/checkpoint-17000/pytorch_model.bin\n",
        "\t\tImage processor saved in ./vit-base-letters/checkpoint-17000/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [vit-base-letters/checkpoint-16000] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./vit-base-letters/checkpoint-17500\n",
        "\t\tConfiguration saved in ./vit-base-letters/checkpoint-17500/config.json\n",
        "\t\tModel weights saved in ./vit-base-letters/checkpoint-17500/pytorch_model.bin\n",
        "\t\tImage processor saved in ./vit-base-letters/checkpoint-17500/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [vit-base-letters/checkpoint-16500] due to args.save_total_limit\n",
        "\n",
        "\n",
        "\t\tTraining completed. Do not forget to share your model on huggingface.co/models =)\n",
        "\n",
        "\n",
        "\t\tSaving model checkpoint to /content/drive/MyDrive/models/vit-base-letters_14032023\n",
        "\t\tConfiguration saved in /content/drive/MyDrive/models/vit-base-letters_14032023/config.json\n",
        "\t\tModel weights saved in /content/drive/MyDrive/models/vit-base-letters_14032023/pytorch_model.bin\n",
        "\t\tImage processor saved in /content/drive/MyDrive/models/vit-base-letters_14032023/preprocessor_config.json\n",
        "\n",
        "Текстовая ячейка <ADXPe0H2SeUt>\n",
        "# %% [markdown]\n",
        "### Evaluation\n",
        "\n",
        "Кодовая ячейка <2q0sgFBPSctF>\n",
        "# %% [code]\n",
        "new_vit_model_name = '/content/drive/MyDrive/models/vit-base-letters_14032023'\n",
        "vit_model_name = 'google/vit-base-patch16-224'\n",
        "vit_feature_extractor = ViTFeatureExtractor.from_pretrained(vit_model_name)\n",
        "\n",
        "Кодовая ячейка <RkvOjqInSaKD>\n",
        "# %% [code]\n",
        "labels = df.labels.unique()\n",
        "\n",
        "new_model = ViTForImageClassification.from_pretrained(\n",
        "    new_vit_model_name,\n",
        "    num_labels=labels.shape[0],\n",
        "    id2label={str(i): c for i, c in enumerate(le.classes_)},  # to convert id of class to real label\n",
        "    label2id={c: str(i) for i, c in enumerate(le.classes_)},\n",
        "    ignore_mismatched_sizes=True\n",
        ")\n",
        "\n",
        "Кодовая ячейка <F1m7olkatPMC>\n",
        "# %% [code]\n",
        "# sample\n",
        "outputs = new_model(test_dataset[0]['pixel_values'])\n",
        "logits = outputs.logits\n",
        "predicted_class_idx = logits.argmax(-1).item()\n",
        "print(\"Predicted class:\", new_model.config.id2label[str(predicted_class_idx)])\n",
        "print('Actual class:', new_model.config.id2label[str(test_dataset[0]['labels'])])\n",
        "Получены выходные данные.\n",
        "0KB\n",
        "\tStream\n",
        "\t\tPredicted class: к\n",
        "\t\tActual class: е\n",
        "\n",
        "Текстовая ячейка <LGlHhQNxvChT>\n",
        "# %% [markdown]\n",
        ":(\n",
        "\n",
        "Кодовая ячейка <gjBAq6b4vuDo>\n",
        "# %% [code]\n",
        "new_model.to(device)\n",
        "Получены выходные данные.\n",
        "15KB\n",
        "\ttext/plain\n",
        "\t\tViTForImageClassification(\n",
        "\t\t  (vit): ViTModel(\n",
        "\t\t    (embeddings): ViTEmbeddings(\n",
        "\t\t      (patch_embeddings): ViTPatchEmbeddings(\n",
        "\t\t        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
        "\t\t      )\n",
        "\t\t      (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t    )\n",
        "\t\t    (encoder): ViTEncoder(\n",
        "\t\t      (layer): ModuleList(\n",
        "\t\t        (0): ViTLayer(\n",
        "\t\t          (attention): ViTAttention(\n",
        "\t\t            (attention): ViTSelfAttention(\n",
        "\t\t              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (key): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t            (output): ViTSelfOutput(\n",
        "\t\t              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t          )\n",
        "\t\t          (intermediate): ViTIntermediate(\n",
        "\t\t            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "\t\t            (intermediate_act_fn): GELUActivation()\n",
        "\t\t          )\n",
        "\t\t          (output): ViTOutput(\n",
        "\t\t            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "\t\t            (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t          )\n",
        "\t\t          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t        )\n",
        "\t\t        (1): ViTLayer(\n",
        "\t\t          (attention): ViTAttention(\n",
        "\t\t            (attention): ViTSelfAttention(\n",
        "\t\t              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (key): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t            (output): ViTSelfOutput(\n",
        "\t\t              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t          )\n",
        "\t\t          (intermediate): ViTIntermediate(\n",
        "\t\t            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "\t\t            (intermediate_act_fn): GELUActivation()\n",
        "\t\t          )\n",
        "\t\t          (output): ViTOutput(\n",
        "\t\t            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "\t\t            (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t          )\n",
        "\t\t          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t        )\n",
        "\t\t        (2): ViTLayer(\n",
        "\t\t          (attention): ViTAttention(\n",
        "\t\t            (attention): ViTSelfAttention(\n",
        "\t\t              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (key): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t            (output): ViTSelfOutput(\n",
        "\t\t              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t          )\n",
        "\t\t          (intermediate): ViTIntermediate(\n",
        "\t\t            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "\t\t            (intermediate_act_fn): GELUActivation()\n",
        "\t\t          )\n",
        "\t\t          (output): ViTOutput(\n",
        "\t\t            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "\t\t            (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t          )\n",
        "\t\t          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t        )\n",
        "\t\t        (3): ViTLayer(\n",
        "\t\t          (attention): ViTAttention(\n",
        "\t\t            (attention): ViTSelfAttention(\n",
        "\t\t              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (key): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t            (output): ViTSelfOutput(\n",
        "\t\t              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t          )\n",
        "\t\t          (intermediate): ViTIntermediate(\n",
        "\t\t            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "\t\t            (intermediate_act_fn): GELUActivation()\n",
        "\t\t          )\n",
        "\t\t          (output): ViTOutput(\n",
        "\t\t            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "\t\t            (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t          )\n",
        "\t\t          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t        )\n",
        "\t\t        (4): ViTLayer(\n",
        "\t\t          (attention): ViTAttention(\n",
        "\t\t            (attention): ViTSelfAttention(\n",
        "\t\t              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (key): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t            (output): ViTSelfOutput(\n",
        "\t\t              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t          )\n",
        "\t\t          (intermediate): ViTIntermediate(\n",
        "\t\t            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "\t\t            (intermediate_act_fn): GELUActivation()\n",
        "\t\t          )\n",
        "\t\t          (output): ViTOutput(\n",
        "\t\t            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "\t\t            (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t          )\n",
        "\t\t          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t        )\n",
        "\t\t        (5): ViTLayer(\n",
        "\t\t          (attention): ViTAttention(\n",
        "\t\t            (attention): ViTSelfAttention(\n",
        "\t\t              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (key): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t            (output): ViTSelfOutput(\n",
        "\t\t              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t          )\n",
        "\t\t          (intermediate): ViTIntermediate(\n",
        "\t\t            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "\t\t            (intermediate_act_fn): GELUActivation()\n",
        "\t\t          )\n",
        "\t\t          (output): ViTOutput(\n",
        "\t\t            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "\t\t            (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t          )\n",
        "\t\t          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t        )\n",
        "\t\t        (6): ViTLayer(\n",
        "\t\t          (attention): ViTAttention(\n",
        "\t\t            (attention): ViTSelfAttention(\n",
        "\t\t              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (key): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t            (output): ViTSelfOutput(\n",
        "\t\t              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t          )\n",
        "\t\t          (intermediate): ViTIntermediate(\n",
        "\t\t            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "\t\t            (intermediate_act_fn): GELUActivation()\n",
        "\t\t          )\n",
        "\t\t          (output): ViTOutput(\n",
        "\t\t            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "\t\t            (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t          )\n",
        "\t\t          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t        )\n",
        "\t\t        (7): ViTLayer(\n",
        "\t\t          (attention): ViTAttention(\n",
        "\t\t            (attention): ViTSelfAttention(\n",
        "\t\t              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (key): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t            (output): ViTSelfOutput(\n",
        "\t\t              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t          )\n",
        "\t\t          (intermediate): ViTIntermediate(\n",
        "\t\t            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "\t\t            (intermediate_act_fn): GELUActivation()\n",
        "\t\t          )\n",
        "\t\t          (output): ViTOutput(\n",
        "\t\t            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "\t\t            (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t          )\n",
        "\t\t          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t        )\n",
        "\t\t        (8): ViTLayer(\n",
        "\t\t          (attention): ViTAttention(\n",
        "\t\t            (attention): ViTSelfAttention(\n",
        "\t\t              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (key): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t            (output): ViTSelfOutput(\n",
        "\t\t              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t          )\n",
        "\t\t          (intermediate): ViTIntermediate(\n",
        "\t\t            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "\t\t            (intermediate_act_fn): GELUActivation()\n",
        "\t\t          )\n",
        "\t\t          (output): ViTOutput(\n",
        "\t\t            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "\t\t            (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t          )\n",
        "\t\t          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t        )\n",
        "\t\t        (9): ViTLayer(\n",
        "\t\t          (attention): ViTAttention(\n",
        "\t\t            (attention): ViTSelfAttention(\n",
        "\t\t              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (key): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t            (output): ViTSelfOutput(\n",
        "\t\t              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t          )\n",
        "\t\t          (intermediate): ViTIntermediate(\n",
        "\t\t            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "\t\t            (intermediate_act_fn): GELUActivation()\n",
        "\t\t          )\n",
        "\t\t          (output): ViTOutput(\n",
        "\t\t            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "\t\t            (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t          )\n",
        "\t\t          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t        )\n",
        "\t\t        (10): ViTLayer(\n",
        "\t\t          (attention): ViTAttention(\n",
        "\t\t            (attention): ViTSelfAttention(\n",
        "\t\t              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (key): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t            (output): ViTSelfOutput(\n",
        "\t\t              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t          )\n",
        "\t\t          (intermediate): ViTIntermediate(\n",
        "\t\t            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "\t\t            (intermediate_act_fn): GELUActivation()\n",
        "\t\t          )\n",
        "\t\t          (output): ViTOutput(\n",
        "\t\t            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "\t\t            (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t          )\n",
        "\t\t          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t        )\n",
        "\t\t        (11): ViTLayer(\n",
        "\t\t          (attention): ViTAttention(\n",
        "\t\t            (attention): ViTSelfAttention(\n",
        "\t\t              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (key): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t            (output): ViTSelfOutput(\n",
        "\t\t              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t          )\n",
        "\t\t          (intermediate): ViTIntermediate(\n",
        "\t\t            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "\t\t            (intermediate_act_fn): GELUActivation()\n",
        "\t\t          )\n",
        "\t\t          (output): ViTOutput(\n",
        "\t\t            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "\t\t            (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t          )\n",
        "\t\t          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t        )\n",
        "\t\t      )\n",
        "\t\t    )\n",
        "\t\t    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t  )\n",
        "\t\t  (classifier): Linear(in_features=768, out_features=34, bias=True)\n",
        "\t\t)\n",
        "\n",
        "Кодовая ячейка <b1ZpZjXBu9FU>\n",
        "# %% [code]\n",
        "actual_labels = []\n",
        "predicted_labels = []\n",
        "\n",
        "for item in tqdm(test_dataset):\n",
        "    pixels = item['pixel_values'].to(device)\n",
        "    outputs = new_model(pixels)\n",
        "    logits = outputs.logits.to('cpu')\n",
        "    predicted_class_idx = logits.argmax(-1).item()\n",
        "    predicted_labels.append(new_model.config.id2label[str(predicted_class_idx)])\n",
        "    actual_labels.append(new_model.config.id2label[str(item['labels'])])\n",
        "Получены выходные данные.\n",
        "0KB\n",
        "\ttext/plain\n",
        "\t\t0%|          | 0/17691 [00:00<?, ?it/s]\n",
        "\n",
        "Кодовая ячейка <rnR0y1_9fu3c>\n",
        "# %% [code]\n",
        "len(predicted_labels)\n",
        "Получены выходные данные.\n",
        "0KB\n",
        "\ttext/plain\n",
        "\t\t17691\n",
        "\n",
        "Кодовая ячейка <-cr62wvFWuTM>\n",
        "# %% [code]\n",
        "print(classification_report(actual_labels, predicted_labels))\n",
        "Получены выходные данные.\n",
        "2KB\n",
        "\tStream\n",
        "\t\tprecision    recall  f1-score   support\n",
        "\n",
        "\t\t           -       0.00      0.00      0.00         1\n",
        "\t\t           а       0.98      0.98      0.98      3042\n",
        "\t\t           б       0.97      0.83      0.90       264\n",
        "\t\t           в       0.99      0.97      0.98      1026\n",
        "\t\t           г       0.91      0.95      0.93       265\n",
        "\t\t           д       0.95      0.91      0.93       441\n",
        "\t\t           е       0.97      0.97      0.97      1165\n",
        "\t\t           ж       0.98      0.93      0.95        85\n",
        "\t\t           з       0.88      0.95      0.92       154\n",
        "\t\t           и       0.88      0.96      0.92      1389\n",
        "\t\t           й       0.62      0.15      0.24       137\n",
        "\t\t           к       0.97      0.97      0.97       934\n",
        "\t\t           л       0.94      0.96      0.95       979\n",
        "\t\t           м       0.97      0.95      0.96       518\n",
        "\t\t           н       0.95      0.97      0.96      1427\n",
        "\t\t           о       0.98      0.98      0.98      1361\n",
        "\t\t           п       0.96      0.94      0.95       238\n",
        "\t\t           р       0.98      0.97      0.97       996\n",
        "\t\t           с       0.97      0.97      0.97       703\n",
        "\t\t           т       0.98      0.97      0.98       654\n",
        "\t\t           у       0.95      0.95      0.95       311\n",
        "\t\t           ф       0.94      0.93      0.94       111\n",
        "\t\t           х       0.99      0.98      0.99       111\n",
        "\t\t           ц       0.90      0.88      0.89        80\n",
        "\t\t           ч       0.87      0.85      0.86       150\n",
        "\t\t           ш       0.94      0.94      0.94       154\n",
        "\t\t           щ       0.70      1.00      0.82         7\n",
        "\t\t           ъ       0.00      0.00      0.00         1\n",
        "\t\t           ы       0.87      0.23      0.37        56\n",
        "\t\t           ь       0.76      0.96      0.85       285\n",
        "\t\t           э       0.83      1.00      0.91        30\n",
        "\t\t           ю       0.96      0.95      0.95        97\n",
        "\t\t           я       0.99      0.98      0.98       513\n",
        "\t\t           ё       0.00      0.00      0.00         6\n",
        "\n",
        "\t\t    accuracy                           0.95     17691\n",
        "\t\t   macro avg       0.84      0.82      0.82     17691\n",
        "\t\tweighted avg       0.95      0.95      0.95     17691\n",
        "\n",
        "Текстовая ячейка <2lWiIpAQtoiu>\n",
        "# %% [markdown]\n",
        "## BEiT Model\n",
        "\n",
        "Кодовая ячейка <l7IGXb06trLe>\n",
        "# %% [code]\n",
        "beit_model_name = 'microsoft/beit-base-patch16-224'\n",
        "beit_feature_extractor = BeitFeatureExtractor.from_pretrained(beit_model_name)\n",
        "Получены выходные данные.\n",
        "2KB\n",
        "\tStream\n",
        "\t\tloading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--beit-base-patch16-224/snapshots/5f0307aec45cf4c914ee8700108b779fb8390654/preprocessor_config.json\n",
        "\t\tsize should be a dictionary on of the following set of keys: ({'width', 'height'}, {'shortest_edge'}, {'longest_edge', 'shortest_edge'}), got 224. Converted to {'height': 224, 'width': 224}.\n",
        "\t\tcrop_size should be a dictionary on of the following set of keys: ({'width', 'height'}, {'shortest_edge'}, {'longest_edge', 'shortest_edge'}), got 224. Converted to {'height': 224, 'width': 224}.\n",
        "\t\tImage processor BeitFeatureExtractor {\n",
        "\t\t  \"crop_size\": {\n",
        "\t\t    \"height\": 224,\n",
        "\t\t    \"width\": 224\n",
        "\t\t  },\n",
        "\t\t  \"do_center_crop\": false,\n",
        "\t\t  \"do_normalize\": true,\n",
        "\t\t  \"do_reduce_labels\": false,\n",
        "\t\t  \"do_rescale\": true,\n",
        "\t\t  \"do_resize\": true,\n",
        "\t\t  \"feature_extractor_type\": \"BeitFeatureExtractor\",\n",
        "\t\t  \"image_mean\": [\n",
        "\t\t    0.5,\n",
        "\t\t    0.5,\n",
        "\t\t    0.5\n",
        "\t\t  ],\n",
        "\t\t  \"image_processor_type\": \"BeitFeatureExtractor\",\n",
        "\t\t  \"image_std\": [\n",
        "\t\t    0.5,\n",
        "\t\t    0.5,\n",
        "\t\t    0.5\n",
        "\t\t  ],\n",
        "\t\t  \"resample\": 2,\n",
        "\t\t  \"rescale_factor\": 0.00392156862745098,\n",
        "\t\t  \"size\": {\n",
        "\t\t    \"height\": 224,\n",
        "\t\t    \"width\": 224\n",
        "\t\t  }\n",
        "\t\t}\n",
        "\ttext/plain\n",
        "\t\tDownloading (…)rocessor_config.json:   0%|          | 0.00/276 [00:00<?, ?B/s]\n",
        "\n",
        "Кодовая ячейка <OeVlJYTxewY_>\n",
        "# %% [code]\n",
        "train_dataset = LettersDataset(train_df, beit_feature_extractor, transform=process_example)\n",
        "test_dataset = LettersDataset(test_df, beit_feature_extractor, transform=process_example)\n",
        "\n",
        "Кодовая ячейка <I4s8lO3Xey9K>\n",
        "# %% [code]\n",
        "labels = df.labels.unique()\n",
        "\n",
        "model = BeitForImageClassification.from_pretrained(\n",
        "    beit_model_name,\n",
        "    num_labels=labels.shape[0],\n",
        "    id2label={str(i): c for i, c in enumerate(le.classes_)},  # to convert id of class to real label\n",
        "    label2id={c: str(i) for i, c in enumerate(le.classes_)},\n",
        "    ignore_mismatched_sizes=True\n",
        ")\n",
        "Получены выходные данные.\n",
        "5KB\n",
        "\tStream\n",
        "\t\tloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--beit-base-patch16-224/snapshots/5f0307aec45cf4c914ee8700108b779fb8390654/config.json\n",
        "\t\tModel config BeitConfig {\n",
        "\t\t  \"architectures\": [\n",
        "\t\t    \"BeitForImageClassification\"\n",
        "\t\t  ],\n",
        "\t\t  \"attention_probs_dropout_prob\": 0.0,\n",
        "\t\t  \"auxiliary_channels\": 256,\n",
        "\t\t  \"auxiliary_concat_input\": false,\n",
        "\t\t  \"auxiliary_loss_weight\": 0.4,\n",
        "\t\t  \"auxiliary_num_convs\": 1,\n",
        "\t\t  \"drop_path_rate\": 0.1,\n",
        "\t\t  \"hidden_act\": \"gelu\",\n",
        "\t\t  \"hidden_dropout_prob\": 0.0,\n",
        "\t\t  \"hidden_size\": 768,\n",
        "\t\t  \"id2label\": {\n",
        "\t\t    \"0\": \"-\",\n",
        "\t\t    \"1\": \"\\u0430\",\n",
        "\t\t    \"10\": \"\\u0439\",\n",
        "\t\t    \"11\": \"\\u043a\",\n",
        "\t\t    \"12\": \"\\u043b\",\n",
        "\t\t    \"13\": \"\\u043c\",\n",
        "\t\t    \"14\": \"\\u043d\",\n",
        "\t\t    \"15\": \"\\u043e\",\n",
        "\t\t    \"16\": \"\\u043f\",\n",
        "\t\t    \"17\": \"\\u0440\",\n",
        "\t\t    \"18\": \"\\u0441\",\n",
        "\t\t    \"19\": \"\\u0442\",\n",
        "\t\t    \"2\": \"\\u0431\",\n",
        "\t\t    \"20\": \"\\u0443\",\n",
        "\t\t    \"21\": \"\\u0444\",\n",
        "\t\t    \"22\": \"\\u0445\",\n",
        "\t\t    \"23\": \"\\u0446\",\n",
        "\t\t    \"24\": \"\\u0447\",\n",
        "\t\t    \"25\": \"\\u0448\",\n",
        "\t\t    \"26\": \"\\u0449\",\n",
        "\t\t    \"27\": \"\\u044a\",\n",
        "\t\t    \"28\": \"\\u044b\",\n",
        "\t\t    \"29\": \"\\u044c\",\n",
        "\t\t    \"3\": \"\\u0432\",\n",
        "\t\t    \"30\": \"\\u044d\",\n",
        "\t\t    \"31\": \"\\u044e\",\n",
        "\t\t    \"32\": \"\\u044f\",\n",
        "\t\t    \"33\": \"\\u0451\",\n",
        "\t\t    \"4\": \"\\u0433\",\n",
        "\t\t    \"5\": \"\\u0434\",\n",
        "\t\t    \"6\": \"\\u0435\",\n",
        "\t\t    \"7\": \"\\u0436\",\n",
        "\t\t    \"8\": \"\\u0437\",\n",
        "\t\t    \"9\": \"\\u0438\"\n",
        "\t\t  },\n",
        "\t\t  \"image_size\": 224,\n",
        "\t\t  \"initializer_range\": 0.02,\n",
        "\t\t  \"intermediate_size\": 3072,\n",
        "\t\t  \"label2id\": {\n",
        "\t\t    \"-\": \"0\",\n",
        "\t\t    \"\\u0430\": \"1\",\n",
        "\t\t    \"\\u0431\": \"2\",\n",
        "\t\t    \"\\u0432\": \"3\",\n",
        "\t\t    \"\\u0433\": \"4\",\n",
        "\t\t    \"\\u0434\": \"5\",\n",
        "\t\t    \"\\u0435\": \"6\",\n",
        "\t\t    \"\\u0436\": \"7\",\n",
        "\t\t    \"\\u0437\": \"8\",\n",
        "\t\t    \"\\u0438\": \"9\",\n",
        "\t\t    \"\\u0439\": \"10\",\n",
        "\t\t    \"\\u043a\": \"11\",\n",
        "\t\t    \"\\u043b\": \"12\",\n",
        "\t\t    \"\\u043c\": \"13\",\n",
        "\t\t    \"\\u043d\": \"14\",\n",
        "\t\t    \"\\u043e\": \"15\",\n",
        "\t\t    \"\\u043f\": \"16\",\n",
        "\t\t    \"\\u0440\": \"17\",\n",
        "\t\t    \"\\u0441\": \"18\",\n",
        "\t\t    \"\\u0442\": \"19\",\n",
        "\t\t    \"\\u0443\": \"20\",\n",
        "\t\t    \"\\u0444\": \"21\",\n",
        "\t\t    \"\\u0445\": \"22\",\n",
        "\t\t    \"\\u0446\": \"23\",\n",
        "\t\t    \"\\u0447\": \"24\",\n",
        "\t\t    \"\\u0448\": \"25\",\n",
        "\t\t    \"\\u0449\": \"26\",\n",
        "\t\t    \"\\u044a\": \"27\",\n",
        "\t\t    \"\\u044b\": \"28\",\n",
        "\t\t    \"\\u044c\": \"29\",\n",
        "\t\t    \"\\u044d\": \"30\",\n",
        "\t\t    \"\\u044e\": \"31\",\n",
        "\t\t    \"\\u044f\": \"32\",\n",
        "\t\t    \"\\u0451\": \"33\"\n",
        "\t\t  },\n",
        "\t\t  \"layer_norm_eps\": 1e-12,\n",
        "\t\t  \"layer_scale_init_value\": 0.1,\n",
        "\t\t  \"model_type\": \"beit\",\n",
        "\t\t  \"num_attention_heads\": 12,\n",
        "\t\t  \"num_channels\": 3,\n",
        "\t\t  \"num_hidden_layers\": 12,\n",
        "\t\t  \"out_indices\": [\n",
        "\t\t    3,\n",
        "\t\t    5,\n",
        "\t\t    7,\n",
        "\t\t    11\n",
        "\t\t  ],\n",
        "\t\t  \"patch_size\": 16,\n",
        "\t\t  \"pool_scales\": [\n",
        "\t\t    1,\n",
        "\t\t    2,\n",
        "\t\t    3,\n",
        "\t\t    6\n",
        "\t\t  ],\n",
        "\t\t  \"semantic_loss_ignore_index\": 255,\n",
        "\t\t  \"torch_dtype\": \"float32\",\n",
        "\t\t  \"transformers_version\": \"4.26.1\",\n",
        "\t\t  \"use_absolute_position_embeddings\": false,\n",
        "\t\t  \"use_auxiliary_head\": true,\n",
        "\t\t  \"use_mask_token\": false,\n",
        "\t\t  \"use_mean_pooling\": true,\n",
        "\t\t  \"use_relative_position_bias\": true,\n",
        "\t\t  \"use_shared_relative_position_bias\": false,\n",
        "\t\t  \"vocab_size\": 8192\n",
        "\t\t}\n",
        "\t\tloading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--microsoft--beit-base-patch16-224/snapshots/5f0307aec45cf4c914ee8700108b779fb8390654/pytorch_model.bin\n",
        "\t\tAll model checkpoint weights were used when initializing BeitForImageClassification.\n",
        "\n",
        "\t\tSome weights of BeitForImageClassification were not initialized from the model checkpoint at microsoft/beit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
        "\t\t- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([34, 768]) in the model instantiated\n",
        "\t\t- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([34]) in the model instantiated\n",
        "\t\tYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
        "\ttext/plain\n",
        "\t\tDownloading (…)lve/main/config.json:   0%|          | 0.00/69.9k [00:00<?, ?B/s]\n",
        "\t\tDownloading pytorch_model.bin:   0%|          | 0.00/350M [00:00<?, ?B/s]\n",
        "\n",
        "Кодовая ячейка <SP_k0hvUfBTp>\n",
        "# %% [code]\n",
        "model.to(device)\n",
        "Получены выходные данные.\n",
        "17KB\n",
        "\ttext/plain\n",
        "\t\tBeitForImageClassification(\n",
        "\t\t  (beit): BeitModel(\n",
        "\t\t    (embeddings): BeitEmbeddings(\n",
        "\t\t      (patch_embeddings): BeitPatchEmbeddings(\n",
        "\t\t        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
        "\t\t      )\n",
        "\t\t      (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t    )\n",
        "\t\t    (encoder): BeitEncoder(\n",
        "\t\t      (layer): ModuleList(\n",
        "\t\t        (0): BeitLayer(\n",
        "\t\t          (attention): BeitAttention(\n",
        "\t\t            (attention): BeitSelfAttention(\n",
        "\t\t              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (key): Linear(in_features=768, out_features=768, bias=False)\n",
        "\t\t              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t              (relative_position_bias): BeitRelativePositionBias()\n",
        "\t\t            )\n",
        "\t\t            (output): BeitSelfOutput(\n",
        "\t\t              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t          )\n",
        "\t\t          (intermediate): BeitIntermediate(\n",
        "\t\t            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "\t\t            (intermediate_act_fn): GELUActivation()\n",
        "\t\t          )\n",
        "\t\t          (output): BeitOutput(\n",
        "\t\t            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "\t\t            (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t          )\n",
        "\t\t          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t          (drop_path): Identity()\n",
        "\t\t          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t        )\n",
        "\t\t        (1): BeitLayer(\n",
        "\t\t          (attention): BeitAttention(\n",
        "\t\t            (attention): BeitSelfAttention(\n",
        "\t\t              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (key): Linear(in_features=768, out_features=768, bias=False)\n",
        "\t\t              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t              (relative_position_bias): BeitRelativePositionBias()\n",
        "\t\t            )\n",
        "\t\t            (output): BeitSelfOutput(\n",
        "\t\t              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t          )\n",
        "\t\t          (intermediate): BeitIntermediate(\n",
        "\t\t            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "\t\t            (intermediate_act_fn): GELUActivation()\n",
        "\t\t          )\n",
        "\t\t          (output): BeitOutput(\n",
        "\t\t            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "\t\t            (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t          )\n",
        "\t\t          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t          (drop_path): BeitDropPath(p=0.00909090880304575)\n",
        "\t\t          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t        )\n",
        "\t\t        (2): BeitLayer(\n",
        "\t\t          (attention): BeitAttention(\n",
        "\t\t            (attention): BeitSelfAttention(\n",
        "\t\t              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (key): Linear(in_features=768, out_features=768, bias=False)\n",
        "\t\t              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t              (relative_position_bias): BeitRelativePositionBias()\n",
        "\t\t            )\n",
        "\t\t            (output): BeitSelfOutput(\n",
        "\t\t              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t          )\n",
        "\t\t          (intermediate): BeitIntermediate(\n",
        "\t\t            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "\t\t            (intermediate_act_fn): GELUActivation()\n",
        "\t\t          )\n",
        "\t\t          (output): BeitOutput(\n",
        "\t\t            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "\t\t            (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t          )\n",
        "\t\t          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t          (drop_path): BeitDropPath(p=0.0181818176060915)\n",
        "\t\t          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t        )\n",
        "\t\t        (3): BeitLayer(\n",
        "\t\t          (attention): BeitAttention(\n",
        "\t\t            (attention): BeitSelfAttention(\n",
        "\t\t              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (key): Linear(in_features=768, out_features=768, bias=False)\n",
        "\t\t              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t              (relative_position_bias): BeitRelativePositionBias()\n",
        "\t\t            )\n",
        "\t\t            (output): BeitSelfOutput(\n",
        "\t\t              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t          )\n",
        "\t\t          (intermediate): BeitIntermediate(\n",
        "\t\t            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "\t\t            (intermediate_act_fn): GELUActivation()\n",
        "\t\t          )\n",
        "\t\t          (output): BeitOutput(\n",
        "\t\t            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "\t\t            (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t          )\n",
        "\t\t          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t          (drop_path): BeitDropPath(p=0.027272727340459824)\n",
        "\t\t          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t        )\n",
        "\t\t        (4): BeitLayer(\n",
        "\t\t          (attention): BeitAttention(\n",
        "\t\t            (attention): BeitSelfAttention(\n",
        "\t\t              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (key): Linear(in_features=768, out_features=768, bias=False)\n",
        "\t\t              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t              (relative_position_bias): BeitRelativePositionBias()\n",
        "\t\t            )\n",
        "\t\t            (output): BeitSelfOutput(\n",
        "\t\t              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t          )\n",
        "\t\t          (intermediate): BeitIntermediate(\n",
        "\t\t            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "\t\t            (intermediate_act_fn): GELUActivation()\n",
        "\t\t          )\n",
        "\t\t          (output): BeitOutput(\n",
        "\t\t            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "\t\t            (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t          )\n",
        "\t\t          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t          (drop_path): BeitDropPath(p=0.036363635212183)\n",
        "\t\t          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t        )\n",
        "\t\t        (5): BeitLayer(\n",
        "\t\t          (attention): BeitAttention(\n",
        "\t\t            (attention): BeitSelfAttention(\n",
        "\t\t              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (key): Linear(in_features=768, out_features=768, bias=False)\n",
        "\t\t              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t              (relative_position_bias): BeitRelativePositionBias()\n",
        "\t\t            )\n",
        "\t\t            (output): BeitSelfOutput(\n",
        "\t\t              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t          )\n",
        "\t\t          (intermediate): BeitIntermediate(\n",
        "\t\t            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "\t\t            (intermediate_act_fn): GELUActivation()\n",
        "\t\t          )\n",
        "\t\t          (output): BeitOutput(\n",
        "\t\t            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "\t\t            (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t          )\n",
        "\t\t          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t          (drop_path): BeitDropPath(p=0.045454543083906174)\n",
        "\t\t          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t        )\n",
        "\t\t        (6): BeitLayer(\n",
        "\t\t          (attention): BeitAttention(\n",
        "\t\t            (attention): BeitSelfAttention(\n",
        "\t\t              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (key): Linear(in_features=768, out_features=768, bias=False)\n",
        "\t\t              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t              (relative_position_bias): BeitRelativePositionBias()\n",
        "\t\t            )\n",
        "\t\t            (output): BeitSelfOutput(\n",
        "\t\t              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t          )\n",
        "\t\t          (intermediate): BeitIntermediate(\n",
        "\t\t            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "\t\t            (intermediate_act_fn): GELUActivation()\n",
        "\t\t          )\n",
        "\t\t          (output): BeitOutput(\n",
        "\t\t            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "\t\t            (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t          )\n",
        "\t\t          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t          (drop_path): BeitDropPath(p=0.054545458406209946)\n",
        "\t\t          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t        )\n",
        "\t\t        (7): BeitLayer(\n",
        "\t\t          (attention): BeitAttention(\n",
        "\t\t            (attention): BeitSelfAttention(\n",
        "\t\t              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (key): Linear(in_features=768, out_features=768, bias=False)\n",
        "\t\t              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t              (relative_position_bias): BeitRelativePositionBias()\n",
        "\t\t            )\n",
        "\t\t            (output): BeitSelfOutput(\n",
        "\t\t              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t          )\n",
        "\t\t          (intermediate): BeitIntermediate(\n",
        "\t\t            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "\t\t            (intermediate_act_fn): GELUActivation()\n",
        "\t\t          )\n",
        "\t\t          (output): BeitOutput(\n",
        "\t\t            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "\t\t            (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t          )\n",
        "\t\t          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t          (drop_path): BeitDropPath(p=0.06363636255264282)\n",
        "\t\t          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t        )\n",
        "\t\t        (8): BeitLayer(\n",
        "\t\t          (attention): BeitAttention(\n",
        "\t\t            (attention): BeitSelfAttention(\n",
        "\t\t              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (key): Linear(in_features=768, out_features=768, bias=False)\n",
        "\t\t              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t              (relative_position_bias): BeitRelativePositionBias()\n",
        "\t\t            )\n",
        "\t\t            (output): BeitSelfOutput(\n",
        "\t\t              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t          )\n",
        "\t\t          (intermediate): BeitIntermediate(\n",
        "\t\t            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "\t\t            (intermediate_act_fn): GELUActivation()\n",
        "\t\t          )\n",
        "\t\t          (output): BeitOutput(\n",
        "\t\t            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "\t\t            (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t          )\n",
        "\t\t          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t          (drop_path): BeitDropPath(p=0.0727272778749466)\n",
        "\t\t          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t        )\n",
        "\t\t        (9): BeitLayer(\n",
        "\t\t          (attention): BeitAttention(\n",
        "\t\t            (attention): BeitSelfAttention(\n",
        "\t\t              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (key): Linear(in_features=768, out_features=768, bias=False)\n",
        "\t\t              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t              (relative_position_bias): BeitRelativePositionBias()\n",
        "\t\t            )\n",
        "\t\t            (output): BeitSelfOutput(\n",
        "\t\t              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t          )\n",
        "\t\t          (intermediate): BeitIntermediate(\n",
        "\t\t            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "\t\t            (intermediate_act_fn): GELUActivation()\n",
        "\t\t          )\n",
        "\t\t          (output): BeitOutput(\n",
        "\t\t            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "\t\t            (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t          )\n",
        "\t\t          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t          (drop_path): BeitDropPath(p=0.08181818574666977)\n",
        "\t\t          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t        )\n",
        "\t\t        (10): BeitLayer(\n",
        "\t\t          (attention): BeitAttention(\n",
        "\t\t            (attention): BeitSelfAttention(\n",
        "\t\t              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (key): Linear(in_features=768, out_features=768, bias=False)\n",
        "\t\t              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t              (relative_position_bias): BeitRelativePositionBias()\n",
        "\t\t            )\n",
        "\t\t            (output): BeitSelfOutput(\n",
        "\t\t              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t          )\n",
        "\t\t          (intermediate): BeitIntermediate(\n",
        "\t\t            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "\t\t            (intermediate_act_fn): GELUActivation()\n",
        "\t\t          )\n",
        "\t\t          (output): BeitOutput(\n",
        "\t\t            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "\t\t            (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t          )\n",
        "\t\t          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t          (drop_path): BeitDropPath(p=0.09090909361839294)\n",
        "\t\t          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t        )\n",
        "\t\t        (11): BeitLayer(\n",
        "\t\t          (attention): BeitAttention(\n",
        "\t\t            (attention): BeitSelfAttention(\n",
        "\t\t              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (key): Linear(in_features=768, out_features=768, bias=False)\n",
        "\t\t              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t              (relative_position_bias): BeitRelativePositionBias()\n",
        "\t\t            )\n",
        "\t\t            (output): BeitSelfOutput(\n",
        "\t\t              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t          )\n",
        "\t\t          (intermediate): BeitIntermediate(\n",
        "\t\t            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "\t\t            (intermediate_act_fn): GELUActivation()\n",
        "\t\t          )\n",
        "\t\t          (output): BeitOutput(\n",
        "\t\t            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "\t\t            (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t          )\n",
        "\t\t          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t          (drop_path): BeitDropPath(p=0.10000000149011612)\n",
        "\t\t          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t        )\n",
        "\t\t      )\n",
        "\t\t    )\n",
        "\t\t    (layernorm): Identity()\n",
        "\t\t    (pooler): BeitPooler(\n",
        "\t\t      (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t    )\n",
        "\t\t  )\n",
        "\t\t  (classifier): Linear(in_features=768, out_features=34, bias=True)\n",
        "\t\t)\n",
        "\n",
        "Кодовая ячейка <NRgTP-YZezIU>\n",
        "# %% [code]\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./beit-base-letters\",\n",
        "    per_device_train_batch_size=16,\n",
        "    num_train_epochs=4,\n",
        "    fp16=True,\n",
        "    logging_steps=100,\n",
        "    learning_rate=2e-4,\n",
        "    save_total_limit=2,\n",
        "    remove_unused_columns=False,\n",
        "    push_to_hub=False,\n",
        "    report_to='tensorboard',\n",
        ")\n",
        "Получены выходные данные.\n",
        "0KB\n",
        "\tStream\n",
        "\t\tPyTorch: setting up devices\n",
        "\n",
        "Кодовая ячейка <9-ZI6WOnfFM->\n",
        "# %% [code]\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=collate_fn,\n",
        "    compute_metrics=compute_metrics,\n",
        "    train_dataset=train_dataset,\n",
        "    tokenizer=beit_feature_extractor,\n",
        ")\n",
        "Получены выходные данные.\n",
        "0KB\n",
        "\tStream\n",
        "\t\tUsing cuda_amp half precision backend\n",
        "\n",
        "Кодовая ячейка <u1v3Ymm9fSiN>\n",
        "# %% [code]\n",
        "trainer.train()\n",
        "trainer.save_model('/content/drive/MyDrive/models/beit-base-letters_14032023')\n",
        "Получены выходные данные.\n",
        "29KB\n",
        "\tStream\n",
        "\t\t***** Running training *****\n",
        "\t\t  Num examples = 70761\n",
        "\t\t  Num Epochs = 4\n",
        "\t\t  Instantaneous batch size per device = 16\n",
        "\t\t  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
        "\t\t  Gradient Accumulation steps = 1\n",
        "\t\t  Total optimization steps = 17692\n",
        "\t\t  Number of trainable parameters = 85788130\n",
        "\t\tSaving model checkpoint to ./beit-base-letters/checkpoint-500\n",
        "\t\tConfiguration saved in ./beit-base-letters/checkpoint-500/config.json\n",
        "\t\tModel weights saved in ./beit-base-letters/checkpoint-500/pytorch_model.bin\n",
        "\t\tImage processor saved in ./beit-base-letters/checkpoint-500/preprocessor_config.json\n",
        "\t\tSaving model checkpoint to ./beit-base-letters/checkpoint-1000\n",
        "\t\tConfiguration saved in ./beit-base-letters/checkpoint-1000/config.json\n",
        "\t\tModel weights saved in ./beit-base-letters/checkpoint-1000/pytorch_model.bin\n",
        "\t\tImage processor saved in ./beit-base-letters/checkpoint-1000/preprocessor_config.json\n",
        "\t\tSaving model checkpoint to ./beit-base-letters/checkpoint-1500\n",
        "\t\tConfiguration saved in ./beit-base-letters/checkpoint-1500/config.json\n",
        "\t\tModel weights saved in ./beit-base-letters/checkpoint-1500/pytorch_model.bin\n",
        "\t\tImage processor saved in ./beit-base-letters/checkpoint-1500/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [beit-base-letters/checkpoint-500] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./beit-base-letters/checkpoint-2000\n",
        "\t\tConfiguration saved in ./beit-base-letters/checkpoint-2000/config.json\n",
        "\t\tModel weights saved in ./beit-base-letters/checkpoint-2000/pytorch_model.bin\n",
        "\t\tImage processor saved in ./beit-base-letters/checkpoint-2000/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [beit-base-letters/checkpoint-1000] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./beit-base-letters/checkpoint-2500\n",
        "\t\tConfiguration saved in ./beit-base-letters/checkpoint-2500/config.json\n",
        "\t\tModel weights saved in ./beit-base-letters/checkpoint-2500/pytorch_model.bin\n",
        "\t\tImage processor saved in ./beit-base-letters/checkpoint-2500/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [beit-base-letters/checkpoint-1500] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./beit-base-letters/checkpoint-3000\n",
        "\t\tConfiguration saved in ./beit-base-letters/checkpoint-3000/config.json\n",
        "\t\tModel weights saved in ./beit-base-letters/checkpoint-3000/pytorch_model.bin\n",
        "\t\tImage processor saved in ./beit-base-letters/checkpoint-3000/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [beit-base-letters/checkpoint-2000] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./beit-base-letters/checkpoint-3500\n",
        "\t\tConfiguration saved in ./beit-base-letters/checkpoint-3500/config.json\n",
        "\t\tModel weights saved in ./beit-base-letters/checkpoint-3500/pytorch_model.bin\n",
        "\t\tImage processor saved in ./beit-base-letters/checkpoint-3500/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [beit-base-letters/checkpoint-2500] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./beit-base-letters/checkpoint-4000\n",
        "\t\tConfiguration saved in ./beit-base-letters/checkpoint-4000/config.json\n",
        "\t\tModel weights saved in ./beit-base-letters/checkpoint-4000/pytorch_model.bin\n",
        "\t\tImage processor saved in ./beit-base-letters/checkpoint-4000/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [beit-base-letters/checkpoint-3000] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./beit-base-letters/checkpoint-4500\n",
        "\t\tConfiguration saved in ./beit-base-letters/checkpoint-4500/config.json\n",
        "\t\tModel weights saved in ./beit-base-letters/checkpoint-4500/pytorch_model.bin\n",
        "\t\tImage processor saved in ./beit-base-letters/checkpoint-4500/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [beit-base-letters/checkpoint-3500] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./beit-base-letters/checkpoint-5000\n",
        "\t\tConfiguration saved in ./beit-base-letters/checkpoint-5000/config.json\n",
        "\t\tModel weights saved in ./beit-base-letters/checkpoint-5000/pytorch_model.bin\n",
        "\t\tImage processor saved in ./beit-base-letters/checkpoint-5000/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [beit-base-letters/checkpoint-4000] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./beit-base-letters/checkpoint-5500\n",
        "\t\tConfiguration saved in ./beit-base-letters/checkpoint-5500/config.json\n",
        "\t\tModel weights saved in ./beit-base-letters/checkpoint-5500/pytorch_model.bin\n",
        "\t\tImage processor saved in ./beit-base-letters/checkpoint-5500/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [beit-base-letters/checkpoint-4500] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./beit-base-letters/checkpoint-6000\n",
        "\t\tConfiguration saved in ./beit-base-letters/checkpoint-6000/config.json\n",
        "\t\tModel weights saved in ./beit-base-letters/checkpoint-6000/pytorch_model.bin\n",
        "\t\tImage processor saved in ./beit-base-letters/checkpoint-6000/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [beit-base-letters/checkpoint-5000] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./beit-base-letters/checkpoint-6500\n",
        "\t\tConfiguration saved in ./beit-base-letters/checkpoint-6500/config.json\n",
        "\t\tModel weights saved in ./beit-base-letters/checkpoint-6500/pytorch_model.bin\n",
        "\t\tImage processor saved in ./beit-base-letters/checkpoint-6500/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [beit-base-letters/checkpoint-5500] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./beit-base-letters/checkpoint-7000\n",
        "\t\tConfiguration saved in ./beit-base-letters/checkpoint-7000/config.json\n",
        "\t\tModel weights saved in ./beit-base-letters/checkpoint-7000/pytorch_model.bin\n",
        "\t\tImage processor saved in ./beit-base-letters/checkpoint-7000/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [beit-base-letters/checkpoint-6000] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./beit-base-letters/checkpoint-7500\n",
        "\t\tConfiguration saved in ./beit-base-letters/checkpoint-7500/config.json\n",
        "\t\tModel weights saved in ./beit-base-letters/checkpoint-7500/pytorch_model.bin\n",
        "\t\tImage processor saved in ./beit-base-letters/checkpoint-7500/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [beit-base-letters/checkpoint-6500] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./beit-base-letters/checkpoint-8000\n",
        "\t\tConfiguration saved in ./beit-base-letters/checkpoint-8000/config.json\n",
        "\t\tModel weights saved in ./beit-base-letters/checkpoint-8000/pytorch_model.bin\n",
        "\t\tImage processor saved in ./beit-base-letters/checkpoint-8000/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [beit-base-letters/checkpoint-7000] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./beit-base-letters/checkpoint-8500\n",
        "\t\tConfiguration saved in ./beit-base-letters/checkpoint-8500/config.json\n",
        "\t\tModel weights saved in ./beit-base-letters/checkpoint-8500/pytorch_model.bin\n",
        "\t\tImage processor saved in ./beit-base-letters/checkpoint-8500/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [beit-base-letters/checkpoint-7500] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./beit-base-letters/checkpoint-9000\n",
        "\t\tConfiguration saved in ./beit-base-letters/checkpoint-9000/config.json\n",
        "\t\tModel weights saved in ./beit-base-letters/checkpoint-9000/pytorch_model.bin\n",
        "\t\tImage processor saved in ./beit-base-letters/checkpoint-9000/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [beit-base-letters/checkpoint-8000] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./beit-base-letters/checkpoint-9500\n",
        "\t\tConfiguration saved in ./beit-base-letters/checkpoint-9500/config.json\n",
        "\t\tModel weights saved in ./beit-base-letters/checkpoint-9500/pytorch_model.bin\n",
        "\t\tImage processor saved in ./beit-base-letters/checkpoint-9500/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [beit-base-letters/checkpoint-8500] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./beit-base-letters/checkpoint-10000\n",
        "\t\tConfiguration saved in ./beit-base-letters/checkpoint-10000/config.json\n",
        "\t\tModel weights saved in ./beit-base-letters/checkpoint-10000/pytorch_model.bin\n",
        "\t\tImage processor saved in ./beit-base-letters/checkpoint-10000/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [beit-base-letters/checkpoint-9000] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./beit-base-letters/checkpoint-10500\n",
        "\t\tConfiguration saved in ./beit-base-letters/checkpoint-10500/config.json\n",
        "\t\tModel weights saved in ./beit-base-letters/checkpoint-10500/pytorch_model.bin\n",
        "\t\tImage processor saved in ./beit-base-letters/checkpoint-10500/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [beit-base-letters/checkpoint-9500] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./beit-base-letters/checkpoint-11000\n",
        "\t\tConfiguration saved in ./beit-base-letters/checkpoint-11000/config.json\n",
        "\t\tModel weights saved in ./beit-base-letters/checkpoint-11000/pytorch_model.bin\n",
        "\t\tImage processor saved in ./beit-base-letters/checkpoint-11000/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [beit-base-letters/checkpoint-10000] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./beit-base-letters/checkpoint-11500\n",
        "\t\tConfiguration saved in ./beit-base-letters/checkpoint-11500/config.json\n",
        "\t\tModel weights saved in ./beit-base-letters/checkpoint-11500/pytorch_model.bin\n",
        "\t\tImage processor saved in ./beit-base-letters/checkpoint-11500/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [beit-base-letters/checkpoint-10500] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./beit-base-letters/checkpoint-12000\n",
        "\t\tConfiguration saved in ./beit-base-letters/checkpoint-12000/config.json\n",
        "\t\tModel weights saved in ./beit-base-letters/checkpoint-12000/pytorch_model.bin\n",
        "\t\tImage processor saved in ./beit-base-letters/checkpoint-12000/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [beit-base-letters/checkpoint-11000] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./beit-base-letters/checkpoint-12500\n",
        "\t\tConfiguration saved in ./beit-base-letters/checkpoint-12500/config.json\n",
        "\t\tModel weights saved in ./beit-base-letters/checkpoint-12500/pytorch_model.bin\n",
        "\t\tImage processor saved in ./beit-base-letters/checkpoint-12500/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [beit-base-letters/checkpoint-11500] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./beit-base-letters/checkpoint-13000\n",
        "\t\tConfiguration saved in ./beit-base-letters/checkpoint-13000/config.json\n",
        "\t\tModel weights saved in ./beit-base-letters/checkpoint-13000/pytorch_model.bin\n",
        "\t\tImage processor saved in ./beit-base-letters/checkpoint-13000/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [beit-base-letters/checkpoint-12000] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./beit-base-letters/checkpoint-13500\n",
        "\t\tConfiguration saved in ./beit-base-letters/checkpoint-13500/config.json\n",
        "\t\tModel weights saved in ./beit-base-letters/checkpoint-13500/pytorch_model.bin\n",
        "\t\tImage processor saved in ./beit-base-letters/checkpoint-13500/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [beit-base-letters/checkpoint-12500] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./beit-base-letters/checkpoint-14000\n",
        "\t\tConfiguration saved in ./beit-base-letters/checkpoint-14000/config.json\n",
        "\t\tModel weights saved in ./beit-base-letters/checkpoint-14000/pytorch_model.bin\n",
        "\t\tImage processor saved in ./beit-base-letters/checkpoint-14000/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [beit-base-letters/checkpoint-13000] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./beit-base-letters/checkpoint-14500\n",
        "\t\tConfiguration saved in ./beit-base-letters/checkpoint-14500/config.json\n",
        "\t\tModel weights saved in ./beit-base-letters/checkpoint-14500/pytorch_model.bin\n",
        "\t\tImage processor saved in ./beit-base-letters/checkpoint-14500/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [beit-base-letters/checkpoint-13500] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./beit-base-letters/checkpoint-15000\n",
        "\t\tConfiguration saved in ./beit-base-letters/checkpoint-15000/config.json\n",
        "\t\tModel weights saved in ./beit-base-letters/checkpoint-15000/pytorch_model.bin\n",
        "\t\tImage processor saved in ./beit-base-letters/checkpoint-15000/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [beit-base-letters/checkpoint-14000] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./beit-base-letters/checkpoint-15500\n",
        "\t\tConfiguration saved in ./beit-base-letters/checkpoint-15500/config.json\n",
        "\t\tModel weights saved in ./beit-base-letters/checkpoint-15500/pytorch_model.bin\n",
        "\t\tImage processor saved in ./beit-base-letters/checkpoint-15500/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [beit-base-letters/checkpoint-14500] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./beit-base-letters/checkpoint-16000\n",
        "\t\tConfiguration saved in ./beit-base-letters/checkpoint-16000/config.json\n",
        "\t\tModel weights saved in ./beit-base-letters/checkpoint-16000/pytorch_model.bin\n",
        "\t\tImage processor saved in ./beit-base-letters/checkpoint-16000/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [beit-base-letters/checkpoint-15000] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./beit-base-letters/checkpoint-16500\n",
        "\t\tConfiguration saved in ./beit-base-letters/checkpoint-16500/config.json\n",
        "\t\tModel weights saved in ./beit-base-letters/checkpoint-16500/pytorch_model.bin\n",
        "\t\tImage processor saved in ./beit-base-letters/checkpoint-16500/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [beit-base-letters/checkpoint-15500] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./beit-base-letters/checkpoint-17000\n",
        "\t\tConfiguration saved in ./beit-base-letters/checkpoint-17000/config.json\n",
        "\t\tModel weights saved in ./beit-base-letters/checkpoint-17000/pytorch_model.bin\n",
        "\t\tImage processor saved in ./beit-base-letters/checkpoint-17000/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [beit-base-letters/checkpoint-16000] due to args.save_total_limit\n",
        "\t\tSaving model checkpoint to ./beit-base-letters/checkpoint-17500\n",
        "\t\tConfiguration saved in ./beit-base-letters/checkpoint-17500/config.json\n",
        "\t\tModel weights saved in ./beit-base-letters/checkpoint-17500/pytorch_model.bin\n",
        "\t\tImage processor saved in ./beit-base-letters/checkpoint-17500/preprocessor_config.json\n",
        "\t\tDeleting older checkpoint [beit-base-letters/checkpoint-16500] due to args.save_total_limit\n",
        "\n",
        "\n",
        "\t\tTraining completed. Do not forget to share your model on huggingface.co/models =)\n",
        "\n",
        "\n",
        "\t\tSaving model checkpoint to /content/drive/MyDrive/models/beit-base-letters_14032023\n",
        "\t\tConfiguration saved in /content/drive/MyDrive/models/beit-base-letters_14032023/config.json\n",
        "\t\tModel weights saved in /content/drive/MyDrive/models/beit-base-letters_14032023/pytorch_model.bin\n",
        "\t\tImage processor saved in /content/drive/MyDrive/models/beit-base-letters_14032023/preprocessor_config.json\n",
        "\n",
        "Текстовая ячейка <lnzLhVS8jJ1X>\n",
        "# %% [markdown]\n",
        "### Evaluation\n",
        "\n",
        "Кодовая ячейка <nY0uNHSfdhjE>\n",
        "# %% [code]\n",
        "new_beit_model_name = '/content/drive/MyDrive/models/beit-base-letters_14032023'\n",
        "beit_model_name = 'microsoft/beit-base-patch16-224'\n",
        "beit_feature_extractor = BeitFeatureExtractor.from_pretrained(beit_model_name)\n",
        "Получены выходные данные.\n",
        "1KB\n",
        "\tStream\n",
        "\t\tloading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--beit-base-patch16-224/snapshots/5f0307aec45cf4c914ee8700108b779fb8390654/preprocessor_config.json\n",
        "\t\tsize should be a dictionary on of the following set of keys: ({'width', 'height'}, {'shortest_edge'}, {'longest_edge', 'shortest_edge'}), got 224. Converted to {'height': 224, 'width': 224}.\n",
        "\t\tcrop_size should be a dictionary on of the following set of keys: ({'width', 'height'}, {'shortest_edge'}, {'longest_edge', 'shortest_edge'}), got 224. Converted to {'height': 224, 'width': 224}.\n",
        "\t\tImage processor BeitFeatureExtractor {\n",
        "\t\t  \"crop_size\": {\n",
        "\t\t    \"height\": 224,\n",
        "\t\t    \"width\": 224\n",
        "\t\t  },\n",
        "\t\t  \"do_center_crop\": false,\n",
        "\t\t  \"do_normalize\": true,\n",
        "\t\t  \"do_reduce_labels\": false,\n",
        "\t\t  \"do_rescale\": true,\n",
        "\t\t  \"do_resize\": true,\n",
        "\t\t  \"feature_extractor_type\": \"BeitFeatureExtractor\",\n",
        "\t\t  \"image_mean\": [\n",
        "\t\t    0.5,\n",
        "\t\t    0.5,\n",
        "\t\t    0.5\n",
        "\t\t  ],\n",
        "\t\t  \"image_processor_type\": \"BeitFeatureExtractor\",\n",
        "\t\t  \"image_std\": [\n",
        "\t\t    0.5,\n",
        "\t\t    0.5,\n",
        "\t\t    0.5\n",
        "\t\t  ],\n",
        "\t\t  \"resample\": 2,\n",
        "\t\t  \"rescale_factor\": 0.00392156862745098,\n",
        "\t\t  \"size\": {\n",
        "\t\t    \"height\": 224,\n",
        "\t\t    \"width\": 224\n",
        "\t\t  }\n",
        "\t\t}\n",
        "\n",
        "Кодовая ячейка <XRygcgv3T3_O>\n",
        "# %% [code]\n",
        "labels = df.labels.unique()\n",
        "\n",
        "new_model = BeitForImageClassification.from_pretrained(\n",
        "    new_beit_model_name,\n",
        "    num_labels=labels.shape[0],\n",
        "    id2label={str(i): c for i, c in enumerate(le.classes_)},  # to convert id of class to real label\n",
        "    label2id={c: str(i) for i, c in enumerate(le.classes_)},\n",
        "    ignore_mismatched_sizes=True\n",
        ")\n",
        "Получены выходные данные.\n",
        "4KB\n",
        "\tStream\n",
        "\t\tloading configuration file /content/drive/MyDrive/models/beit-base-letters_14032023/config.json\n",
        "\t\tModel config BeitConfig {\n",
        "\t\t  \"_name_or_path\": \"microsoft/beit-base-patch16-224\",\n",
        "\t\t  \"architectures\": [\n",
        "\t\t    \"BeitForImageClassification\"\n",
        "\t\t  ],\n",
        "\t\t  \"attention_probs_dropout_prob\": 0.0,\n",
        "\t\t  \"auxiliary_channels\": 256,\n",
        "\t\t  \"auxiliary_concat_input\": false,\n",
        "\t\t  \"auxiliary_loss_weight\": 0.4,\n",
        "\t\t  \"auxiliary_num_convs\": 1,\n",
        "\t\t  \"drop_path_rate\": 0.1,\n",
        "\t\t  \"hidden_act\": \"gelu\",\n",
        "\t\t  \"hidden_dropout_prob\": 0.0,\n",
        "\t\t  \"hidden_size\": 768,\n",
        "\t\t  \"id2label\": {\n",
        "\t\t    \"0\": \"-\",\n",
        "\t\t    \"1\": \"\\u0430\",\n",
        "\t\t    \"10\": \"\\u0439\",\n",
        "\t\t    \"11\": \"\\u043a\",\n",
        "\t\t    \"12\": \"\\u043b\",\n",
        "\t\t    \"13\": \"\\u043c\",\n",
        "\t\t    \"14\": \"\\u043d\",\n",
        "\t\t    \"15\": \"\\u043e\",\n",
        "\t\t    \"16\": \"\\u043f\",\n",
        "\t\t    \"17\": \"\\u0440\",\n",
        "\t\t    \"18\": \"\\u0441\",\n",
        "\t\t    \"19\": \"\\u0442\",\n",
        "\t\t    \"2\": \"\\u0431\",\n",
        "\t\t    \"20\": \"\\u0443\",\n",
        "\t\t    \"21\": \"\\u0444\",\n",
        "\t\t    \"22\": \"\\u0445\",\n",
        "\t\t    \"23\": \"\\u0446\",\n",
        "\t\t    \"24\": \"\\u0447\",\n",
        "\t\t    \"25\": \"\\u0448\",\n",
        "\t\t    \"26\": \"\\u0449\",\n",
        "\t\t    \"27\": \"\\u044a\",\n",
        "\t\t    \"28\": \"\\u044b\",\n",
        "\t\t    \"29\": \"\\u044c\",\n",
        "\t\t    \"3\": \"\\u0432\",\n",
        "\t\t    \"30\": \"\\u044d\",\n",
        "\t\t    \"31\": \"\\u044e\",\n",
        "\t\t    \"32\": \"\\u044f\",\n",
        "\t\t    \"33\": \"\\u0451\",\n",
        "\t\t    \"4\": \"\\u0433\",\n",
        "\t\t    \"5\": \"\\u0434\",\n",
        "\t\t    \"6\": \"\\u0435\",\n",
        "\t\t    \"7\": \"\\u0436\",\n",
        "\t\t    \"8\": \"\\u0437\",\n",
        "\t\t    \"9\": \"\\u0438\"\n",
        "\t\t  },\n",
        "\t\t  \"image_size\": 224,\n",
        "\t\t  \"initializer_range\": 0.02,\n",
        "\t\t  \"intermediate_size\": 3072,\n",
        "\t\t  \"label2id\": {\n",
        "\t\t    \"-\": \"0\",\n",
        "\t\t    \"\\u0430\": \"1\",\n",
        "\t\t    \"\\u0431\": \"2\",\n",
        "\t\t    \"\\u0432\": \"3\",\n",
        "\t\t    \"\\u0433\": \"4\",\n",
        "\t\t    \"\\u0434\": \"5\",\n",
        "\t\t    \"\\u0435\": \"6\",\n",
        "\t\t    \"\\u0436\": \"7\",\n",
        "\t\t    \"\\u0437\": \"8\",\n",
        "\t\t    \"\\u0438\": \"9\",\n",
        "\t\t    \"\\u0439\": \"10\",\n",
        "\t\t    \"\\u043a\": \"11\",\n",
        "\t\t    \"\\u043b\": \"12\",\n",
        "\t\t    \"\\u043c\": \"13\",\n",
        "\t\t    \"\\u043d\": \"14\",\n",
        "\t\t    \"\\u043e\": \"15\",\n",
        "\t\t    \"\\u043f\": \"16\",\n",
        "\t\t    \"\\u0440\": \"17\",\n",
        "\t\t    \"\\u0441\": \"18\",\n",
        "\t\t    \"\\u0442\": \"19\",\n",
        "\t\t    \"\\u0443\": \"20\",\n",
        "\t\t    \"\\u0444\": \"21\",\n",
        "\t\t    \"\\u0445\": \"22\",\n",
        "\t\t    \"\\u0446\": \"23\",\n",
        "\t\t    \"\\u0447\": \"24\",\n",
        "\t\t    \"\\u0448\": \"25\",\n",
        "\t\t    \"\\u0449\": \"26\",\n",
        "\t\t    \"\\u044a\": \"27\",\n",
        "\t\t    \"\\u044b\": \"28\",\n",
        "\t\t    \"\\u044c\": \"29\",\n",
        "\t\t    \"\\u044d\": \"30\",\n",
        "\t\t    \"\\u044e\": \"31\",\n",
        "\t\t    \"\\u044f\": \"32\",\n",
        "\t\t    \"\\u0451\": \"33\"\n",
        "\t\t  },\n",
        "\t\t  \"layer_norm_eps\": 1e-12,\n",
        "\t\t  \"layer_scale_init_value\": 0.1,\n",
        "\t\t  \"model_type\": \"beit\",\n",
        "\t\t  \"num_attention_heads\": 12,\n",
        "\t\t  \"num_channels\": 3,\n",
        "\t\t  \"num_hidden_layers\": 12,\n",
        "\t\t  \"out_indices\": [\n",
        "\t\t    3,\n",
        "\t\t    5,\n",
        "\t\t    7,\n",
        "\t\t    11\n",
        "\t\t  ],\n",
        "\t\t  \"patch_size\": 16,\n",
        "\t\t  \"pool_scales\": [\n",
        "\t\t    1,\n",
        "\t\t    2,\n",
        "\t\t    3,\n",
        "\t\t    6\n",
        "\t\t  ],\n",
        "\t\t  \"problem_type\": \"single_label_classification\",\n",
        "\t\t  \"semantic_loss_ignore_index\": 255,\n",
        "\t\t  \"torch_dtype\": \"float32\",\n",
        "\t\t  \"transformers_version\": \"4.26.1\",\n",
        "\t\t  \"use_absolute_position_embeddings\": false,\n",
        "\t\t  \"use_auxiliary_head\": true,\n",
        "\t\t  \"use_mask_token\": false,\n",
        "\t\t  \"use_mean_pooling\": true,\n",
        "\t\t  \"use_relative_position_bias\": true,\n",
        "\t\t  \"use_shared_relative_position_bias\": false,\n",
        "\t\t  \"vocab_size\": 8192\n",
        "\t\t}\n",
        "\n",
        "\t\tloading weights file /content/drive/MyDrive/models/beit-base-letters_14032023/pytorch_model.bin\n",
        "\t\tAll model checkpoint weights were used when initializing BeitForImageClassification.\n",
        "\n",
        "\t\tAll the weights of BeitForImageClassification were initialized from the model checkpoint at /content/drive/MyDrive/models/beit-base-letters_14032023.\n",
        "\t\tIf your task is similar to the task the model of the checkpoint was trained on, you can already use BeitForImageClassification for predictions without further training.\n",
        "\n",
        "Кодовая ячейка <VnSheNDfT8x8>\n",
        "# %% [code]\n",
        "new_model.to(device)\n",
        "Получены выходные данные.\n",
        "17KB\n",
        "\ttext/plain\n",
        "\t\tBeitForImageClassification(\n",
        "\t\t  (beit): BeitModel(\n",
        "\t\t    (embeddings): BeitEmbeddings(\n",
        "\t\t      (patch_embeddings): BeitPatchEmbeddings(\n",
        "\t\t        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
        "\t\t      )\n",
        "\t\t      (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t    )\n",
        "\t\t    (encoder): BeitEncoder(\n",
        "\t\t      (layer): ModuleList(\n",
        "\t\t        (0): BeitLayer(\n",
        "\t\t          (attention): BeitAttention(\n",
        "\t\t            (attention): BeitSelfAttention(\n",
        "\t\t              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (key): Linear(in_features=768, out_features=768, bias=False)\n",
        "\t\t              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t              (relative_position_bias): BeitRelativePositionBias()\n",
        "\t\t            )\n",
        "\t\t            (output): BeitSelfOutput(\n",
        "\t\t              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t          )\n",
        "\t\t          (intermediate): BeitIntermediate(\n",
        "\t\t            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "\t\t            (intermediate_act_fn): GELUActivation()\n",
        "\t\t          )\n",
        "\t\t          (output): BeitOutput(\n",
        "\t\t            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "\t\t            (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t          )\n",
        "\t\t          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t          (drop_path): Identity()\n",
        "\t\t          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t        )\n",
        "\t\t        (1): BeitLayer(\n",
        "\t\t          (attention): BeitAttention(\n",
        "\t\t            (attention): BeitSelfAttention(\n",
        "\t\t              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (key): Linear(in_features=768, out_features=768, bias=False)\n",
        "\t\t              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t              (relative_position_bias): BeitRelativePositionBias()\n",
        "\t\t            )\n",
        "\t\t            (output): BeitSelfOutput(\n",
        "\t\t              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t          )\n",
        "\t\t          (intermediate): BeitIntermediate(\n",
        "\t\t            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "\t\t            (intermediate_act_fn): GELUActivation()\n",
        "\t\t          )\n",
        "\t\t          (output): BeitOutput(\n",
        "\t\t            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "\t\t            (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t          )\n",
        "\t\t          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t          (drop_path): BeitDropPath(p=0.00909090880304575)\n",
        "\t\t          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t        )\n",
        "\t\t        (2): BeitLayer(\n",
        "\t\t          (attention): BeitAttention(\n",
        "\t\t            (attention): BeitSelfAttention(\n",
        "\t\t              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (key): Linear(in_features=768, out_features=768, bias=False)\n",
        "\t\t              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t              (relative_position_bias): BeitRelativePositionBias()\n",
        "\t\t            )\n",
        "\t\t            (output): BeitSelfOutput(\n",
        "\t\t              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t          )\n",
        "\t\t          (intermediate): BeitIntermediate(\n",
        "\t\t            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "\t\t            (intermediate_act_fn): GELUActivation()\n",
        "\t\t          )\n",
        "\t\t          (output): BeitOutput(\n",
        "\t\t            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "\t\t            (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t          )\n",
        "\t\t          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t          (drop_path): BeitDropPath(p=0.0181818176060915)\n",
        "\t\t          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t        )\n",
        "\t\t        (3): BeitLayer(\n",
        "\t\t          (attention): BeitAttention(\n",
        "\t\t            (attention): BeitSelfAttention(\n",
        "\t\t              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (key): Linear(in_features=768, out_features=768, bias=False)\n",
        "\t\t              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t              (relative_position_bias): BeitRelativePositionBias()\n",
        "\t\t            )\n",
        "\t\t            (output): BeitSelfOutput(\n",
        "\t\t              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t          )\n",
        "\t\t          (intermediate): BeitIntermediate(\n",
        "\t\t            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "\t\t            (intermediate_act_fn): GELUActivation()\n",
        "\t\t          )\n",
        "\t\t          (output): BeitOutput(\n",
        "\t\t            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "\t\t            (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t          )\n",
        "\t\t          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t          (drop_path): BeitDropPath(p=0.027272727340459824)\n",
        "\t\t          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t        )\n",
        "\t\t        (4): BeitLayer(\n",
        "\t\t          (attention): BeitAttention(\n",
        "\t\t            (attention): BeitSelfAttention(\n",
        "\t\t              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (key): Linear(in_features=768, out_features=768, bias=False)\n",
        "\t\t              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t              (relative_position_bias): BeitRelativePositionBias()\n",
        "\t\t            )\n",
        "\t\t            (output): BeitSelfOutput(\n",
        "\t\t              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t          )\n",
        "\t\t          (intermediate): BeitIntermediate(\n",
        "\t\t            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "\t\t            (intermediate_act_fn): GELUActivation()\n",
        "\t\t          )\n",
        "\t\t          (output): BeitOutput(\n",
        "\t\t            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "\t\t            (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t          )\n",
        "\t\t          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t          (drop_path): BeitDropPath(p=0.036363635212183)\n",
        "\t\t          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t        )\n",
        "\t\t        (5): BeitLayer(\n",
        "\t\t          (attention): BeitAttention(\n",
        "\t\t            (attention): BeitSelfAttention(\n",
        "\t\t              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (key): Linear(in_features=768, out_features=768, bias=False)\n",
        "\t\t              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t              (relative_position_bias): BeitRelativePositionBias()\n",
        "\t\t            )\n",
        "\t\t            (output): BeitSelfOutput(\n",
        "\t\t              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t          )\n",
        "\t\t          (intermediate): BeitIntermediate(\n",
        "\t\t            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "\t\t            (intermediate_act_fn): GELUActivation()\n",
        "\t\t          )\n",
        "\t\t          (output): BeitOutput(\n",
        "\t\t            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "\t\t            (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t          )\n",
        "\t\t          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t          (drop_path): BeitDropPath(p=0.045454543083906174)\n",
        "\t\t          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t        )\n",
        "\t\t        (6): BeitLayer(\n",
        "\t\t          (attention): BeitAttention(\n",
        "\t\t            (attention): BeitSelfAttention(\n",
        "\t\t              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (key): Linear(in_features=768, out_features=768, bias=False)\n",
        "\t\t              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t              (relative_position_bias): BeitRelativePositionBias()\n",
        "\t\t            )\n",
        "\t\t            (output): BeitSelfOutput(\n",
        "\t\t              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t          )\n",
        "\t\t          (intermediate): BeitIntermediate(\n",
        "\t\t            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "\t\t            (intermediate_act_fn): GELUActivation()\n",
        "\t\t          )\n",
        "\t\t          (output): BeitOutput(\n",
        "\t\t            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "\t\t            (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t          )\n",
        "\t\t          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t          (drop_path): BeitDropPath(p=0.054545458406209946)\n",
        "\t\t          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t        )\n",
        "\t\t        (7): BeitLayer(\n",
        "\t\t          (attention): BeitAttention(\n",
        "\t\t            (attention): BeitSelfAttention(\n",
        "\t\t              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (key): Linear(in_features=768, out_features=768, bias=False)\n",
        "\t\t              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t              (relative_position_bias): BeitRelativePositionBias()\n",
        "\t\t            )\n",
        "\t\t            (output): BeitSelfOutput(\n",
        "\t\t              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t          )\n",
        "\t\t          (intermediate): BeitIntermediate(\n",
        "\t\t            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "\t\t            (intermediate_act_fn): GELUActivation()\n",
        "\t\t          )\n",
        "\t\t          (output): BeitOutput(\n",
        "\t\t            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "\t\t            (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t          )\n",
        "\t\t          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t          (drop_path): BeitDropPath(p=0.06363636255264282)\n",
        "\t\t          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t        )\n",
        "\t\t        (8): BeitLayer(\n",
        "\t\t          (attention): BeitAttention(\n",
        "\t\t            (attention): BeitSelfAttention(\n",
        "\t\t              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (key): Linear(in_features=768, out_features=768, bias=False)\n",
        "\t\t              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t              (relative_position_bias): BeitRelativePositionBias()\n",
        "\t\t            )\n",
        "\t\t            (output): BeitSelfOutput(\n",
        "\t\t              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t          )\n",
        "\t\t          (intermediate): BeitIntermediate(\n",
        "\t\t            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "\t\t            (intermediate_act_fn): GELUActivation()\n",
        "\t\t          )\n",
        "\t\t          (output): BeitOutput(\n",
        "\t\t            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "\t\t            (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t          )\n",
        "\t\t          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t          (drop_path): BeitDropPath(p=0.0727272778749466)\n",
        "\t\t          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t        )\n",
        "\t\t        (9): BeitLayer(\n",
        "\t\t          (attention): BeitAttention(\n",
        "\t\t            (attention): BeitSelfAttention(\n",
        "\t\t              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (key): Linear(in_features=768, out_features=768, bias=False)\n",
        "\t\t              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t              (relative_position_bias): BeitRelativePositionBias()\n",
        "\t\t            )\n",
        "\t\t            (output): BeitSelfOutput(\n",
        "\t\t              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t          )\n",
        "\t\t          (intermediate): BeitIntermediate(\n",
        "\t\t            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "\t\t            (intermediate_act_fn): GELUActivation()\n",
        "\t\t          )\n",
        "\t\t          (output): BeitOutput(\n",
        "\t\t            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "\t\t            (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t          )\n",
        "\t\t          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t          (drop_path): BeitDropPath(p=0.08181818574666977)\n",
        "\t\t          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t        )\n",
        "\t\t        (10): BeitLayer(\n",
        "\t\t          (attention): BeitAttention(\n",
        "\t\t            (attention): BeitSelfAttention(\n",
        "\t\t              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (key): Linear(in_features=768, out_features=768, bias=False)\n",
        "\t\t              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t              (relative_position_bias): BeitRelativePositionBias()\n",
        "\t\t            )\n",
        "\t\t            (output): BeitSelfOutput(\n",
        "\t\t              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t          )\n",
        "\t\t          (intermediate): BeitIntermediate(\n",
        "\t\t            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "\t\t            (intermediate_act_fn): GELUActivation()\n",
        "\t\t          )\n",
        "\t\t          (output): BeitOutput(\n",
        "\t\t            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "\t\t            (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t          )\n",
        "\t\t          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t          (drop_path): BeitDropPath(p=0.09090909361839294)\n",
        "\t\t          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t        )\n",
        "\t\t        (11): BeitLayer(\n",
        "\t\t          (attention): BeitAttention(\n",
        "\t\t            (attention): BeitSelfAttention(\n",
        "\t\t              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (key): Linear(in_features=768, out_features=768, bias=False)\n",
        "\t\t              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t              (relative_position_bias): BeitRelativePositionBias()\n",
        "\t\t            )\n",
        "\t\t            (output): BeitSelfOutput(\n",
        "\t\t              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "\t\t              (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t            )\n",
        "\t\t          )\n",
        "\t\t          (intermediate): BeitIntermediate(\n",
        "\t\t            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "\t\t            (intermediate_act_fn): GELUActivation()\n",
        "\t\t          )\n",
        "\t\t          (output): BeitOutput(\n",
        "\t\t            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "\t\t            (dropout): Dropout(p=0.0, inplace=False)\n",
        "\t\t          )\n",
        "\t\t          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t          (drop_path): BeitDropPath(p=0.10000000149011612)\n",
        "\t\t          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t        )\n",
        "\t\t      )\n",
        "\t\t    )\n",
        "\t\t    (layernorm): Identity()\n",
        "\t\t    (pooler): BeitPooler(\n",
        "\t\t      (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
        "\t\t    )\n",
        "\t\t  )\n",
        "\t\t  (classifier): Linear(in_features=768, out_features=34, bias=True)\n",
        "\t\t)\n",
        "\n",
        "Кодовая ячейка <1duQZTXujLQs>\n",
        "# %% [code]\n",
        "actual_labels = []\n",
        "predicted_labels = []\n",
        "\n",
        "for item in tqdm(test_dataset):\n",
        "    pixels = item['pixel_values'].to(device)\n",
        "    outputs = new_model(pixels)\n",
        "    logits = outputs.logits.to('cpu')\n",
        "    predicted_class_idx = logits.argmax(-1).item()\n",
        "    predicted_labels.append(new_model.config.id2label[str(predicted_class_idx)])\n",
        "    actual_labels.append(new_model.config.id2label[str(item['labels'])])\n",
        "Получены выходные данные.\n",
        "0KB\n",
        "\ttext/plain\n",
        "\t\t0%|          | 0/17691 [00:00<?, ?it/s]\n",
        "\n",
        "Кодовая ячейка <NtNlmtLgT0fq>\n",
        "# %% [code]\n",
        "print(classification_report(actual_labels, predicted_labels))\n",
        "Получены выходные данные.\n",
        "2KB\n",
        "\tStream\n",
        "\t\tprecision    recall  f1-score   support\n",
        "\n",
        "\t\t           -       0.00      0.00      0.00         1\n",
        "\t\t           а       0.98      0.98      0.98      3042\n",
        "\t\t           б       0.99      0.83      0.90       264\n",
        "\t\t           в       0.99      0.96      0.98      1026\n",
        "\t\t           г       0.89      0.93      0.91       265\n",
        "\t\t           д       0.96      0.91      0.93       441\n",
        "\t\t           е       0.97      0.97      0.97      1165\n",
        "\t\t           ж       0.97      0.91      0.94        85\n",
        "\t\t           з       0.85      0.97      0.90       154\n",
        "\t\t           и       0.88      0.97      0.92      1389\n",
        "\t\t           й       0.75      0.09      0.16       137\n",
        "\t\t           к       0.97      0.98      0.97       934\n",
        "\t\t           л       0.94      0.96      0.95       979\n",
        "\t\t           м       0.98      0.94      0.96       518\n",
        "\t\t           н       0.95      0.97      0.96      1427\n",
        "\t\t           о       0.98      0.98      0.98      1361\n",
        "\t\t           п       0.96      0.94      0.95       238\n",
        "\t\t           р       0.97      0.96      0.97       996\n",
        "\t\t           с       0.96      0.97      0.96       703\n",
        "\t\t           т       0.98      0.97      0.97       654\n",
        "\t\t           у       0.95      0.95      0.95       311\n",
        "\t\t           ф       0.94      0.91      0.92       111\n",
        "\t\t           х       0.98      0.97      0.98       111\n",
        "\t\t           ц       0.89      0.85      0.87        80\n",
        "\t\t           ч       0.85      0.85      0.85       150\n",
        "\t\t           ш       0.94      0.94      0.94       154\n",
        "\t\t           щ       0.57      0.57      0.57         7\n",
        "\t\t           ъ       0.00      0.00      0.00         1\n",
        "\t\t           ы       1.00      0.23      0.38        56\n",
        "\t\t           ь       0.75      0.96      0.85       285\n",
        "\t\t           э       0.76      0.93      0.84        30\n",
        "\t\t           ю       0.96      0.96      0.96        97\n",
        "\t\t           я       0.99      0.98      0.98       513\n",
        "\t\t           ё       0.00      0.00      0.00         6\n",
        "\n",
        "\t\t    accuracy                           0.95     17691\n",
        "\t\t   macro avg       0.84      0.80      0.80     17691\n",
        "\t\tweighted avg       0.95      0.95      0.95     17691\n",
        "\n",
        "Атрибуты блокнота\n",
        "Свернуть разделы [\"2lWiIpAQtoiu\"]\n"
      ],
      "metadata": {
        "id": "ABjR-s_2smfC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}